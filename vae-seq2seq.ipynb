{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\keshavpc\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\gensim\\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\keshavPC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import datasets\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = os.path.join(\"data\", \"cornell\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 83097/83097 [00:04<00:00, 20236.84it/s]\n"
     ]
    }
   ],
   "source": [
    "data = datasets.readCornellData(dataset_path, max_len=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = os.path.join(\"data\", \"opensubs/OpenSubtitles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading OpenSubtitles conversations in data\\opensubs/OpenSubtitles.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OpenSubtitles data files: 0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "data += datasets.readOpensubsData(dataset_path, max_len=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hello diamond ranch', 'no i m sorry she doesn t work here anymore'),\n",
       " ('i know', 'but if you come in we have a number of very beautiful girls'),\n",
       " ('but if you come in we have a number of very beautiful girls',\n",
       "  'and i m sure you il find someone you can be satisfied with okay')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[-3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1431666"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv_embeddings = KeyedVectors.load_word2vec_format(\n",
    "    'data/GoogleNews-vectors-negative300.bin',\n",
    "    binary=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove short sentences\n",
    "data = [sentences for sentences in data if len(sentences[0].split()) > 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('cesc ma tete this is my head', 'right see youre ready for the quiz')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1638457"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('til they checked her over said there was nothing to be done',\n",
       " 'took 2 months til she was stable enough to move')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[-2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[sentences[0].split(), sentences[1].split()]for sentences in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['can',\n",
       "  'we',\n",
       "  'make',\n",
       "  'this',\n",
       "  'quick',\n",
       "  'roxanne',\n",
       "  'korrine',\n",
       "  'and',\n",
       "  'andrew',\n",
       "  'barrett',\n",
       "  'are',\n",
       "  'having',\n",
       "  'an',\n",
       "  'incredibly',\n",
       "  'horrendous',\n",
       "  'public',\n",
       "  'break',\n",
       "  'up',\n",
       "  'on',\n",
       "  'the',\n",
       "  'quad',\n",
       "  'again'],\n",
       " ['well',\n",
       "  'i',\n",
       "  'thought',\n",
       "  'wed',\n",
       "  'start',\n",
       "  'with',\n",
       "  'pronunciation',\n",
       "  'if',\n",
       "  'thats',\n",
       "  'okay',\n",
       "  'with',\n",
       "  'you']]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [sentences for sentences in data if len(sentences[0]) >= 3]\n",
    "data = [sentences for sentences in data if len(sentences[0]) <= 20]\n",
    "data = [sentences for sentences in data if len(sentences[1]) >= 3]\n",
    "data = [sentences for sentences in data if len(sentences[1]) <= 20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "word_count = defaultdict(int)\n",
    "for sentences in data:\n",
    "    for word in sentences[0]:\n",
    "        word_count[word] += 1\n",
    "    for word in sentences[1]:\n",
    "        word_count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc_threshold = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the lines with unknown words\n",
    "data_filtered = []\n",
    "for sentence1, sentence2 in data:\n",
    "    valid = True\n",
    "    for word in sentence1 + sentence2:\n",
    "        if word not in wv_embeddings or word_count[word] < wc_threshold:\n",
    "            valid = False\n",
    "            break\n",
    "    if valid:\n",
    "        data_filtered.append([sentence1, sentence2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "379897"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.sort(key=lambda x:len(x[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['is', 'that', 'right'],\n",
       " ['yeah', 'but', 'then', 'i', 'fucked', 'up', 'i', 'fell', 'for', 'her']]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "379897"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_set = set()\n",
    "for sentence1, sentence2 in data:\n",
    "    word_set |= set(sentence1)\n",
    "    word_set |= set(sentence2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19926"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_token = \"[START]\"\n",
    "end_token = \"[END]\"\n",
    "pad_token = \"[PAD]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_to_word = [start_token, end_token, pad_token] + [None] * len(word_set)\n",
    "word_to_idx = {start_token:0, end_token:1, pad_token:2}\n",
    "index = 3\n",
    "for word in word_set:\n",
    "    word_to_idx[word] = index\n",
    "    idx_to_word[index] = word\n",
    "    index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_idx, end_idx = 0, 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 300\n",
    "word_embeddings = np.zeros((len(word_set)+3, embedding_dim), dtype='float32')\n",
    "word_embeddings[0,:] = 0.\n",
    "word_embeddings[1,:] = 1.\n",
    "word_embeddings[2,:] = -1.\n",
    "for i in range(3, len(word_set)+3):\n",
    "    word_embeddings[i, :] = wv_embeddings[idx_to_word[i]]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentences in data:\n",
    "    sentences[1].append(end_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['is', 'that', 'right'],\n",
       " ['yeah',\n",
       "  'but',\n",
       "  'then',\n",
       "  'i',\n",
       "  'fucked',\n",
       "  'up',\n",
       "  'i',\n",
       "  'fell',\n",
       "  'for',\n",
       "  'her',\n",
       "  '[END]']]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_lengths = [len(sentences[0]) for sentences in data]\n",
    "ground_truth_lengths = [len(sentences[1]) for sentences in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add padding\n",
    "max_input_lengths = max(input_lengths)\n",
    "max_ground_truth_lengths = max(ground_truth_lengths)\n",
    "input_sentences = []\n",
    "ground_truth_sentences = []\n",
    "for sentences in data:\n",
    "    input_sentences.append(sentences[0] + [pad_token]*(max_input_lengths-len(sentences[0])))\n",
    "    ground_truth_sentences.append(sentences[1] + [pad_token]*(max_ground_truth_lengths-len(sentences[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sentences_idx = [[word_to_idx[word] if word in word_to_idx else unknown_idx for word in sentence] for sentence in input_sentences]\n",
    "ground_truth_sentences_idx = [[word_to_idx[word] if word in word_to_idx else unknown_idx for word in sentence] for sentence in ground_truth_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sentences_idx = np.array(input_sentences_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_sentences_idx = np.array(ground_truth_sentences_idx)\n",
    "input_lengths = np.array(input_lengths)\n",
    "ground_truth_lengths = np.array(ground_truth_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['you', 'know', 'chastity'],\n",
       " ['i', 'believe', 'we', 'share', 'an', 'art', 'instructor', '[END]']]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generator(batch_size,\n",
    "                    input_sentences_idx,\n",
    "                    ground_truth_sentences_idx,\n",
    "                    input_lengths,\n",
    "                    ground_truth_lengths):\n",
    "    index = 0\n",
    "    while index < len(input_sentences_idx):\n",
    "        batch_input_length = input_lengths[index:index+batch_size]\n",
    "        batch_input_sentences_idx = input_sentences_idx[index:index+batch_size, :batch_input_length.max()]\n",
    "        batch_ground_truth_length = ground_truth_lengths[index:index+batch_size]\n",
    "        batch_ground_truth_sentences_idx = ground_truth_sentences_idx[index:index+batch_size, :batch_ground_truth_length.max()]\n",
    "        yield (batch_input_sentences_idx, batch_input_length,\n",
    "              batch_ground_truth_sentences_idx, batch_ground_truth_length)\n",
    "        index += batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = batch_generator(10, input_sentences_idx, ground_truth_sentences_idx, input_lengths, ground_truth_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(word_set) + 3\n",
    "num_units = 128\n",
    "embedding_size = 300\n",
    "num_encoder_layers = 2\n",
    "num_decoder_layers = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_prepare(text):\n",
    "    \"\"\"Performs tokenization and simple preprocessing.\"\"\"\n",
    "    \n",
    "    replace_by_space_re = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "    good_symbols_re = re.compile('[^0-9a-z #+_]')\n",
    "#     stopwords_set = set(stopwords.words('english'))\n",
    "\n",
    "    text = text.lower()\n",
    "    text = replace_by_space_re.sub(' ', text)\n",
    "    text = good_symbols_re.sub('', text)\n",
    "#     text = ' '.join([x for x in text.split() if x and x not in stopwords_set])\n",
    "\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatBot:\n",
    "    def __init__(self):\n",
    "        self.declare_placeholders()\n",
    "        self.build_input_encoder()\n",
    "        self.build_ground_truth_encoder()\n",
    "        self.build_hidden_state()\n",
    "        self.build_decoder()\n",
    "        self.define_loss_and_train()\n",
    "    \n",
    "    def declare_placeholders(self):\n",
    "        \"\"\"Specifies placeholders for the model.\"\"\"\n",
    "        # Placeholders for input and its actual lengths.\n",
    "        self.input_batch = tf.placeholder(shape=(None, None), dtype=tf.int32, name='input_batch')\n",
    "        self.input_batch_lengths = tf.placeholder(shape=(None, ), dtype=tf.int32, name='input_batch_lengths')\n",
    "\n",
    "        # Placeholders for groundtruth and its actual lengths.\n",
    "        self.ground_truth = tf.placeholder(shape=(None, None), dtype=tf.int32, name='ground_truth')\n",
    "        self.ground_truth_lengths = tf.placeholder(shape=(None, ), dtype=tf.int32, name='ground_truth_lengths')\n",
    "\n",
    "        self.dropout_ph = tf.placeholder_with_default(tf.cast(1.0, tf.float32), shape=[])\n",
    "        self.learning_rate_ph = tf.placeholder(dtype=tf.float32, shape=[])\n",
    "\n",
    "    def build_input_encoder(self):\n",
    "        with tf.variable_scope('input_encoder') as input_encoder_scope:\n",
    "#             random_initializer = tf.random_uniform((vocab_size, embedding_size), -1.0, 1.0)\n",
    "            self.embeddings = tf.Variable(initial_value=word_embeddings, name='embeddings', dtype=tf.float32) \n",
    "\n",
    "            # Perform embeddings lookup for self.input_batch. \n",
    "            self.input_batch_embedded = tf.nn.embedding_lookup(self.embeddings, self.input_batch)\n",
    "            # Create encoder cells\n",
    "            rnn_layers = []\n",
    "            for i in range(num_encoder_layers-1):\n",
    "                with tf.variable_scope('input_encoder_rnn_layer' + str(i + 1)) as scope:\n",
    "                    cell = tf.nn.rnn_cell.GRUCell(num_units, activation=tf.nn.relu)\n",
    "                    cell = tf.nn.rnn_cell.DropoutWrapper(cell, input_keep_prob=self.dropout_ph, dtype=tf.float32)\n",
    "                    rnn_layers.append(cell)\n",
    "            with tf.variable_scope('input_encoder_rnn_layer' + str(num_encoder_layers)) as scope:\n",
    "                cell = tf.nn.rnn_cell.GRUCell(num_units)\n",
    "                cell = tf.nn.rnn_cell.DropoutWrapper(cell, input_keep_prob=self.dropout_ph, dtype=tf.float32)\n",
    "                rnn_layers.append(cell)\n",
    "            encoder_cell = tf.contrib.rnn.MultiRNNCell(rnn_layers)\n",
    "            self.input_encoder_outputs, self.final_input_encoder_state = tf.nn.dynamic_rnn(\n",
    "                encoder_cell,\n",
    "                self.input_batch_embedded,\n",
    "                sequence_length=self.input_batch_lengths,\n",
    "                dtype=tf.float32\n",
    "            )\n",
    "            self.final_input_encoder_state = self.final_input_encoder_state[-1]\n",
    "\n",
    "    def build_ground_truth_encoder(self):\n",
    "        with tf.variable_scope('ground_truth_encoder') as ground_truth_encoder:\n",
    "            self.ground_truth_batch_embedded = tf.nn.embedding_lookup(self.embeddings, self.ground_truth)\n",
    "            rnn_layers = []\n",
    "            for i in range(num_encoder_layers-1):\n",
    "                with tf.variable_scope('ground_truth_encoder_rnn_layer' + str(i + 1)) as scope:\n",
    "                    cell = tf.nn.rnn_cell.GRUCell(num_units, activation=tf.nn.relu)\n",
    "                    cell = tf.nn.rnn_cell.DropoutWrapper(cell, input_keep_prob=self.dropout_ph, dtype=tf.float32)\n",
    "                    rnn_layers.append(cell)\n",
    "            with tf.variable_scope('ground_truth_encoder_rnn_layer' + str(num_encoder_layers)) as scope:\n",
    "                cell = tf.nn.rnn_cell.GRUCell(num_units)\n",
    "                cell = tf.nn.rnn_cell.DropoutWrapper(cell, input_keep_prob=self.dropout_ph, dtype=tf.float32)\n",
    "                rnn_layers.append(cell)\n",
    "            encoder_cell = tf.contrib.rnn.MultiRNNCell(rnn_layers)\n",
    "            _, self.final_ground_truth_encoder_state = tf.nn.dynamic_rnn(\n",
    "                encoder_cell,\n",
    "                self.ground_truth_batch_embedded,\n",
    "                sequence_length=self.ground_truth_lengths,\n",
    "                dtype=tf.float32\n",
    "            )\n",
    "            self.final_ground_truth_encoder_state = self.final_ground_truth_encoder_state[-1]\n",
    "\n",
    "    def build_hidden_state(self):\n",
    "        self.z_mean_from_input, self.z_log_var_from_input = tf.split(\n",
    "            self.final_input_encoder_state, num_or_size_splits=2, axis=1)\n",
    "        \n",
    "        self.z_mean_from_ground_truth, self.z_log_var_from_grount_truth = tf.split(\n",
    "            self.final_input_encoder_state, num_or_size_splits=2, axis=1)\n",
    "        \n",
    "        \n",
    "        self.z = (self.z_mean_from_ground_truth +\n",
    "                  tf.exp(0.5*self.z_log_var_from_grount_truth) *\n",
    "                  tf.random_normal(tf.shape(self.z_log_var_from_grount_truth), 0, 1, dtype=tf.float32))\n",
    "\n",
    "    def build_decoder(self):\n",
    "        batch_size = tf.shape(self.input_batch)[0]\n",
    "        start_tokens = tf.fill([batch_size], start_idx)\n",
    "        ground_truth_as_input = tf.concat([tf.expand_dims(start_tokens, 1), self.ground_truth], 1)\n",
    "        self.ground_truth_embedded = tf.nn.embedding_lookup(\n",
    "            self.embeddings, ground_truth_as_input)\n",
    "        train_helper = tf.contrib.seq2seq.TrainingHelper(self.ground_truth_embedded,\n",
    "                                                         self.ground_truth_lengths)\n",
    "        infer_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(self.embeddings, start_tokens, end_idx)\n",
    "\n",
    "        def decode(helper, scope, reuse=None):\n",
    "            \"\"\"Creates decoder and return the results of the decoding with a given helper.\"\"\"\n",
    "            with tf.variable_scope(scope, reuse=reuse):\n",
    "                # Create GRUCell with dropout. Do not forget to set the reuse flag properly.\n",
    "                rnn_layers = []\n",
    "                for i in range(num_decoder_layers-1):\n",
    "                    with tf.variable_scope('decoder_rnn_layer' + str(i + 1)) as scope:\n",
    "                        decoder_cell = tf.contrib.rnn.GRUCell(num_units=num_units/2, reuse=reuse, activation=tf.nn.tanh)\n",
    "                        decoder_cell = tf.contrib.rnn.DropoutWrapper(decoder_cell, input_keep_prob=self.dropout_ph)\n",
    "                        rnn_layers.append(decoder_cell)\n",
    "                with tf.variable_scope('decoder_rnn_layer' + str(num_decoder_layers)) as scope:\n",
    "                    decoder_cell = tf.contrib.rnn.GRUCell(num_units=num_units/2, reuse=reuse)\n",
    "                    decoder_cell = tf.contrib.rnn.DropoutWrapper(decoder_cell, input_keep_prob=self.dropout_ph)\n",
    "                    rnn_layers.append(decoder_cell)\n",
    "                decoder_cell = tf.contrib.rnn.MultiRNNCell(rnn_layers)\n",
    "                # Create attention\n",
    "                attention_mechanism = tf.contrib.seq2seq.BahdanauAttention(\n",
    "                    num_units=num_units, memory=tf.split(self.input_encoder_outputs, num_or_size_splits=2, axis=-1)[0],\n",
    "                    memory_sequence_length=self.input_batch_lengths)\n",
    "                decoder_cell = tf.contrib.seq2seq.AttentionWrapper(\n",
    "                    decoder_cell, attention_mechanism, attention_layer_size=num_units)\n",
    "                # Create a projection wrapper.\n",
    "                decoder_cell = tf.contrib.rnn.OutputProjectionWrapper(decoder_cell, vocab_size, reuse=reuse)\n",
    "                # Create BasicDecoder, pass the defined cell, a helper, and initial state.\n",
    "                # The initial state should be equal to the final state of the encoder!\n",
    "                second_state = tf.zeros((1, num_units/2))\n",
    "                second_state = tf.tile(second_state, [batch_size, 1])\n",
    "                decoder_initial_state = decoder_cell.zero_state(dtype=tf.float32, batch_size=batch_size).clone(\n",
    "                    cell_state=(self.z, second_state))\n",
    "                decoder = tf.contrib.seq2seq.BasicDecoder(cell=decoder_cell, helper=helper, initial_state=decoder_initial_state)\n",
    "\n",
    "                # The first returning argument of dynamic_decode contains two fields:\n",
    "                #   rnn_output (predicted logits)\n",
    "                #   sample_id (predictions)\n",
    "                outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(decoder=decoder, maximum_iterations=tf.reduce_max(self.ground_truth_lengths), \n",
    "                                                                  output_time_major=False, impute_finished=True)\n",
    "\n",
    "                return outputs\n",
    "\n",
    "        self.train_outputs = decode(train_helper, 'decode')\n",
    "        self.infer_outputs = decode(infer_helper, 'decode', reuse=True)\n",
    "        self.train_predictions = self.train_outputs.sample_id\n",
    "        self.infer_predictions = self.infer_outputs.sample_id\n",
    "\n",
    "    def define_loss_and_train(self):\n",
    "        weights = tf.cast(tf.sequence_mask(self.ground_truth_lengths), dtype=tf.float32)\n",
    "        self.reconstruction_loss = tf.contrib.seq2seq.sequence_loss(\n",
    "            self.train_outputs.rnn_output,\n",
    "            self.ground_truth,\n",
    "            weights\n",
    "        )\n",
    "        self.kl_loss = (0.5*(self.z_log_var_from_input-self.z_log_var_from_grount_truth)+\n",
    "                       (tf.exp(self.z_log_var_from_grount_truth)+(self.z_mean_from_ground_truth-self.z_mean_from_input)**2)/\n",
    "                       (2*tf.exp(self.z_log_var_from_input))-0.5)\n",
    "        self.kl_loss = tf.reduce_mean(self.kl_loss)\n",
    "        self.loss = self.kl_loss + self.reconstruction_loss\n",
    "        self.train_op = tf.contrib.layers.optimize_loss(\n",
    "            loss=self.loss,\n",
    "            optimizer='Adam',\n",
    "            learning_rate=self.learning_rate_ph,\n",
    "            clip_gradients=1.0,\n",
    "            global_step=tf.train.get_global_step()\n",
    "        )\n",
    "\n",
    "    def train_on_batch(self, session, X, X_seq_len, Y, Y_seq_len, learning_rate, dropout_keep_probability):\n",
    "        feed_dict = {\n",
    "            self.input_batch: X,\n",
    "            self.input_batch_lengths: X_seq_len,\n",
    "            self.ground_truth: Y,\n",
    "            self.ground_truth_lengths: Y_seq_len,\n",
    "            self.learning_rate_ph: learning_rate,\n",
    "            self.dropout_ph: dropout_keep_probability\n",
    "        }\n",
    "        loss, _ = session.run([\n",
    "            self.loss,\n",
    "            self.train_op], feed_dict=feed_dict)\n",
    "        return loss\n",
    "    \n",
    "    def get_reply(self, session, input_sentence):\n",
    "        input_sentence = text_prepare(input_sentence)\n",
    "        X = [[word_to_idx[word] if word in word_to_idx else unknown_idx for word in input_sentence]]\n",
    "        X = np.array(X)\n",
    "        feed_dict = {\n",
    "            self.input_batch: X,\n",
    "            self.input_batch_lengths: np.array([len(input_sentence)])\n",
    "        }\n",
    "        z_mean, z_log_var, input_encoder_outputs = session.run([\n",
    "            self.z_mean_from_input,\n",
    "            self.z_log_var_from_input,\n",
    "            self.input_encoder_outputs\n",
    "        ], feed_dict=feed_dict)\n",
    "        z_mean = z_mean[0]\n",
    "        z_log_var = z_log_var[0]\n",
    "        z = np.random.normal(z_mean, np.exp(0.5*z_log_var), z_mean.size)\n",
    "        z = z[np.newaxis,:]\n",
    "        feed_dict = {\n",
    "            self.z: z,\n",
    "            self.input_batch: X,\n",
    "            self.input_batch_lengths: np.array([len(input_sentence)]),\n",
    "            self.ground_truth_lengths: np.array([15])\n",
    "        }\n",
    "        pred = session.run([self.infer_predictions], feed_dict=feed_dict)\n",
    "        return \" \".join([idx_to_word[index] for index in pred[0][0]])\n",
    "\n",
    "    def train(self, session, epochs, batch_size, input_sentences_idx, ground_truth_sentences_idx, input_lengths, ground_truth_lengths, learning_rate, dropout_keep_probability):\n",
    "        for i in range(epochs):\n",
    "            batch_num = 1\n",
    "            for (batch_input_sentences_idx,\n",
    "                 batch_input_length,\n",
    "                 batch_ground_truth_sentences_idx,\n",
    "                 batch_ground_truth_length) in batch_generator(\n",
    "                batch_size, input_sentences_idx, ground_truth_sentences_idx,\n",
    "                input_lengths, ground_truth_lengths):\n",
    "                loss = self.train_on_batch(\n",
    "                    session,\n",
    "                    batch_input_sentences_idx,\n",
    "                    batch_input_length,\n",
    "                    batch_ground_truth_sentences_idx,\n",
    "                    batch_ground_truth_length,\n",
    "                    learning_rate,\n",
    "                    dropout_keep_probability\n",
    "                )\n",
    "                print(\"Epoch {i}, batch {batch}, loss = {loss}\".format(i=i+1, batch=batch_num, loss=loss))\n",
    "                batch_num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatbot = ChatBot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = tf.Session()\n",
    "session.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, batch 1, loss = 9.899627685546875\n",
      "Epoch 1, batch 2, loss = 9.899077415466309\n",
      "Epoch 1, batch 3, loss = 9.898760795593262\n",
      "Epoch 1, batch 4, loss = 9.898591041564941\n",
      "Epoch 1, batch 5, loss = 9.897552490234375\n",
      "Epoch 1, batch 6, loss = 9.89767074584961\n",
      "Epoch 1, batch 7, loss = 9.896485328674316\n",
      "Epoch 1, batch 8, loss = 9.896812438964844\n",
      "Epoch 1, batch 9, loss = 9.895785331726074\n",
      "Epoch 1, batch 10, loss = 9.895318984985352\n",
      "Epoch 1, batch 11, loss = 9.895915985107422\n",
      "Epoch 1, batch 12, loss = 9.894783020019531\n",
      "Epoch 1, batch 13, loss = 9.892687797546387\n",
      "Epoch 1, batch 14, loss = 9.894875526428223\n",
      "Epoch 1, batch 15, loss = 9.892996788024902\n",
      "Epoch 1, batch 16, loss = 9.891974449157715\n",
      "Epoch 1, batch 17, loss = 9.891645431518555\n",
      "Epoch 1, batch 18, loss = 9.890048027038574\n",
      "Epoch 1, batch 19, loss = 9.888581275939941\n",
      "Epoch 1, batch 20, loss = 9.88957405090332\n",
      "Epoch 1, batch 21, loss = 9.887972831726074\n",
      "Epoch 1, batch 22, loss = 9.889283180236816\n",
      "Epoch 1, batch 23, loss = 9.888226509094238\n",
      "Epoch 1, batch 24, loss = 9.886801719665527\n",
      "Epoch 1, batch 25, loss = 9.885839462280273\n",
      "Epoch 1, batch 26, loss = 9.884379386901855\n",
      "Epoch 1, batch 27, loss = 9.882915496826172\n",
      "Epoch 1, batch 28, loss = 9.882392883300781\n",
      "Epoch 1, batch 29, loss = 9.88146686553955\n",
      "Epoch 1, batch 30, loss = 9.880864143371582\n",
      "Epoch 1, batch 31, loss = 9.880279541015625\n",
      "Epoch 1, batch 32, loss = 9.877147674560547\n",
      "Epoch 1, batch 33, loss = 9.87468433380127\n",
      "Epoch 1, batch 34, loss = 9.872844696044922\n",
      "Epoch 1, batch 35, loss = 9.871726036071777\n",
      "Epoch 1, batch 36, loss = 9.870573043823242\n",
      "Epoch 1, batch 37, loss = 9.868302345275879\n",
      "Epoch 1, batch 38, loss = 9.867485046386719\n",
      "Epoch 1, batch 39, loss = 9.861624717712402\n",
      "Epoch 1, batch 40, loss = 9.858893394470215\n",
      "Epoch 1, batch 41, loss = 9.85855484008789\n",
      "Epoch 1, batch 42, loss = 9.853764533996582\n",
      "Epoch 1, batch 43, loss = 9.85060977935791\n",
      "Epoch 1, batch 44, loss = 9.847535133361816\n",
      "Epoch 1, batch 45, loss = 9.838824272155762\n",
      "Epoch 1, batch 46, loss = 9.834745407104492\n",
      "Epoch 1, batch 47, loss = 9.833357810974121\n",
      "Epoch 1, batch 48, loss = 9.830708503723145\n",
      "Epoch 1, batch 49, loss = 9.82361125946045\n",
      "Epoch 1, batch 50, loss = 9.82058048248291\n",
      "Epoch 1, batch 51, loss = 9.810320854187012\n",
      "Epoch 1, batch 52, loss = 9.806234359741211\n",
      "Epoch 1, batch 53, loss = 9.806066513061523\n",
      "Epoch 1, batch 54, loss = 9.79149341583252\n",
      "Epoch 1, batch 55, loss = 9.774142265319824\n",
      "Epoch 1, batch 56, loss = 9.775148391723633\n",
      "Epoch 1, batch 57, loss = 9.776217460632324\n",
      "Epoch 1, batch 58, loss = 9.761051177978516\n",
      "Epoch 1, batch 59, loss = 9.762231826782227\n",
      "Epoch 1, batch 60, loss = 9.738593101501465\n",
      "Epoch 1, batch 61, loss = 9.720613479614258\n",
      "Epoch 1, batch 62, loss = 9.715948104858398\n",
      "Epoch 1, batch 63, loss = 9.698729515075684\n",
      "Epoch 1, batch 64, loss = 9.669646263122559\n",
      "Epoch 1, batch 65, loss = 9.666687965393066\n",
      "Epoch 1, batch 66, loss = 9.654523849487305\n",
      "Epoch 1, batch 67, loss = 9.62964153289795\n",
      "Epoch 1, batch 68, loss = 9.606544494628906\n",
      "Epoch 1, batch 69, loss = 9.582082748413086\n",
      "Epoch 1, batch 70, loss = 9.56772232055664\n",
      "Epoch 1, batch 71, loss = 9.54334545135498\n",
      "Epoch 1, batch 72, loss = 9.532609939575195\n",
      "Epoch 1, batch 73, loss = 9.499343872070312\n",
      "Epoch 1, batch 74, loss = 9.463120460510254\n",
      "Epoch 1, batch 75, loss = 9.437697410583496\n",
      "Epoch 1, batch 76, loss = 9.420310974121094\n",
      "Epoch 1, batch 77, loss = 9.377676010131836\n",
      "Epoch 1, batch 78, loss = 9.318041801452637\n",
      "Epoch 1, batch 79, loss = 9.273293495178223\n",
      "Epoch 1, batch 80, loss = 9.254735946655273\n",
      "Epoch 1, batch 81, loss = 9.19360065460205\n",
      "Epoch 1, batch 82, loss = 9.15532398223877\n",
      "Epoch 1, batch 83, loss = 9.140813827514648\n",
      "Epoch 1, batch 84, loss = 9.036320686340332\n",
      "Epoch 1, batch 85, loss = 9.002330780029297\n",
      "Epoch 1, batch 86, loss = 8.931851387023926\n",
      "Epoch 1, batch 87, loss = 8.880559921264648\n",
      "Epoch 1, batch 88, loss = 8.816848754882812\n",
      "Epoch 1, batch 89, loss = 8.828377723693848\n",
      "Epoch 1, batch 90, loss = 8.750287055969238\n",
      "Epoch 1, batch 91, loss = 8.701128959655762\n",
      "Epoch 1, batch 92, loss = 8.625182151794434\n",
      "Epoch 1, batch 93, loss = 8.627527236938477\n",
      "Epoch 1, batch 94, loss = 8.524137496948242\n",
      "Epoch 1, batch 95, loss = 8.443099975585938\n",
      "Epoch 1, batch 96, loss = 8.35981273651123\n",
      "Epoch 1, batch 97, loss = 8.354642868041992\n",
      "Epoch 1, batch 98, loss = 8.223871231079102\n",
      "Epoch 1, batch 99, loss = 8.179840087890625\n",
      "Epoch 1, batch 100, loss = 8.113982200622559\n",
      "Epoch 1, batch 101, loss = 7.980840682983398\n",
      "Epoch 1, batch 102, loss = 7.834567546844482\n",
      "Epoch 1, batch 103, loss = 7.837106227874756\n",
      "Epoch 1, batch 104, loss = 7.73406982421875\n",
      "Epoch 1, batch 105, loss = 7.6163787841796875\n",
      "Epoch 1, batch 106, loss = 7.476978302001953\n",
      "Epoch 1, batch 107, loss = 7.439605236053467\n",
      "Epoch 1, batch 108, loss = 7.515315532684326\n",
      "Epoch 1, batch 109, loss = 7.416463375091553\n",
      "Epoch 1, batch 110, loss = 7.358521461486816\n",
      "Epoch 1, batch 111, loss = 7.38551664352417\n",
      "Epoch 1, batch 112, loss = 7.167817115783691\n",
      "Epoch 1, batch 113, loss = 7.094106197357178\n",
      "Epoch 1, batch 114, loss = 6.919925212860107\n",
      "Epoch 1, batch 115, loss = 6.927702903747559\n",
      "Epoch 1, batch 116, loss = 6.9064717292785645\n",
      "Epoch 1, batch 117, loss = 6.825565814971924\n",
      "Epoch 1, batch 118, loss = 6.719040393829346\n",
      "Epoch 1, batch 119, loss = 6.688920497894287\n",
      "Epoch 1, batch 120, loss = 6.627885341644287\n",
      "Epoch 1, batch 121, loss = 6.449293613433838\n",
      "Epoch 1, batch 122, loss = 6.506927490234375\n",
      "Epoch 1, batch 123, loss = 6.5217437744140625\n",
      "Epoch 1, batch 124, loss = 6.267281532287598\n",
      "Epoch 1, batch 125, loss = 6.345534801483154\n",
      "Epoch 1, batch 126, loss = 6.330135345458984\n",
      "Epoch 1, batch 127, loss = 6.3617262840271\n",
      "Epoch 1, batch 128, loss = 6.298266410827637\n",
      "Epoch 1, batch 129, loss = 6.160725116729736\n",
      "Epoch 1, batch 130, loss = 6.0701003074646\n",
      "Epoch 1, batch 131, loss = 6.1224212646484375\n",
      "Epoch 1, batch 132, loss = 6.089686870574951\n",
      "Epoch 1, batch 133, loss = 5.985062599182129\n",
      "Epoch 1, batch 134, loss = 6.063472747802734\n",
      "Epoch 1, batch 135, loss = 6.017139434814453\n",
      "Epoch 1, batch 136, loss = 6.0347795486450195\n",
      "Epoch 1, batch 137, loss = 5.959182262420654\n",
      "Epoch 1, batch 138, loss = 5.9973464012146\n",
      "Epoch 1, batch 139, loss = 6.078447341918945\n",
      "Epoch 1, batch 140, loss = 5.950576305389404\n",
      "Epoch 1, batch 141, loss = 5.971832275390625\n",
      "Epoch 1, batch 142, loss = 6.00815486907959\n",
      "Epoch 1, batch 143, loss = 6.0291643142700195\n",
      "Epoch 1, batch 144, loss = 5.914748668670654\n",
      "Epoch 1, batch 145, loss = 5.983365535736084\n",
      "Epoch 1, batch 146, loss = 6.058907508850098\n",
      "Epoch 1, batch 147, loss = 6.032541275024414\n",
      "Epoch 1, batch 148, loss = 6.013065338134766\n",
      "Epoch 1, batch 149, loss = 5.913200855255127\n",
      "Epoch 1, batch 150, loss = 5.996781826019287\n",
      "Epoch 1, batch 151, loss = 6.021122932434082\n",
      "Epoch 1, batch 152, loss = 5.8717803955078125\n",
      "Epoch 1, batch 153, loss = 5.772552490234375\n",
      "Epoch 1, batch 154, loss = 5.890827178955078\n",
      "Epoch 1, batch 155, loss = 5.903435707092285\n",
      "Epoch 1, batch 156, loss = 5.942631244659424\n",
      "Epoch 1, batch 157, loss = 5.855160713195801\n",
      "Epoch 1, batch 158, loss = 5.755771160125732\n",
      "Epoch 1, batch 159, loss = 5.869065284729004\n",
      "Epoch 1, batch 160, loss = 6.035220146179199\n",
      "Epoch 1, batch 161, loss = 5.8223676681518555\n",
      "Epoch 1, batch 162, loss = 5.772493362426758\n",
      "Epoch 1, batch 163, loss = 5.948653221130371\n",
      "Epoch 1, batch 164, loss = 5.851853847503662\n",
      "Epoch 1, batch 165, loss = 5.949911594390869\n",
      "Epoch 1, batch 166, loss = 5.81154203414917\n",
      "Epoch 1, batch 167, loss = 6.006634712219238\n",
      "Epoch 1, batch 168, loss = 5.887721538543701\n",
      "Epoch 1, batch 169, loss = 5.925201416015625\n",
      "Epoch 1, batch 170, loss = 5.906489849090576\n",
      "Epoch 1, batch 171, loss = 5.750312805175781\n",
      "Epoch 1, batch 172, loss = 5.6369218826293945\n",
      "Epoch 1, batch 173, loss = 5.874298095703125\n",
      "Epoch 1, batch 174, loss = 5.862325668334961\n",
      "Epoch 1, batch 175, loss = 5.7369513511657715\n",
      "Epoch 1, batch 176, loss = 6.012863636016846\n",
      "Epoch 1, batch 177, loss = 5.668872356414795\n",
      "Epoch 1, batch 178, loss = 5.720143795013428\n",
      "Epoch 1, batch 179, loss = 5.732316493988037\n",
      "Epoch 1, batch 180, loss = 5.806420803070068\n",
      "Epoch 1, batch 181, loss = 5.869625091552734\n",
      "Epoch 1, batch 182, loss = 5.753172397613525\n",
      "Epoch 1, batch 183, loss = 5.772610187530518\n",
      "Epoch 1, batch 184, loss = 5.853152751922607\n",
      "Epoch 1, batch 185, loss = 5.889032363891602\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, batch 186, loss = 5.736809253692627\n",
      "Epoch 1, batch 187, loss = 5.926025390625\n",
      "Epoch 1, batch 188, loss = 5.9170708656311035\n",
      "Epoch 1, batch 189, loss = 5.84435510635376\n",
      "Epoch 1, batch 190, loss = 5.814812660217285\n",
      "Epoch 1, batch 191, loss = 5.97196102142334\n",
      "Epoch 1, batch 192, loss = 6.279820919036865\n",
      "Epoch 1, batch 193, loss = 6.136571884155273\n",
      "Epoch 1, batch 194, loss = 6.129721641540527\n",
      "Epoch 1, batch 195, loss = 6.123713493347168\n",
      "Epoch 1, batch 196, loss = 6.160660266876221\n",
      "Epoch 1, batch 197, loss = 6.067513465881348\n",
      "Epoch 1, batch 198, loss = 6.307403564453125\n",
      "Epoch 1, batch 199, loss = 6.037540435791016\n",
      "Epoch 1, batch 200, loss = 5.996161460876465\n",
      "Epoch 1, batch 201, loss = 6.182076930999756\n",
      "Epoch 1, batch 202, loss = 6.04791784286499\n",
      "Epoch 1, batch 203, loss = 6.016354560852051\n",
      "Epoch 1, batch 204, loss = 6.001806259155273\n",
      "Epoch 1, batch 205, loss = 5.908658027648926\n",
      "Epoch 1, batch 206, loss = 5.962727069854736\n",
      "Epoch 1, batch 207, loss = 5.919088363647461\n",
      "Epoch 1, batch 208, loss = 6.23574161529541\n",
      "Epoch 1, batch 209, loss = 5.9375529289245605\n",
      "Epoch 1, batch 210, loss = 6.142545223236084\n",
      "Epoch 1, batch 211, loss = 5.945938587188721\n",
      "Epoch 1, batch 212, loss = 5.902935981750488\n",
      "Epoch 1, batch 213, loss = 6.088588714599609\n",
      "Epoch 1, batch 214, loss = 6.08814001083374\n",
      "Epoch 1, batch 215, loss = 6.1712541580200195\n",
      "Epoch 1, batch 216, loss = 5.971579074859619\n",
      "Epoch 1, batch 217, loss = 5.962636947631836\n",
      "Epoch 1, batch 218, loss = 5.828485488891602\n",
      "Epoch 1, batch 219, loss = 5.76414680480957\n",
      "Epoch 1, batch 220, loss = 5.7983269691467285\n",
      "Epoch 1, batch 221, loss = 5.850020885467529\n",
      "Epoch 1, batch 222, loss = 5.734344005584717\n",
      "Epoch 1, batch 223, loss = 5.922083377838135\n",
      "Epoch 1, batch 224, loss = 5.939157009124756\n",
      "Epoch 1, batch 225, loss = 5.869937419891357\n",
      "Epoch 1, batch 226, loss = 6.066777229309082\n",
      "Epoch 1, batch 227, loss = 6.118373870849609\n",
      "Epoch 1, batch 228, loss = 5.832668781280518\n",
      "Epoch 1, batch 229, loss = 5.907339096069336\n",
      "Epoch 1, batch 230, loss = 5.901470184326172\n",
      "Epoch 1, batch 231, loss = 6.178069591522217\n",
      "Epoch 1, batch 232, loss = 6.019687652587891\n",
      "Epoch 1, batch 233, loss = 6.037075519561768\n",
      "Epoch 1, batch 234, loss = 5.989124774932861\n",
      "Epoch 1, batch 235, loss = 6.1617112159729\n",
      "Epoch 1, batch 236, loss = 6.213608264923096\n",
      "Epoch 1, batch 237, loss = 5.774546146392822\n",
      "Epoch 1, batch 238, loss = 5.730717182159424\n",
      "Epoch 1, batch 239, loss = 5.87457275390625\n",
      "Epoch 1, batch 240, loss = 6.057028293609619\n",
      "Epoch 1, batch 241, loss = 5.855708122253418\n",
      "Epoch 1, batch 242, loss = 5.975177764892578\n",
      "Epoch 1, batch 243, loss = 5.893146991729736\n",
      "Epoch 1, batch 244, loss = 5.883151531219482\n",
      "Epoch 1, batch 245, loss = 5.808856010437012\n",
      "Epoch 1, batch 246, loss = 5.836294174194336\n",
      "Epoch 1, batch 247, loss = 5.979740619659424\n",
      "Epoch 1, batch 248, loss = 5.753566265106201\n",
      "Epoch 1, batch 249, loss = 5.72790002822876\n",
      "Epoch 1, batch 250, loss = 5.724865436553955\n",
      "Epoch 1, batch 251, loss = 5.863258361816406\n",
      "Epoch 1, batch 252, loss = 6.115701675415039\n",
      "Epoch 1, batch 253, loss = 5.901763439178467\n",
      "Epoch 1, batch 254, loss = 5.750979423522949\n",
      "Epoch 1, batch 255, loss = 5.864127159118652\n",
      "Epoch 1, batch 256, loss = 5.997620582580566\n",
      "Epoch 1, batch 257, loss = 5.860625743865967\n",
      "Epoch 1, batch 258, loss = 5.822566509246826\n",
      "Epoch 1, batch 259, loss = 5.799713134765625\n",
      "Epoch 1, batch 260, loss = 6.0078935623168945\n",
      "Epoch 1, batch 261, loss = 6.138195037841797\n",
      "Epoch 1, batch 262, loss = 5.810094356536865\n",
      "Epoch 1, batch 263, loss = 5.947994709014893\n",
      "Epoch 1, batch 264, loss = 5.6840620040893555\n",
      "Epoch 1, batch 265, loss = 5.624668598175049\n",
      "Epoch 1, batch 266, loss = 5.670482635498047\n",
      "Epoch 1, batch 267, loss = 5.871418476104736\n",
      "Epoch 1, batch 268, loss = 5.943027973175049\n",
      "Epoch 1, batch 269, loss = 5.774625301361084\n",
      "Epoch 1, batch 270, loss = 5.759331226348877\n",
      "Epoch 1, batch 271, loss = 5.712281227111816\n",
      "Epoch 1, batch 272, loss = 5.916210174560547\n",
      "Epoch 1, batch 273, loss = 5.865577220916748\n",
      "Epoch 1, batch 274, loss = 5.703896522521973\n",
      "Epoch 1, batch 275, loss = 5.606194972991943\n",
      "Epoch 1, batch 276, loss = 5.707226753234863\n",
      "Epoch 1, batch 277, loss = 5.817811012268066\n",
      "Epoch 1, batch 278, loss = 5.787845134735107\n",
      "Epoch 1, batch 279, loss = 5.837516784667969\n",
      "Epoch 1, batch 280, loss = 5.80091667175293\n",
      "Epoch 1, batch 281, loss = 5.815081596374512\n",
      "Epoch 1, batch 282, loss = 5.6966142654418945\n",
      "Epoch 1, batch 283, loss = 5.8925065994262695\n",
      "Epoch 1, batch 284, loss = 5.800540924072266\n",
      "Epoch 1, batch 285, loss = 5.846210479736328\n",
      "Epoch 1, batch 286, loss = 5.855106353759766\n",
      "Epoch 1, batch 287, loss = 5.8925700187683105\n",
      "Epoch 1, batch 288, loss = 5.892175674438477\n",
      "Epoch 1, batch 289, loss = 5.816775798797607\n",
      "Epoch 1, batch 290, loss = 5.980376243591309\n",
      "Epoch 1, batch 291, loss = 5.892045021057129\n",
      "Epoch 1, batch 292, loss = 5.961977481842041\n",
      "Epoch 1, batch 293, loss = 6.0145344734191895\n",
      "Epoch 1, batch 294, loss = 5.8940839767456055\n",
      "Epoch 1, batch 295, loss = 5.824679374694824\n",
      "Epoch 1, batch 296, loss = 5.938251972198486\n",
      "Epoch 1, batch 297, loss = 5.817789077758789\n",
      "Epoch 1, batch 298, loss = 6.048164367675781\n",
      "Epoch 1, batch 299, loss = 5.931624889373779\n",
      "Epoch 1, batch 300, loss = 6.114191055297852\n",
      "Epoch 1, batch 301, loss = 5.912627220153809\n",
      "Epoch 1, batch 302, loss = 5.937751293182373\n",
      "Epoch 1, batch 303, loss = 5.793966770172119\n",
      "Epoch 1, batch 304, loss = 5.912994384765625\n",
      "Epoch 1, batch 305, loss = 6.047455787658691\n",
      "Epoch 1, batch 306, loss = 5.898829936981201\n",
      "Epoch 1, batch 307, loss = 5.96273946762085\n",
      "Epoch 1, batch 308, loss = 5.93207311630249\n",
      "Epoch 1, batch 309, loss = 5.753348350524902\n",
      "Epoch 1, batch 310, loss = 5.749109745025635\n",
      "Epoch 1, batch 311, loss = 5.711196422576904\n",
      "Epoch 1, batch 312, loss = 5.891714572906494\n",
      "Epoch 1, batch 313, loss = 5.772555351257324\n",
      "Epoch 1, batch 314, loss = 5.622020721435547\n",
      "Epoch 1, batch 315, loss = 5.621712684631348\n",
      "Epoch 1, batch 316, loss = 5.819687843322754\n",
      "Epoch 1, batch 317, loss = 5.975386142730713\n",
      "Epoch 1, batch 318, loss = 5.952609062194824\n",
      "Epoch 1, batch 319, loss = 5.725778579711914\n",
      "Epoch 1, batch 320, loss = 5.947231769561768\n",
      "Epoch 1, batch 321, loss = 5.9655561447143555\n",
      "Epoch 1, batch 322, loss = 5.80998420715332\n",
      "Epoch 1, batch 323, loss = 5.781164169311523\n",
      "Epoch 1, batch 324, loss = 5.932314395904541\n",
      "Epoch 1, batch 325, loss = 6.053933143615723\n",
      "Epoch 1, batch 326, loss = 5.957117557525635\n",
      "Epoch 1, batch 327, loss = 5.8656697273254395\n",
      "Epoch 1, batch 328, loss = 6.016966819763184\n",
      "Epoch 1, batch 329, loss = 5.687195301055908\n",
      "Epoch 1, batch 330, loss = 5.793455600738525\n",
      "Epoch 1, batch 331, loss = 5.66288423538208\n",
      "Epoch 1, batch 332, loss = 5.807491779327393\n",
      "Epoch 1, batch 333, loss = 6.124388217926025\n",
      "Epoch 1, batch 334, loss = 5.598702430725098\n",
      "Epoch 1, batch 335, loss = 6.038078784942627\n",
      "Epoch 1, batch 336, loss = 5.983832359313965\n",
      "Epoch 1, batch 337, loss = 5.804724216461182\n",
      "Epoch 1, batch 338, loss = 5.92385196685791\n",
      "Epoch 1, batch 339, loss = 5.913492202758789\n",
      "Epoch 1, batch 340, loss = 5.762465953826904\n",
      "Epoch 1, batch 341, loss = 5.821770668029785\n",
      "Epoch 1, batch 342, loss = 5.779594898223877\n",
      "Epoch 1, batch 343, loss = 5.907654285430908\n",
      "Epoch 1, batch 344, loss = 5.857924938201904\n",
      "Epoch 1, batch 345, loss = 5.807256698608398\n",
      "Epoch 1, batch 346, loss = 5.755152225494385\n",
      "Epoch 1, batch 347, loss = 5.799172878265381\n",
      "Epoch 1, batch 348, loss = 5.857876300811768\n",
      "Epoch 1, batch 349, loss = 5.668833255767822\n",
      "Epoch 1, batch 350, loss = 5.622786521911621\n",
      "Epoch 1, batch 351, loss = 5.800604820251465\n",
      "Epoch 1, batch 352, loss = 5.865611553192139\n",
      "Epoch 1, batch 353, loss = 5.849149227142334\n",
      "Epoch 1, batch 354, loss = 5.773418426513672\n",
      "Epoch 1, batch 355, loss = 5.761964797973633\n",
      "Epoch 1, batch 356, loss = 5.747828006744385\n",
      "Epoch 1, batch 357, loss = 5.9141669273376465\n",
      "Epoch 1, batch 358, loss = 5.733241081237793\n",
      "Epoch 1, batch 359, loss = 5.710529327392578\n",
      "Epoch 1, batch 360, loss = 5.771719455718994\n",
      "Epoch 1, batch 361, loss = 5.905411243438721\n",
      "Epoch 1, batch 362, loss = 5.7856574058532715\n",
      "Epoch 1, batch 363, loss = 5.793687343597412\n",
      "Epoch 1, batch 364, loss = 5.7633514404296875\n",
      "Epoch 1, batch 365, loss = 5.766857147216797\n",
      "Epoch 1, batch 366, loss = 5.795375823974609\n",
      "Epoch 1, batch 367, loss = 5.81284236907959\n",
      "Epoch 1, batch 368, loss = 5.81872034072876\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, batch 369, loss = 5.718180179595947\n",
      "Epoch 1, batch 370, loss = 5.601260662078857\n",
      "Epoch 1, batch 371, loss = 5.7197041511535645\n",
      "Epoch 1, batch 372, loss = 5.823256015777588\n",
      "Epoch 1, batch 373, loss = 5.7688889503479\n",
      "Epoch 1, batch 374, loss = 5.759445667266846\n",
      "Epoch 1, batch 375, loss = 5.760220050811768\n",
      "Epoch 1, batch 376, loss = 5.808256149291992\n",
      "Epoch 1, batch 377, loss = 5.731132507324219\n",
      "Epoch 1, batch 378, loss = 5.822715759277344\n",
      "Epoch 1, batch 379, loss = 5.749683856964111\n",
      "Epoch 1, batch 380, loss = 5.616331577301025\n",
      "Epoch 1, batch 381, loss = 5.647697448730469\n",
      "Epoch 1, batch 382, loss = 5.710161209106445\n",
      "Epoch 1, batch 383, loss = 5.593897819519043\n",
      "Epoch 1, batch 384, loss = 5.785258769989014\n",
      "Epoch 1, batch 385, loss = 5.614696502685547\n",
      "Epoch 1, batch 386, loss = 5.685454368591309\n",
      "Epoch 1, batch 387, loss = 5.804566860198975\n",
      "Epoch 1, batch 388, loss = 5.8603835105896\n",
      "Epoch 1, batch 389, loss = 5.874810218811035\n",
      "Epoch 1, batch 390, loss = 5.625037670135498\n",
      "Epoch 1, batch 391, loss = 5.598730564117432\n",
      "Epoch 1, batch 392, loss = 5.681646347045898\n",
      "Epoch 1, batch 393, loss = 5.815720558166504\n",
      "Epoch 1, batch 394, loss = 5.482954025268555\n",
      "Epoch 1, batch 395, loss = 5.614114284515381\n",
      "Epoch 1, batch 396, loss = 5.784973621368408\n",
      "Epoch 1, batch 397, loss = 5.7460126876831055\n",
      "Epoch 1, batch 398, loss = 5.473519325256348\n",
      "Epoch 1, batch 399, loss = 5.639224529266357\n",
      "Epoch 1, batch 400, loss = 5.584371566772461\n",
      "Epoch 1, batch 401, loss = 5.695631504058838\n",
      "Epoch 1, batch 402, loss = 5.677269458770752\n",
      "Epoch 1, batch 403, loss = 5.784362316131592\n",
      "Epoch 1, batch 404, loss = 5.679079532623291\n",
      "Epoch 1, batch 405, loss = 5.734042644500732\n",
      "Epoch 1, batch 406, loss = 5.768953800201416\n",
      "Epoch 1, batch 407, loss = 5.678347110748291\n",
      "Epoch 1, batch 408, loss = 5.72235631942749\n",
      "Epoch 1, batch 409, loss = 5.798699855804443\n",
      "Epoch 1, batch 410, loss = 5.804012298583984\n",
      "Epoch 1, batch 411, loss = 5.667600154876709\n",
      "Epoch 1, batch 412, loss = 5.684574127197266\n",
      "Epoch 1, batch 413, loss = 5.824923992156982\n",
      "Epoch 1, batch 414, loss = 5.579713344573975\n",
      "Epoch 1, batch 415, loss = 5.898037910461426\n",
      "Epoch 1, batch 416, loss = 6.1054582595825195\n",
      "Epoch 1, batch 417, loss = 5.951352596282959\n",
      "Epoch 1, batch 418, loss = 5.916263103485107\n",
      "Epoch 1, batch 419, loss = 5.931575298309326\n",
      "Epoch 1, batch 420, loss = 5.872847557067871\n",
      "Epoch 1, batch 421, loss = 6.110852241516113\n",
      "Epoch 1, batch 422, loss = 5.93704080581665\n",
      "Epoch 1, batch 423, loss = 5.940624237060547\n",
      "Epoch 1, batch 424, loss = 5.9535980224609375\n",
      "Epoch 1, batch 425, loss = 6.0653533935546875\n",
      "Epoch 1, batch 426, loss = 5.8658647537231445\n",
      "Epoch 1, batch 427, loss = 5.835251331329346\n",
      "Epoch 1, batch 428, loss = 6.039266109466553\n",
      "Epoch 1, batch 429, loss = 5.926408767700195\n",
      "Epoch 1, batch 430, loss = 6.030965328216553\n",
      "Epoch 1, batch 431, loss = 5.86535120010376\n",
      "Epoch 1, batch 432, loss = 5.648045063018799\n",
      "Epoch 1, batch 433, loss = 5.879519462585449\n",
      "Epoch 1, batch 434, loss = 6.037841320037842\n",
      "Epoch 1, batch 435, loss = 5.9262261390686035\n",
      "Epoch 1, batch 436, loss = 5.770248889923096\n",
      "Epoch 1, batch 437, loss = 5.791356086730957\n",
      "Epoch 1, batch 438, loss = 5.677225112915039\n",
      "Epoch 1, batch 439, loss = 5.645290374755859\n",
      "Epoch 1, batch 440, loss = 5.739805698394775\n",
      "Epoch 1, batch 441, loss = 5.656192779541016\n",
      "Epoch 1, batch 442, loss = 5.603938579559326\n",
      "Epoch 1, batch 443, loss = 5.744000434875488\n",
      "Epoch 1, batch 444, loss = 5.90254020690918\n",
      "Epoch 1, batch 445, loss = 5.8788838386535645\n",
      "Epoch 1, batch 446, loss = 5.955267429351807\n",
      "Epoch 1, batch 447, loss = 5.714653968811035\n",
      "Epoch 1, batch 448, loss = 5.722128868103027\n",
      "Epoch 1, batch 449, loss = 5.729386329650879\n",
      "Epoch 1, batch 450, loss = 6.091810703277588\n",
      "Epoch 1, batch 451, loss = 5.831852436065674\n",
      "Epoch 1, batch 452, loss = 5.865450859069824\n",
      "Epoch 1, batch 453, loss = 5.853305816650391\n",
      "Epoch 1, batch 454, loss = 5.863280296325684\n",
      "Epoch 1, batch 455, loss = 5.893718719482422\n",
      "Epoch 1, batch 456, loss = 5.692233085632324\n",
      "Epoch 1, batch 457, loss = 5.553279876708984\n",
      "Epoch 1, batch 458, loss = 5.796024322509766\n",
      "Epoch 1, batch 459, loss = 5.972428321838379\n",
      "Epoch 1, batch 460, loss = 5.63971471786499\n",
      "Epoch 1, batch 461, loss = 5.856236457824707\n",
      "Epoch 1, batch 462, loss = 5.8784918785095215\n",
      "Epoch 1, batch 463, loss = 5.721277713775635\n",
      "Epoch 1, batch 464, loss = 5.841032981872559\n",
      "Epoch 1, batch 465, loss = 5.692143440246582\n",
      "Epoch 1, batch 466, loss = 5.739692211151123\n",
      "Epoch 1, batch 467, loss = 5.732234001159668\n",
      "Epoch 1, batch 468, loss = 5.659448623657227\n",
      "Epoch 1, batch 469, loss = 5.683809757232666\n",
      "Epoch 1, batch 470, loss = 5.931675910949707\n",
      "Epoch 1, batch 471, loss = 5.8389811515808105\n",
      "Epoch 1, batch 472, loss = 5.784124374389648\n",
      "Epoch 1, batch 473, loss = 5.81387996673584\n",
      "Epoch 1, batch 474, loss = 5.861665725708008\n",
      "Epoch 1, batch 475, loss = 5.926699161529541\n",
      "Epoch 1, batch 476, loss = 5.734994411468506\n",
      "Epoch 1, batch 477, loss = 5.906368732452393\n",
      "Epoch 1, batch 478, loss = 5.7739434242248535\n",
      "Epoch 1, batch 479, loss = 6.0027570724487305\n",
      "Epoch 1, batch 480, loss = 5.874807834625244\n",
      "Epoch 1, batch 481, loss = 5.926380634307861\n",
      "Epoch 1, batch 482, loss = 5.699118614196777\n",
      "Epoch 1, batch 483, loss = 5.6193013191223145\n",
      "Epoch 1, batch 484, loss = 5.657499313354492\n",
      "Epoch 1, batch 485, loss = 5.757538795471191\n",
      "Epoch 1, batch 486, loss = 5.7678327560424805\n",
      "Epoch 1, batch 487, loss = 5.636505126953125\n",
      "Epoch 1, batch 488, loss = 5.65002965927124\n",
      "Epoch 1, batch 489, loss = 5.485976219177246\n",
      "Epoch 1, batch 490, loss = 5.668829441070557\n",
      "Epoch 1, batch 491, loss = 5.733328342437744\n",
      "Epoch 1, batch 492, loss = 5.714441776275635\n",
      "Epoch 1, batch 493, loss = 5.722127914428711\n",
      "Epoch 1, batch 494, loss = 5.726485729217529\n",
      "Epoch 1, batch 495, loss = 5.71821928024292\n",
      "Epoch 1, batch 496, loss = 5.674420356750488\n",
      "Epoch 1, batch 497, loss = 5.78180456161499\n",
      "Epoch 1, batch 498, loss = 5.8635149002075195\n",
      "Epoch 1, batch 499, loss = 5.6859307289123535\n",
      "Epoch 1, batch 500, loss = 5.7891526222229\n",
      "Epoch 1, batch 501, loss = 5.6904520988464355\n",
      "Epoch 1, batch 502, loss = 5.801760673522949\n",
      "Epoch 1, batch 503, loss = 5.701784610748291\n",
      "Epoch 1, batch 504, loss = 5.6380181312561035\n",
      "Epoch 1, batch 505, loss = 5.719338893890381\n",
      "Epoch 1, batch 506, loss = 5.813491344451904\n",
      "Epoch 1, batch 507, loss = 5.727478981018066\n",
      "Epoch 1, batch 508, loss = 5.734790802001953\n",
      "Epoch 1, batch 509, loss = 5.789098739624023\n",
      "Epoch 1, batch 510, loss = 5.826472282409668\n",
      "Epoch 1, batch 511, loss = 5.768793106079102\n",
      "Epoch 1, batch 512, loss = 5.837489604949951\n",
      "Epoch 1, batch 513, loss = 5.757370948791504\n",
      "Epoch 1, batch 514, loss = 5.660844326019287\n",
      "Epoch 1, batch 515, loss = 5.818892002105713\n",
      "Epoch 1, batch 516, loss = 5.765498161315918\n",
      "Epoch 1, batch 517, loss = 5.852124214172363\n",
      "Epoch 1, batch 518, loss = 5.779512882232666\n",
      "Epoch 1, batch 519, loss = 5.962146759033203\n",
      "Epoch 1, batch 520, loss = 6.10180139541626\n",
      "Epoch 1, batch 521, loss = 5.965611457824707\n",
      "Epoch 1, batch 522, loss = 5.856917858123779\n",
      "Epoch 1, batch 523, loss = 5.760396480560303\n",
      "Epoch 1, batch 524, loss = 5.893953800201416\n",
      "Epoch 1, batch 525, loss = 5.757664203643799\n",
      "Epoch 1, batch 526, loss = 5.5989508628845215\n",
      "Epoch 1, batch 527, loss = 5.7174882888793945\n",
      "Epoch 1, batch 528, loss = 5.686048984527588\n",
      "Epoch 1, batch 529, loss = 5.735574722290039\n",
      "Epoch 1, batch 530, loss = 5.689817905426025\n",
      "Epoch 1, batch 531, loss = 5.563174247741699\n",
      "Epoch 1, batch 532, loss = 5.605408191680908\n",
      "Epoch 1, batch 533, loss = 5.7979607582092285\n",
      "Epoch 1, batch 534, loss = 5.797238826751709\n",
      "Epoch 1, batch 535, loss = 5.916505813598633\n",
      "Epoch 1, batch 536, loss = 5.770909309387207\n",
      "Epoch 1, batch 537, loss = 5.9109625816345215\n",
      "Epoch 1, batch 538, loss = 5.753614902496338\n",
      "Epoch 1, batch 539, loss = 5.759340286254883\n",
      "Epoch 1, batch 540, loss = 5.959043979644775\n",
      "Epoch 1, batch 541, loss = 6.109402656555176\n",
      "Epoch 1, batch 542, loss = 5.928667068481445\n",
      "Epoch 1, batch 543, loss = 5.667116165161133\n",
      "Epoch 1, batch 544, loss = 5.900174617767334\n",
      "Epoch 1, batch 545, loss = 5.954863548278809\n",
      "Epoch 1, batch 546, loss = 5.802860260009766\n",
      "Epoch 1, batch 547, loss = 5.697549819946289\n",
      "Epoch 1, batch 548, loss = 5.671103000640869\n",
      "Epoch 1, batch 549, loss = 5.700782775878906\n",
      "Epoch 1, batch 550, loss = 5.887068271636963\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, batch 551, loss = 5.782552719116211\n",
      "Epoch 1, batch 552, loss = 5.710867881774902\n",
      "Epoch 1, batch 553, loss = 5.805628299713135\n",
      "Epoch 1, batch 554, loss = 5.732217788696289\n",
      "Epoch 1, batch 555, loss = 5.748960494995117\n",
      "Epoch 1, batch 556, loss = 5.734989643096924\n",
      "Epoch 1, batch 557, loss = 5.839034080505371\n",
      "Epoch 1, batch 558, loss = 5.5739359855651855\n",
      "Epoch 1, batch 559, loss = 5.670580863952637\n",
      "Epoch 1, batch 560, loss = 5.726480007171631\n",
      "Epoch 1, batch 561, loss = 5.644850730895996\n",
      "Epoch 1, batch 562, loss = 5.847506046295166\n",
      "Epoch 1, batch 563, loss = 5.873259544372559\n",
      "Epoch 1, batch 564, loss = 5.7432780265808105\n",
      "Epoch 1, batch 565, loss = 5.669466495513916\n",
      "Epoch 1, batch 566, loss = 5.665438175201416\n",
      "Epoch 1, batch 567, loss = 5.68215274810791\n",
      "Epoch 1, batch 568, loss = 5.768172740936279\n",
      "Epoch 1, batch 569, loss = 5.722362995147705\n",
      "Epoch 1, batch 570, loss = 5.872249603271484\n",
      "Epoch 1, batch 571, loss = 5.720802307128906\n",
      "Epoch 1, batch 572, loss = 5.910502910614014\n",
      "Epoch 1, batch 573, loss = 5.737696170806885\n",
      "Epoch 1, batch 574, loss = 5.5389275550842285\n",
      "Epoch 1, batch 575, loss = 5.800199508666992\n",
      "Epoch 1, batch 576, loss = 5.782782077789307\n",
      "Epoch 1, batch 577, loss = 5.838522911071777\n",
      "Epoch 1, batch 578, loss = 5.817195892333984\n",
      "Epoch 1, batch 579, loss = 5.620659828186035\n",
      "Epoch 1, batch 580, loss = 5.699978351593018\n",
      "Epoch 1, batch 581, loss = 5.684169292449951\n",
      "Epoch 1, batch 582, loss = 5.797104358673096\n",
      "Epoch 1, batch 583, loss = 5.743701457977295\n",
      "Epoch 1, batch 584, loss = 5.6081671714782715\n",
      "Epoch 1, batch 585, loss = 5.704206943511963\n",
      "Epoch 1, batch 586, loss = 5.7314300537109375\n",
      "Epoch 1, batch 587, loss = 5.738792419433594\n",
      "Epoch 1, batch 588, loss = 5.593408107757568\n",
      "Epoch 1, batch 589, loss = 5.645720481872559\n",
      "Epoch 1, batch 590, loss = 5.813114643096924\n",
      "Epoch 1, batch 591, loss = 5.6715545654296875\n",
      "Epoch 1, batch 592, loss = 5.658421039581299\n",
      "Epoch 1, batch 593, loss = 5.701937198638916\n",
      "Epoch 1, batch 594, loss = 5.588038921356201\n",
      "Epoch 1, batch 595, loss = 5.616605281829834\n",
      "Epoch 1, batch 596, loss = 5.579592227935791\n",
      "Epoch 1, batch 597, loss = 5.6222243309021\n",
      "Epoch 1, batch 598, loss = 5.557844638824463\n",
      "Epoch 1, batch 599, loss = 5.658885955810547\n",
      "Epoch 1, batch 600, loss = 5.511838436126709\n",
      "Epoch 1, batch 601, loss = 5.807194709777832\n",
      "Epoch 1, batch 602, loss = 5.961572170257568\n",
      "Epoch 1, batch 603, loss = 5.777130126953125\n",
      "Epoch 1, batch 604, loss = 5.684271335601807\n",
      "Epoch 1, batch 605, loss = 5.489506721496582\n",
      "Epoch 1, batch 606, loss = 5.623359203338623\n",
      "Epoch 1, batch 607, loss = 5.6947221755981445\n",
      "Epoch 1, batch 608, loss = 5.548712730407715\n",
      "Epoch 1, batch 609, loss = 5.616627216339111\n",
      "Epoch 1, batch 610, loss = 5.757294654846191\n",
      "Epoch 1, batch 611, loss = 5.619335174560547\n",
      "Epoch 1, batch 612, loss = 5.544011116027832\n",
      "Epoch 1, batch 613, loss = 5.582396507263184\n",
      "Epoch 1, batch 614, loss = 5.711386680603027\n",
      "Epoch 1, batch 615, loss = 5.759702682495117\n",
      "Epoch 1, batch 616, loss = 5.656043529510498\n",
      "Epoch 1, batch 617, loss = 5.591188430786133\n",
      "Epoch 1, batch 618, loss = 5.663223743438721\n",
      "Epoch 1, batch 619, loss = 5.671368598937988\n",
      "Epoch 1, batch 620, loss = 5.790332317352295\n",
      "Epoch 1, batch 621, loss = 5.700263500213623\n",
      "Epoch 1, batch 622, loss = 5.589046001434326\n",
      "Epoch 1, batch 623, loss = 5.765342712402344\n",
      "Epoch 1, batch 624, loss = 5.588144302368164\n",
      "Epoch 1, batch 625, loss = 5.695377826690674\n",
      "Epoch 1, batch 626, loss = 5.777381896972656\n",
      "Epoch 1, batch 627, loss = 5.588493824005127\n",
      "Epoch 1, batch 628, loss = 5.913611888885498\n",
      "Epoch 1, batch 629, loss = 6.0629096031188965\n",
      "Epoch 1, batch 630, loss = 5.873468399047852\n",
      "Epoch 1, batch 631, loss = 5.956219673156738\n",
      "Epoch 1, batch 632, loss = 5.863337993621826\n",
      "Epoch 1, batch 633, loss = 5.952229022979736\n",
      "Epoch 1, batch 634, loss = 5.9626569747924805\n",
      "Epoch 1, batch 635, loss = 5.941614627838135\n",
      "Epoch 1, batch 636, loss = 5.922201633453369\n",
      "Epoch 1, batch 637, loss = 5.928845405578613\n",
      "Epoch 1, batch 638, loss = 5.850995063781738\n",
      "Epoch 1, batch 639, loss = 5.994082927703857\n",
      "Epoch 1, batch 640, loss = 5.890681743621826\n",
      "Epoch 1, batch 641, loss = 5.7465291023254395\n",
      "Epoch 1, batch 642, loss = 5.755117416381836\n",
      "Epoch 1, batch 643, loss = 5.976949691772461\n",
      "Epoch 1, batch 644, loss = 5.993315696716309\n",
      "Epoch 1, batch 645, loss = 5.74271821975708\n",
      "Epoch 1, batch 646, loss = 5.71914005279541\n",
      "Epoch 1, batch 647, loss = 5.605273723602295\n",
      "Epoch 1, batch 648, loss = 5.620982646942139\n",
      "Epoch 1, batch 649, loss = 5.578774929046631\n",
      "Epoch 1, batch 650, loss = 5.699271202087402\n",
      "Epoch 1, batch 651, loss = 5.734760284423828\n",
      "Epoch 1, batch 652, loss = 5.650607585906982\n",
      "Epoch 1, batch 653, loss = 5.951932907104492\n",
      "Epoch 1, batch 654, loss = 5.754115104675293\n",
      "Epoch 1, batch 655, loss = 5.669417858123779\n",
      "Epoch 1, batch 656, loss = 5.7390289306640625\n",
      "Epoch 1, batch 657, loss = 6.083587169647217\n",
      "Epoch 1, batch 658, loss = 5.963110446929932\n",
      "Epoch 1, batch 659, loss = 5.714231491088867\n",
      "Epoch 1, batch 660, loss = 6.0075764656066895\n",
      "Epoch 1, batch 661, loss = 5.805914878845215\n",
      "Epoch 1, batch 662, loss = 5.73337459564209\n",
      "Epoch 1, batch 663, loss = 5.631472110748291\n",
      "Epoch 1, batch 664, loss = 5.616338729858398\n",
      "Epoch 1, batch 665, loss = 5.8651299476623535\n",
      "Epoch 1, batch 666, loss = 5.847734451293945\n",
      "Epoch 1, batch 667, loss = 5.722603797912598\n",
      "Epoch 1, batch 668, loss = 5.934563636779785\n",
      "Epoch 1, batch 669, loss = 5.709191799163818\n",
      "Epoch 1, batch 670, loss = 5.82551908493042\n",
      "Epoch 1, batch 671, loss = 5.649841785430908\n",
      "Epoch 1, batch 672, loss = 5.824100017547607\n",
      "Epoch 1, batch 673, loss = 5.625834941864014\n",
      "Epoch 1, batch 674, loss = 5.664709091186523\n",
      "Epoch 1, batch 675, loss = 5.556601047515869\n",
      "Epoch 1, batch 676, loss = 6.0162787437438965\n",
      "Epoch 1, batch 677, loss = 5.834629058837891\n",
      "Epoch 1, batch 678, loss = 5.802309989929199\n",
      "Epoch 1, batch 679, loss = 5.7463788986206055\n",
      "Epoch 1, batch 680, loss = 5.793291091918945\n",
      "Epoch 1, batch 681, loss = 5.571911334991455\n",
      "Epoch 1, batch 682, loss = 5.706332206726074\n",
      "Epoch 1, batch 683, loss = 5.805545806884766\n",
      "Epoch 1, batch 684, loss = 5.884049892425537\n",
      "Epoch 1, batch 685, loss = 5.790641784667969\n",
      "Epoch 1, batch 686, loss = 5.940535068511963\n",
      "Epoch 1, batch 687, loss = 5.634720325469971\n",
      "Epoch 1, batch 688, loss = 5.713287830352783\n",
      "Epoch 1, batch 689, loss = 5.76217794418335\n",
      "Epoch 1, batch 690, loss = 5.80556583404541\n",
      "Epoch 1, batch 691, loss = 5.752912521362305\n",
      "Epoch 1, batch 692, loss = 5.602214336395264\n",
      "Epoch 1, batch 693, loss = 5.652125358581543\n",
      "Epoch 1, batch 694, loss = 5.698427677154541\n",
      "Epoch 1, batch 695, loss = 5.691012382507324\n",
      "Epoch 1, batch 696, loss = 5.804227352142334\n",
      "Epoch 1, batch 697, loss = 5.496569633483887\n",
      "Epoch 1, batch 698, loss = 5.5080952644348145\n",
      "Epoch 1, batch 699, loss = 5.777902126312256\n",
      "Epoch 1, batch 700, loss = 5.689123630523682\n",
      "Epoch 1, batch 701, loss = 5.668285846710205\n",
      "Epoch 1, batch 702, loss = 5.798329830169678\n",
      "Epoch 1, batch 703, loss = 5.700631141662598\n",
      "Epoch 1, batch 704, loss = 5.740572452545166\n",
      "Epoch 1, batch 705, loss = 5.734049320220947\n",
      "Epoch 1, batch 706, loss = 5.7940497398376465\n",
      "Epoch 1, batch 707, loss = 5.685908317565918\n",
      "Epoch 1, batch 708, loss = 5.6950860023498535\n",
      "Epoch 1, batch 709, loss = 5.677565097808838\n",
      "Epoch 1, batch 710, loss = 5.863018035888672\n",
      "Epoch 1, batch 711, loss = 5.689511775970459\n",
      "Epoch 1, batch 712, loss = 5.686572551727295\n",
      "Epoch 1, batch 713, loss = 5.70724630355835\n",
      "Epoch 1, batch 714, loss = 5.732054233551025\n",
      "Epoch 1, batch 715, loss = 5.898470878601074\n",
      "Epoch 1, batch 716, loss = 5.748829364776611\n",
      "Epoch 1, batch 717, loss = 5.8344526290893555\n",
      "Epoch 1, batch 718, loss = 5.680527210235596\n",
      "Epoch 1, batch 719, loss = 5.682737827301025\n",
      "Epoch 1, batch 720, loss = 5.803806304931641\n",
      "Epoch 1, batch 721, loss = 5.903720378875732\n",
      "Epoch 1, batch 722, loss = 5.786888122558594\n",
      "Epoch 1, batch 723, loss = 5.967435836791992\n",
      "Epoch 1, batch 724, loss = 5.74204683303833\n",
      "Epoch 1, batch 725, loss = 5.953481197357178\n",
      "Epoch 1, batch 726, loss = 5.844876289367676\n",
      "Epoch 1, batch 727, loss = 5.842974662780762\n",
      "Epoch 1, batch 728, loss = 5.640686511993408\n",
      "Epoch 1, batch 729, loss = 5.698513031005859\n",
      "Epoch 1, batch 730, loss = 5.518916606903076\n",
      "Epoch 1, batch 731, loss = 5.779311180114746\n",
      "Epoch 1, batch 732, loss = 5.7690629959106445\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, batch 733, loss = 5.59346866607666\n",
      "Epoch 1, batch 734, loss = 5.50395393371582\n",
      "Epoch 1, batch 735, loss = 5.612408638000488\n",
      "Epoch 1, batch 736, loss = 5.614148139953613\n",
      "Epoch 1, batch 737, loss = 5.9874725341796875\n",
      "Epoch 1, batch 738, loss = 5.7712626457214355\n",
      "Epoch 1, batch 739, loss = 5.830273151397705\n",
      "Epoch 1, batch 740, loss = 5.873953819274902\n",
      "Epoch 1, batch 741, loss = 5.836766719818115\n",
      "Epoch 1, batch 742, loss = 5.695374965667725\n",
      "Epoch 1, batch 743, loss = 5.83892822265625\n",
      "Epoch 1, batch 744, loss = 5.807259559631348\n",
      "Epoch 1, batch 745, loss = 5.911267280578613\n",
      "Epoch 1, batch 746, loss = 5.885354518890381\n",
      "Epoch 1, batch 747, loss = 5.861804485321045\n",
      "Epoch 1, batch 748, loss = 5.703033447265625\n",
      "Epoch 1, batch 749, loss = 5.851438045501709\n",
      "Epoch 1, batch 750, loss = 5.753206729888916\n",
      "Epoch 1, batch 751, loss = 5.743318557739258\n",
      "Epoch 1, batch 752, loss = 5.750457763671875\n",
      "Epoch 1, batch 753, loss = 5.745096683502197\n",
      "Epoch 1, batch 754, loss = 5.816944599151611\n",
      "Epoch 1, batch 755, loss = 5.763009071350098\n",
      "Epoch 1, batch 756, loss = 5.775843143463135\n",
      "Epoch 1, batch 757, loss = 5.793639659881592\n",
      "Epoch 1, batch 758, loss = 5.741065502166748\n",
      "Epoch 1, batch 759, loss = 5.8084611892700195\n",
      "Epoch 1, batch 760, loss = 5.767989635467529\n",
      "Epoch 1, batch 761, loss = 5.725557804107666\n",
      "Epoch 1, batch 762, loss = 5.749553203582764\n",
      "Epoch 1, batch 763, loss = 5.787182807922363\n",
      "Epoch 1, batch 764, loss = 5.6454691886901855\n",
      "Epoch 1, batch 765, loss = 5.7418904304504395\n",
      "Epoch 1, batch 766, loss = 5.8732218742370605\n",
      "Epoch 1, batch 767, loss = 5.717164039611816\n",
      "Epoch 1, batch 768, loss = 5.84312629699707\n",
      "Epoch 1, batch 769, loss = 5.7645416259765625\n",
      "Epoch 1, batch 770, loss = 5.735106468200684\n",
      "Epoch 1, batch 771, loss = 5.820807456970215\n",
      "Epoch 1, batch 772, loss = 5.814144611358643\n",
      "Epoch 1, batch 773, loss = 5.682369232177734\n",
      "Epoch 1, batch 774, loss = 5.915526390075684\n",
      "Epoch 1, batch 775, loss = 5.743122100830078\n",
      "Epoch 1, batch 776, loss = 5.833056449890137\n",
      "Epoch 1, batch 777, loss = 5.71085786819458\n",
      "Epoch 1, batch 778, loss = 5.732255458831787\n",
      "Epoch 1, batch 779, loss = 5.75808048248291\n",
      "Epoch 1, batch 780, loss = 5.847142219543457\n",
      "Epoch 1, batch 781, loss = 5.792923927307129\n",
      "Epoch 1, batch 782, loss = 5.627287864685059\n",
      "Epoch 1, batch 783, loss = 5.738577365875244\n",
      "Epoch 1, batch 784, loss = 5.538455009460449\n",
      "Epoch 1, batch 785, loss = 5.627720355987549\n",
      "Epoch 1, batch 786, loss = 5.598923683166504\n",
      "Epoch 1, batch 787, loss = 5.682678699493408\n",
      "Epoch 1, batch 788, loss = 5.749187469482422\n",
      "Epoch 1, batch 789, loss = 5.663092136383057\n",
      "Epoch 1, batch 790, loss = 5.837423801422119\n",
      "Epoch 1, batch 791, loss = 5.714389324188232\n",
      "Epoch 1, batch 792, loss = 5.663576602935791\n",
      "Epoch 1, batch 793, loss = 5.6539459228515625\n",
      "Epoch 1, batch 794, loss = 5.500041961669922\n",
      "Epoch 1, batch 795, loss = 5.673874378204346\n",
      "Epoch 1, batch 796, loss = 5.472200393676758\n",
      "Epoch 1, batch 797, loss = 5.593642711639404\n",
      "Epoch 1, batch 798, loss = 5.748274803161621\n",
      "Epoch 1, batch 799, loss = 5.590421676635742\n",
      "Epoch 1, batch 800, loss = 5.808534145355225\n",
      "Epoch 1, batch 801, loss = 5.558929443359375\n",
      "Epoch 1, batch 802, loss = 5.644613742828369\n",
      "Epoch 1, batch 803, loss = 5.664209365844727\n",
      "Epoch 1, batch 804, loss = 5.743747711181641\n",
      "Epoch 1, batch 805, loss = 5.723966121673584\n",
      "Epoch 1, batch 806, loss = 5.749387264251709\n",
      "Epoch 1, batch 807, loss = 5.5131049156188965\n",
      "Epoch 1, batch 808, loss = 5.689218997955322\n",
      "Epoch 1, batch 809, loss = 5.7650532722473145\n",
      "Epoch 1, batch 810, loss = 5.802847862243652\n",
      "Epoch 1, batch 811, loss = 5.633853912353516\n",
      "Epoch 1, batch 812, loss = 5.6299543380737305\n",
      "Epoch 1, batch 813, loss = 5.708135604858398\n",
      "Epoch 1, batch 814, loss = 5.712179183959961\n",
      "Epoch 1, batch 815, loss = 5.684806823730469\n",
      "Epoch 1, batch 816, loss = 5.755135536193848\n",
      "Epoch 1, batch 817, loss = 5.683897972106934\n",
      "Epoch 1, batch 818, loss = 5.717201232910156\n",
      "Epoch 1, batch 819, loss = 5.737139701843262\n",
      "Epoch 1, batch 820, loss = 5.6455864906311035\n",
      "Epoch 1, batch 821, loss = 5.805569648742676\n",
      "Epoch 1, batch 822, loss = 5.651800155639648\n",
      "Epoch 1, batch 823, loss = 5.807088375091553\n",
      "Epoch 1, batch 824, loss = 5.896217346191406\n",
      "Epoch 1, batch 825, loss = 5.885521411895752\n",
      "Epoch 1, batch 826, loss = 5.8654561042785645\n",
      "Epoch 1, batch 827, loss = 5.830549240112305\n",
      "Epoch 1, batch 828, loss = 6.049815654754639\n",
      "Epoch 1, batch 829, loss = 5.930523872375488\n",
      "Epoch 1, batch 830, loss = 6.0181050300598145\n",
      "Epoch 1, batch 831, loss = 5.8502116203308105\n",
      "Epoch 1, batch 832, loss = 5.890260696411133\n",
      "Epoch 1, batch 833, loss = 5.838761329650879\n",
      "Epoch 1, batch 834, loss = 6.045482158660889\n",
      "Epoch 1, batch 835, loss = 5.667302131652832\n",
      "Epoch 1, batch 836, loss = 5.621579647064209\n",
      "Epoch 1, batch 837, loss = 5.78535795211792\n",
      "Epoch 1, batch 838, loss = 5.681461811065674\n",
      "Epoch 1, batch 839, loss = 5.585153102874756\n",
      "Epoch 1, batch 840, loss = 5.704012870788574\n",
      "Epoch 1, batch 841, loss = 5.755216121673584\n",
      "Epoch 1, batch 842, loss = 5.7651567459106445\n",
      "Epoch 1, batch 843, loss = 5.768009662628174\n",
      "Epoch 1, batch 844, loss = 5.688724517822266\n",
      "Epoch 1, batch 845, loss = 5.716695785522461\n",
      "Epoch 1, batch 846, loss = 6.140368461608887\n",
      "Epoch 1, batch 847, loss = 5.843611717224121\n",
      "Epoch 1, batch 848, loss = 5.824146747589111\n",
      "Epoch 1, batch 849, loss = 5.890166759490967\n",
      "Epoch 1, batch 850, loss = 5.811275005340576\n",
      "Epoch 1, batch 851, loss = 5.588216781616211\n",
      "Epoch 1, batch 852, loss = 5.768606185913086\n",
      "Epoch 1, batch 853, loss = 5.968156337738037\n",
      "Epoch 1, batch 854, loss = 5.6426777839660645\n",
      "Epoch 1, batch 855, loss = 5.712079048156738\n",
      "Epoch 1, batch 856, loss = 5.76031494140625\n",
      "Epoch 1, batch 857, loss = 5.790107727050781\n",
      "Epoch 1, batch 858, loss = 5.719707012176514\n",
      "Epoch 1, batch 859, loss = 5.748157978057861\n",
      "Epoch 1, batch 860, loss = 5.70911979675293\n",
      "Epoch 1, batch 861, loss = 5.584975242614746\n",
      "Epoch 1, batch 862, loss = 5.968244552612305\n",
      "Epoch 1, batch 863, loss = 5.776246547698975\n",
      "Epoch 1, batch 864, loss = 5.754767894744873\n",
      "Epoch 1, batch 865, loss = 5.779470443725586\n",
      "Epoch 1, batch 866, loss = 5.801753520965576\n",
      "Epoch 1, batch 867, loss = 5.806821823120117\n",
      "Epoch 1, batch 868, loss = 5.780074119567871\n",
      "Epoch 1, batch 869, loss = 5.894926071166992\n",
      "Epoch 1, batch 870, loss = 5.836796760559082\n",
      "Epoch 1, batch 871, loss = 5.7846903800964355\n",
      "Epoch 1, batch 872, loss = 5.437192440032959\n",
      "Epoch 1, batch 873, loss = 5.521434783935547\n",
      "Epoch 1, batch 874, loss = 5.685225963592529\n",
      "Epoch 1, batch 875, loss = 5.784322261810303\n",
      "Epoch 1, batch 876, loss = 5.6358962059021\n",
      "Epoch 1, batch 877, loss = 5.3843488693237305\n",
      "Epoch 1, batch 878, loss = 5.725917339324951\n",
      "Epoch 1, batch 879, loss = 5.677277565002441\n",
      "Epoch 1, batch 880, loss = 5.669907093048096\n",
      "Epoch 1, batch 881, loss = 5.637234210968018\n",
      "Epoch 1, batch 882, loss = 5.6751837730407715\n",
      "Epoch 1, batch 883, loss = 5.560552597045898\n",
      "Epoch 1, batch 884, loss = 5.73928165435791\n",
      "Epoch 1, batch 885, loss = 5.818417072296143\n",
      "Epoch 1, batch 886, loss = 5.766314506530762\n",
      "Epoch 1, batch 887, loss = 5.732086658477783\n",
      "Epoch 1, batch 888, loss = 5.747679233551025\n",
      "Epoch 1, batch 889, loss = 5.742952823638916\n",
      "Epoch 1, batch 890, loss = 5.645852088928223\n",
      "Epoch 1, batch 891, loss = 5.804867267608643\n",
      "Epoch 1, batch 892, loss = 5.774937629699707\n",
      "Epoch 1, batch 893, loss = 5.771936416625977\n",
      "Epoch 1, batch 894, loss = 5.722884178161621\n",
      "Epoch 1, batch 895, loss = 5.7681498527526855\n",
      "Epoch 1, batch 896, loss = 5.8259968757629395\n",
      "Epoch 1, batch 897, loss = 5.829549789428711\n",
      "Epoch 1, batch 898, loss = 5.7966227531433105\n",
      "Epoch 1, batch 899, loss = 5.714191913604736\n",
      "Epoch 1, batch 900, loss = 5.834606647491455\n",
      "Epoch 1, batch 901, loss = 5.869566440582275\n",
      "Epoch 1, batch 902, loss = 5.880064487457275\n",
      "Epoch 1, batch 903, loss = 5.800510406494141\n",
      "Epoch 1, batch 904, loss = 5.984127044677734\n",
      "Epoch 1, batch 905, loss = 5.8368000984191895\n",
      "Epoch 1, batch 906, loss = 5.643583297729492\n",
      "Epoch 1, batch 907, loss = 5.776790618896484\n"
     ]
    }
   ],
   "source": [
    "chatbot.train(session,\n",
    "              1, 256,\n",
    "              input_sentences_idx,\n",
    "              ground_truth_sentences_idx,\n",
    "              input_lengths,\n",
    "              ground_truth_lengths,\n",
    "              1e-4,\n",
    "              0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i [END]'"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatbot.get_reply(session, \"who are you\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.random_normal(tf.shape(100), 0, 1, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot evaluate tensor using `eval()`: No default session is registered. Use `with sess.as_default()` or pass an explicit session to `eval(session=sess)`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-aee1c62ab140>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\keshavpc\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36meval\u001b[1;34m(self, feed_dict, session)\u001b[0m\n\u001b[0;32m    711\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    712\u001b[0m     \"\"\"\n\u001b[1;32m--> 713\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_eval_using_default_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    715\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\keshavpc\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_eval_using_default_session\u001b[1;34m(tensors, feed_dict, graph, session)\u001b[0m\n\u001b[0;32m   5141\u001b[0m     \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_default_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5142\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msession\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5143\u001b[1;33m       raise ValueError(\"Cannot evaluate tensor using `eval()`: No default \"\n\u001b[0m\u001b[0;32m   5144\u001b[0m                        \u001b[1;34m\"session is registered. Use `with \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5145\u001b[0m                        \u001b[1;34m\"sess.as_default()` or pass an explicit session to \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot evaluate tensor using `eval()`: No default session is registered. Use `with sess.as_default()` or pass an explicit session to `eval(session=sess)`"
     ]
    }
   ],
   "source": [
    "x.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with sess.as_default():\n",
    "    a = (x.eval())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1680882"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
