{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\keshavpc\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\gensim\\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\keshavPC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import datasets\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 83097/83097 [00:03<00:00, 21669.50it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('can we make this quick roxanne korrine and andrew barrett are having an incredibly horrendous public break up on the quad again',\n",
       "  'well i thought wed start with pronunciation if thats okay with you'),\n",
       " ('well i thought wed start with pronunciation if thats okay with you',\n",
       "  'not the hacking and gagging and spitting part please'),\n",
       " ('not the hacking and gagging and spitting part please',\n",
       "  'okay then how bout we try out some french cuisine saturday night'),\n",
       " ('youre asking me out thats so cute whats your name again', 'forget it'),\n",
       " ('no no its my fault we didnt have a proper introduction', 'cameron')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_path = os.path.join(\"data\", \"cornell\")\n",
    "data = datasets.readCornellData(dataset_path, max_len=100000)\n",
    "data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading OpenSubtitles conversations in data\\opensubs/OpenSubtitles.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OpenSubtitles data files: 0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset_path = os.path.join(\"data\", \"opensubs/OpenSubtitles\")\n",
    "data = datasets.readOpensubsData(dataset_path, max_len=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "data = [[sentences[0].split(), sentences[1].split()]for sentences in data]\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [sentences for sentences in data if len(sentences[0]) >= 3]\n",
    "data = [sentences for sentences in data if len(sentences[0]) <= 20]\n",
    "data = [sentences for sentences in data if len(sentences[1]) >= 3]\n",
    "data = [sentences for sentences in data if len(sentences[1]) <= 20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv_embeddings = KeyedVectors.load_word2vec_format(\n",
    "    'data/GoogleNews-vectors-negative300.bin',\n",
    "    binary=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "word_count = defaultdict(int)\n",
    "for sentences in data:\n",
    "    for word in sentences[0]:\n",
    "        word_count[word] += 1\n",
    "    for word in sentences[1]:\n",
    "        word_count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc_threshold = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the lines with unknown words\n",
    "data_filtered = []\n",
    "for sentence1, sentence2 in data:\n",
    "    valid = True\n",
    "    for word in sentence1 + sentence2:\n",
    "        if word not in wv_embeddings or word_count[word] < wc_threshold:\n",
    "            valid = False\n",
    "            break\n",
    "    if valid:\n",
    "        data_filtered.append([sentence1, sentence2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "311236"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.sort(key=lambda x:(len(x[0]), len(x[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['thanks', 'very', 'much'], ['remember', 'your', 'bag']]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_set = set()\n",
    "for sentence1, sentence2 in data:\n",
    "    word_set |= set(sentence1)\n",
    "    word_set |= set(sentence2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_token = \"[START]\"\n",
    "end_token = \"[END]\"\n",
    "pad_token = \"[PAD]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_to_word = [start_token, end_token, pad_token] + [None] * len(word_set)\n",
    "word_to_idx = {start_token:0, end_token:1, pad_token:2}\n",
    "index = 3\n",
    "for word in word_set:\n",
    "    word_to_idx[word] = index\n",
    "    idx_to_word[index] = word\n",
    "    index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_idx, end_idx = 0, 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 300\n",
    "word_embeddings = np.zeros((len(word_set)+3, embedding_dim), dtype='float32')\n",
    "word_embeddings[0,:] = 0.\n",
    "word_embeddings[1,:] = 1.\n",
    "word_embeddings[2,:] = -1.\n",
    "for i in range(3, len(word_set)+3):\n",
    "    word_embeddings[i, :] = wv_embeddings[idx_to_word[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentences in data:\n",
    "    sentences[1].append(end_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_lengths = [len(sentences[0]) for sentences in data]\n",
    "ground_truth_lengths = [len(sentences[1]) for sentences in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add padding\n",
    "max_input_lengths = max(input_lengths)\n",
    "max_ground_truth_lengths = max(ground_truth_lengths)\n",
    "input_sentences = []\n",
    "ground_truth_sentences = []\n",
    "for sentences in data:\n",
    "    input_sentences.append(sentences[0] + [pad_token]*(max_input_lengths-len(sentences[0])))\n",
    "    ground_truth_sentences.append(sentences[1] + [pad_token]*(max_ground_truth_lengths-len(sentences[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sentences_idx = [[word_to_idx[word] for word in sentence] for sentence in input_sentences]\n",
    "ground_truth_sentences_idx = [[word_to_idx[word] for word in sentence] for sentence in ground_truth_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sentences_idx = np.array(input_sentences_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_sentences_idx = np.array(ground_truth_sentences_idx)\n",
    "input_lengths = np.array(input_lengths)\n",
    "ground_truth_lengths = np.array(ground_truth_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generator(batch_size,\n",
    "                    input_sentences_idx,\n",
    "                    ground_truth_sentences_idx,\n",
    "                    input_lengths,\n",
    "                    ground_truth_lengths):\n",
    "    index = 0\n",
    "    while index < len(input_sentences_idx):\n",
    "        batch_input_length = input_lengths[index:index+batch_size]\n",
    "        batch_input_sentences_idx = input_sentences_idx[index:index+batch_size, :batch_input_length.max()]\n",
    "        batch_ground_truth_length = ground_truth_lengths[index:index+batch_size]\n",
    "        batch_ground_truth_sentences_idx = ground_truth_sentences_idx[index:index+batch_size, :batch_ground_truth_length.max()]\n",
    "        yield (batch_input_sentences_idx, batch_input_length,\n",
    "              batch_ground_truth_sentences_idx, batch_ground_truth_length)\n",
    "        index += batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(word_set) + 3\n",
    "num_units = 512\n",
    "embedding_size = 300\n",
    "num_encoder_layers = 1\n",
    "num_decoder_layers = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_prepare(text):\n",
    "    \"\"\"Performs tokenization and simple preprocessing.\"\"\"\n",
    "    \n",
    "    replace_by_space_re = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "    good_symbols_re = re.compile('[^0-9a-z #+_]')\n",
    "\n",
    "    text = text.lower()\n",
    "    text = replace_by_space_re.sub(' ', text)\n",
    "    text = good_symbols_re.sub('', text)\n",
    "\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatBot:\n",
    "    def __init__(self):\n",
    "        self.declare_placeholders()\n",
    "        self.build_input_encoder()\n",
    "        self.build_decoder()\n",
    "        self.define_loss_and_train()\n",
    "    \n",
    "    def declare_placeholders(self):\n",
    "        \"\"\"Specifies placeholders for the model.\"\"\"\n",
    "        # Placeholders for input and its actual lengths.\n",
    "        self.input_batch = tf.placeholder(shape=(None, None), dtype=tf.int32, name='input_batch')\n",
    "        self.input_batch_lengths = tf.placeholder(shape=(None, ), dtype=tf.int32, name='input_batch_lengths')\n",
    "\n",
    "        # Placeholders for groundtruth and its actual lengths.\n",
    "        self.ground_truth = tf.placeholder(shape=(None, None), dtype=tf.int32, name='ground_truth')\n",
    "        self.ground_truth_lengths = tf.placeholder(shape=(None, ), dtype=tf.int32, name='ground_truth_lengths')\n",
    "\n",
    "        self.dropout_ph = tf.placeholder_with_default(tf.cast(1.0, tf.float32), shape=[])\n",
    "        self.learning_rate_ph = tf.placeholder(dtype=tf.float32, shape=[])\n",
    "\n",
    "    def build_input_encoder(self):\n",
    "        with tf.variable_scope('input_encoder') as input_encoder_scope:\n",
    "#             random_initializer = tf.random_uniform((vocab_size, embedding_size), -1.0, 1.0)\n",
    "#             self.embeddings = tf.Variable(initial_value=random_initializer, name='embeddings', dtype=tf.float32) \n",
    "            self.embeddings = tf.Variable(initial_value=word_embeddings, name='embeddings', dtype=tf.float32) \n",
    "\n",
    "            # Perform embeddings lookup for self.input_batch. \n",
    "            self.input_batch_embedded = tf.nn.embedding_lookup(self.embeddings, self.input_batch)\n",
    "            # Create encoder cells\n",
    "            rnn_layers = []\n",
    "            for i in range(num_encoder_layers-1):\n",
    "                with tf.variable_scope('input_encoder_rnn_layer' + str(i + 1)) as scope:\n",
    "                    cell = tf.nn.rnn_cell.GRUCell(num_units)\n",
    "                    cell = tf.nn.rnn_cell.DropoutWrapper(cell, input_keep_prob=self.dropout_ph, dtype=tf.float32)\n",
    "                    rnn_layers.append(cell)\n",
    "            with tf.variable_scope('input_encoder_rnn_layer' + str(num_encoder_layers)) as scope:\n",
    "                cell = tf.nn.rnn_cell.GRUCell(num_units)\n",
    "                cell = tf.nn.rnn_cell.DropoutWrapper(cell, input_keep_prob=self.dropout_ph, dtype=tf.float32)\n",
    "                rnn_layers.append(cell)\n",
    "            encoder_cell = tf.contrib.rnn.MultiRNNCell(rnn_layers)\n",
    "            self.input_encoder_outputs, self.final_input_encoder_state = tf.nn.dynamic_rnn(\n",
    "                encoder_cell,\n",
    "                self.input_batch_embedded,\n",
    "                sequence_length=self.input_batch_lengths,\n",
    "                dtype=tf.float32\n",
    "            )\n",
    "            self.final_input_encoder_state = self.final_input_encoder_state[-1]\n",
    "\n",
    "    def build_decoder(self):\n",
    "        batch_size = tf.shape(self.input_batch)[0]\n",
    "        start_tokens = tf.fill([batch_size], start_idx)\n",
    "        ground_truth_as_input = tf.concat([tf.expand_dims(start_tokens, 1), self.ground_truth], 1)\n",
    "        self.ground_truth_embedded = tf.nn.embedding_lookup(\n",
    "            self.embeddings, ground_truth_as_input)\n",
    "        train_helper = tf.contrib.seq2seq.TrainingHelper(self.ground_truth_embedded,\n",
    "                                                         self.ground_truth_lengths)\n",
    "        infer_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(self.embeddings, start_tokens, end_idx)\n",
    "\n",
    "        def decode(helper, scope, reuse=None):\n",
    "            \"\"\"Creates decoder and return the results of the decoding with a given helper.\"\"\"\n",
    "            with tf.variable_scope(scope, reuse=reuse):\n",
    "                # Create GRUCell with dropout. Do not forget to set the reuse flag properly.\n",
    "                rnn_layers = []\n",
    "                for i in range(num_decoder_layers-1):\n",
    "                    with tf.variable_scope('decoder_rnn_layer' + str(i + 1)) as scope:\n",
    "                        decoder_cell = tf.contrib.rnn.GRUCell(num_units=num_units, reuse=reuse)\n",
    "                        decoder_cell = tf.contrib.rnn.DropoutWrapper(decoder_cell, input_keep_prob=self.dropout_ph)\n",
    "                        rnn_layers.append(decoder_cell)\n",
    "                with tf.variable_scope('decoder_rnn_layer' + str(num_decoder_layers)) as scope:\n",
    "                    decoder_cell = tf.contrib.rnn.GRUCell(num_units=num_units, reuse=reuse)\n",
    "                    decoder_cell = tf.contrib.rnn.DropoutWrapper(decoder_cell, input_keep_prob=self.dropout_ph)\n",
    "                    rnn_layers.append(decoder_cell)\n",
    "                decoder_cell = tf.contrib.rnn.MultiRNNCell(rnn_layers)\n",
    "                # Create attention\n",
    "#                 attention_mechanism = tf.contrib.seq2seq.BahdanauAttention(\n",
    "#                     num_units=num_units, memory=tf.split(self.input_encoder_outputs, num_or_size_splits=2, axis=-1)[0],\n",
    "#                     memory_sequence_length=self.input_batch_lengths)\n",
    "#                 decoder_cell = tf.contrib.seq2seq.AttentionWrapper(\n",
    "#                     decoder_cell, attention_mechanism, attention_layer_size=num_units)\n",
    "                # Create a projection wrapper.\n",
    "                decoder_cell = tf.contrib.rnn.OutputProjectionWrapper(decoder_cell, vocab_size, reuse=reuse)\n",
    "                # Create BasicDecoder, pass the defined cell, a helper, and initial state.\n",
    "                # The initial state should be equal to the final state of the encoder!\n",
    "                decoder = tf.contrib.seq2seq.BasicDecoder(cell=decoder_cell, helper=helper, initial_state=decoder_cell.zero_state(\n",
    "                    dtype=tf.float32, batch_size=batch_size))\n",
    "\n",
    "                # The first returning argument of dynamic_decode contains two fields:\n",
    "                #   rnn_output (predicted logits)\n",
    "                #   sample_id (predictions)\n",
    "                outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(decoder=decoder, maximum_iterations=tf.reduce_max(self.ground_truth_lengths), \n",
    "                                                                  output_time_major=False, impute_finished=True)\n",
    "\n",
    "                return outputs\n",
    "\n",
    "        self.train_outputs = decode(train_helper, 'decode')\n",
    "        self.infer_outputs = decode(infer_helper, 'decode', reuse=True)\n",
    "        self.train_predictions = self.train_outputs.sample_id\n",
    "        self.infer_predictions = self.infer_outputs.sample_id\n",
    "\n",
    "    def define_loss_and_train(self):\n",
    "        weights = tf.cast(tf.sequence_mask(self.ground_truth_lengths), dtype=tf.float32)\n",
    "        self.loss = tf.contrib.seq2seq.sequence_loss(\n",
    "            self.train_outputs.rnn_output,\n",
    "            self.ground_truth,\n",
    "            weights\n",
    "        )\n",
    "        self.train_op = tf.contrib.layers.optimize_loss(\n",
    "            loss=self.loss,\n",
    "            optimizer='Adam',\n",
    "            learning_rate=self.learning_rate_ph,\n",
    "            clip_gradients=1.0,\n",
    "            global_step=tf.train.get_global_step()\n",
    "        )\n",
    "\n",
    "    def train_on_batch(self, session, X, X_seq_len, Y, Y_seq_len, learning_rate, dropout_keep_probability):\n",
    "        feed_dict = {\n",
    "            self.input_batch: X,\n",
    "            self.input_batch_lengths: X_seq_len,\n",
    "            self.ground_truth: Y,\n",
    "            self.ground_truth_lengths: Y_seq_len,\n",
    "            self.learning_rate_ph: learning_rate,\n",
    "            self.dropout_ph: dropout_keep_probability\n",
    "        }\n",
    "        loss, _ = session.run([\n",
    "            self.loss,\n",
    "            self.train_op], feed_dict=feed_dict)\n",
    "        return loss\n",
    "    \n",
    "    def get_reply(self, session, input_sentence):\n",
    "        input_sentence = text_prepare(input_sentence)\n",
    "        X = [[word_to_idx[word] if word in word_to_idx else start_idx for word in input_sentence]]\n",
    "        X = np.array(X)\n",
    "        feed_dict = {\n",
    "            self.input_batch: X,\n",
    "            self.input_batch_lengths: np.array([len(input_sentence)]),\n",
    "            self.ground_truth_lengths: np.array([15])\n",
    "        }\n",
    "        pred = session.run([self.infer_predictions], feed_dict=feed_dict)\n",
    "        return \" \".join([idx_to_word[index] for index in pred[0][0][:-1]])\n",
    "\n",
    "    def train(self, session, epochs, batch_size, input_sentences_idx, ground_truth_sentences_idx, input_lengths, ground_truth_lengths, learning_rate, dropout_keep_probability):\n",
    "        for i in range(epochs):\n",
    "            batch_num = 1\n",
    "            for (batch_input_sentences_idx,\n",
    "                 batch_input_length,\n",
    "                 batch_ground_truth_sentences_idx,\n",
    "                 batch_ground_truth_length) in batch_generator(\n",
    "                batch_size, input_sentences_idx, ground_truth_sentences_idx,\n",
    "                input_lengths, ground_truth_lengths):\n",
    "                loss = self.train_on_batch(\n",
    "                    session,\n",
    "                    batch_input_sentences_idx,\n",
    "                    batch_input_length,\n",
    "                    batch_ground_truth_sentences_idx,\n",
    "                    batch_ground_truth_length,\n",
    "                    learning_rate,\n",
    "                    dropout_keep_probability\n",
    "                )\n",
    "                print(\"Epoch {i}, batch {batch}, loss = {loss}\".format(i=i+1, batch=batch_num, loss=loss))\n",
    "                batch_num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatbot = ChatBot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = tf.Session()\n",
    "session.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, batch 1, loss = 4.989370822906494\n",
      "Epoch 1, batch 2, loss = 5.199731826782227\n",
      "Epoch 1, batch 3, loss = 5.3325066566467285\n",
      "Epoch 1, batch 4, loss = 5.551334381103516\n",
      "Epoch 1, batch 5, loss = 5.278797626495361\n",
      "Epoch 1, batch 6, loss = 4.951613426208496\n",
      "Epoch 1, batch 7, loss = 5.352148056030273\n",
      "Epoch 1, batch 8, loss = 5.071322441101074\n",
      "Epoch 1, batch 9, loss = 5.054070472717285\n",
      "Epoch 1, batch 10, loss = 4.929233551025391\n",
      "Epoch 1, batch 11, loss = 5.217537879943848\n",
      "Epoch 1, batch 12, loss = 5.460282802581787\n",
      "Epoch 1, batch 13, loss = 5.179758548736572\n",
      "Epoch 1, batch 14, loss = 5.081521987915039\n",
      "Epoch 1, batch 15, loss = 4.866884231567383\n",
      "Epoch 1, batch 16, loss = 5.13714075088501\n",
      "Epoch 1, batch 17, loss = 4.953582763671875\n",
      "Epoch 1, batch 18, loss = 4.696691036224365\n",
      "Epoch 1, batch 19, loss = 4.883766174316406\n",
      "Epoch 1, batch 20, loss = 4.88119649887085\n",
      "Epoch 1, batch 21, loss = 4.938894271850586\n",
      "Epoch 1, batch 22, loss = 5.012141227722168\n",
      "Epoch 1, batch 23, loss = 5.122858047485352\n",
      "Epoch 1, batch 24, loss = 4.925023555755615\n",
      "Epoch 1, batch 25, loss = 4.711763381958008\n",
      "Epoch 1, batch 26, loss = 4.939277648925781\n",
      "Epoch 1, batch 27, loss = 4.784731864929199\n",
      "Epoch 1, batch 28, loss = 4.834691047668457\n",
      "Epoch 1, batch 29, loss = 4.684488773345947\n",
      "Epoch 1, batch 30, loss = 4.900250434875488\n",
      "Epoch 1, batch 31, loss = 4.898151397705078\n",
      "Epoch 1, batch 32, loss = 4.834170341491699\n",
      "Epoch 1, batch 33, loss = 4.832101345062256\n",
      "Epoch 1, batch 34, loss = 5.12945032119751\n",
      "Epoch 1, batch 35, loss = 4.806839942932129\n",
      "Epoch 1, batch 36, loss = 4.71171236038208\n",
      "Epoch 1, batch 37, loss = 4.981331825256348\n",
      "Epoch 1, batch 38, loss = 4.849645614624023\n",
      "Epoch 1, batch 39, loss = 4.54820442199707\n",
      "Epoch 1, batch 40, loss = 4.853941917419434\n",
      "Epoch 1, batch 41, loss = 4.946778297424316\n",
      "Epoch 1, batch 42, loss = 4.7496657371521\n",
      "Epoch 1, batch 43, loss = 4.746397495269775\n",
      "Epoch 1, batch 44, loss = 4.502745628356934\n",
      "Epoch 1, batch 45, loss = 4.992262840270996\n",
      "Epoch 1, batch 46, loss = 4.941453456878662\n",
      "Epoch 1, batch 47, loss = 4.746927738189697\n",
      "Epoch 1, batch 48, loss = 4.689781665802002\n",
      "Epoch 1, batch 49, loss = 4.759160041809082\n",
      "Epoch 1, batch 50, loss = 4.768486499786377\n",
      "Epoch 1, batch 51, loss = 4.6784515380859375\n",
      "Epoch 1, batch 52, loss = 4.49839448928833\n",
      "Epoch 1, batch 53, loss = 4.519160747528076\n",
      "Epoch 1, batch 54, loss = 4.595417499542236\n",
      "Epoch 1, batch 55, loss = 4.503550052642822\n",
      "Epoch 1, batch 56, loss = 4.445274829864502\n",
      "Epoch 1, batch 57, loss = 4.530104637145996\n",
      "Epoch 1, batch 58, loss = 4.649898529052734\n",
      "Epoch 1, batch 59, loss = 4.707198619842529\n",
      "Epoch 1, batch 60, loss = 5.00359582901001\n",
      "Epoch 1, batch 61, loss = 4.9436445236206055\n",
      "Epoch 1, batch 62, loss = 4.8501362800598145\n",
      "Epoch 1, batch 63, loss = 4.835393905639648\n",
      "Epoch 1, batch 64, loss = 4.880334854125977\n",
      "Epoch 1, batch 65, loss = 4.652166843414307\n",
      "Epoch 1, batch 66, loss = 4.7565436363220215\n",
      "Epoch 1, batch 67, loss = 4.787109375\n",
      "Epoch 1, batch 68, loss = 4.825135707855225\n",
      "Epoch 1, batch 69, loss = 4.719604969024658\n",
      "Epoch 1, batch 70, loss = 4.671570301055908\n",
      "Epoch 1, batch 71, loss = 4.82020378112793\n",
      "Epoch 1, batch 72, loss = 4.6229424476623535\n",
      "Epoch 1, batch 73, loss = 4.709103107452393\n",
      "Epoch 1, batch 74, loss = 4.711167812347412\n",
      "Epoch 1, batch 75, loss = 4.6039605140686035\n",
      "Epoch 1, batch 76, loss = 4.69760799407959\n",
      "Epoch 1, batch 77, loss = 4.586282730102539\n",
      "Epoch 1, batch 78, loss = 4.824037075042725\n",
      "Epoch 1, batch 79, loss = 4.586305141448975\n",
      "Epoch 1, batch 80, loss = 4.574509143829346\n",
      "Epoch 1, batch 81, loss = 4.691023826599121\n",
      "Epoch 1, batch 82, loss = 4.8600850105285645\n",
      "Epoch 1, batch 83, loss = 4.915111541748047\n",
      "Epoch 1, batch 84, loss = 4.947962284088135\n",
      "Epoch 1, batch 85, loss = 4.89852237701416\n",
      "Epoch 1, batch 86, loss = 5.085230350494385\n",
      "Epoch 1, batch 87, loss = 4.756126880645752\n",
      "Epoch 1, batch 88, loss = 4.913383483886719\n",
      "Epoch 1, batch 89, loss = 4.961888790130615\n",
      "Epoch 1, batch 90, loss = 4.876793384552002\n",
      "Epoch 1, batch 91, loss = 4.916464805603027\n",
      "Epoch 1, batch 92, loss = 4.933601379394531\n",
      "Epoch 1, batch 93, loss = 4.837492942810059\n",
      "Epoch 1, batch 94, loss = 5.00907039642334\n",
      "Epoch 1, batch 95, loss = 4.814461708068848\n",
      "Epoch 1, batch 96, loss = 5.044862270355225\n",
      "Epoch 1, batch 97, loss = 4.6407470703125\n",
      "Epoch 1, batch 98, loss = 4.778493404388428\n",
      "Epoch 1, batch 99, loss = 4.768943786621094\n",
      "Epoch 1, batch 100, loss = 4.830617427825928\n",
      "Epoch 1, batch 101, loss = 4.868198394775391\n",
      "Epoch 1, batch 102, loss = 4.856287002563477\n",
      "Epoch 1, batch 103, loss = 5.02353572845459\n",
      "Epoch 1, batch 104, loss = 4.946033477783203\n",
      "Epoch 1, batch 105, loss = 4.918272018432617\n",
      "Epoch 1, batch 106, loss = 4.881450653076172\n",
      "Epoch 1, batch 107, loss = 4.960781574249268\n",
      "Epoch 1, batch 108, loss = 5.078516960144043\n",
      "Epoch 1, batch 109, loss = 4.946588039398193\n",
      "Epoch 1, batch 110, loss = 4.92941427230835\n",
      "Epoch 1, batch 111, loss = 4.912771224975586\n",
      "Epoch 1, batch 112, loss = 5.006475448608398\n",
      "Epoch 1, batch 113, loss = 4.8215227127075195\n",
      "Epoch 1, batch 114, loss = 4.904938220977783\n",
      "Epoch 1, batch 115, loss = 4.880819797515869\n",
      "Epoch 1, batch 116, loss = 4.824901580810547\n",
      "Epoch 1, batch 117, loss = 5.060231685638428\n",
      "Epoch 1, batch 118, loss = 5.112893581390381\n",
      "Epoch 1, batch 119, loss = 5.146934509277344\n",
      "Epoch 1, batch 120, loss = 4.908746719360352\n",
      "Epoch 1, batch 121, loss = 4.925539016723633\n",
      "Epoch 1, batch 122, loss = 4.94565486907959\n",
      "Epoch 1, batch 123, loss = 4.889399528503418\n",
      "Epoch 1, batch 124, loss = 5.039644241333008\n",
      "Epoch 1, batch 125, loss = 4.951529502868652\n",
      "Epoch 1, batch 126, loss = 4.916803359985352\n",
      "Epoch 1, batch 127, loss = 4.972268104553223\n",
      "Epoch 1, batch 128, loss = 4.941678047180176\n",
      "Epoch 1, batch 129, loss = 5.033145904541016\n",
      "Epoch 1, batch 130, loss = 5.134787559509277\n",
      "Epoch 1, batch 131, loss = 4.965265274047852\n",
      "Epoch 1, batch 132, loss = 5.03384256362915\n",
      "Epoch 1, batch 133, loss = 4.9931840896606445\n",
      "Epoch 1, batch 134, loss = 5.0269622802734375\n",
      "Epoch 1, batch 135, loss = 4.993319511413574\n",
      "Epoch 1, batch 136, loss = 5.080515384674072\n",
      "Epoch 1, batch 137, loss = 5.016310214996338\n",
      "Epoch 1, batch 138, loss = 4.945196628570557\n",
      "Epoch 1, batch 139, loss = 5.02200174331665\n",
      "Epoch 1, batch 140, loss = 4.9149394035339355\n",
      "Epoch 1, batch 141, loss = 5.074034214019775\n",
      "Epoch 1, batch 142, loss = 5.104576110839844\n",
      "Epoch 1, batch 143, loss = 4.96044921875\n",
      "Epoch 1, batch 144, loss = 5.0751423835754395\n",
      "Epoch 1, batch 145, loss = 5.059704303741455\n",
      "Epoch 1, batch 146, loss = 5.145715236663818\n",
      "Epoch 1, batch 147, loss = 5.013487339019775\n",
      "Epoch 1, batch 148, loss = 5.093033313751221\n",
      "Epoch 1, batch 149, loss = 5.008358478546143\n",
      "Epoch 1, batch 150, loss = 5.083247661590576\n",
      "Epoch 1, batch 151, loss = 4.993908882141113\n",
      "Epoch 1, batch 152, loss = 4.961491107940674\n",
      "Epoch 1, batch 153, loss = 5.106056213378906\n",
      "Epoch 1, batch 154, loss = 5.012632369995117\n",
      "Epoch 1, batch 155, loss = 4.9919257164001465\n",
      "Epoch 1, batch 156, loss = 5.066617965698242\n",
      "Epoch 1, batch 157, loss = 5.073725700378418\n",
      "Epoch 1, batch 158, loss = 5.151858329772949\n",
      "Epoch 1, batch 159, loss = 4.964326858520508\n",
      "Epoch 1, batch 160, loss = 5.1934356689453125\n",
      "Epoch 1, batch 161, loss = 5.251167297363281\n",
      "Epoch 1, batch 162, loss = 5.421749114990234\n",
      "Epoch 1, batch 163, loss = 4.94801139831543\n",
      "Epoch 1, batch 164, loss = 5.004580497741699\n",
      "Epoch 1, batch 165, loss = 5.124362945556641\n",
      "Epoch 1, batch 166, loss = 5.200570583343506\n",
      "Epoch 1, batch 167, loss = 5.012045383453369\n",
      "Epoch 1, batch 168, loss = 4.8625898361206055\n",
      "Epoch 1, batch 169, loss = 4.971963405609131\n",
      "Epoch 1, batch 170, loss = 5.147098541259766\n",
      "Epoch 1, batch 171, loss = 5.121284484863281\n",
      "Epoch 1, batch 172, loss = 5.087601661682129\n",
      "Epoch 1, batch 173, loss = 5.097681522369385\n",
      "Epoch 1, batch 174, loss = 4.997012615203857\n",
      "Epoch 1, batch 175, loss = 5.117785930633545\n",
      "Epoch 1, batch 176, loss = 4.975593566894531\n",
      "Epoch 1, batch 177, loss = 4.810674667358398\n",
      "Epoch 1, batch 178, loss = 4.973763465881348\n",
      "Epoch 1, batch 179, loss = 4.802214622497559\n",
      "Epoch 1, batch 180, loss = 4.700136184692383\n",
      "Epoch 1, batch 181, loss = 4.793894290924072\n",
      "Epoch 1, batch 182, loss = 4.768996238708496\n",
      "Epoch 1, batch 183, loss = 5.049505233764648\n",
      "Epoch 1, batch 184, loss = 4.8286285400390625\n",
      "Epoch 1, batch 185, loss = 4.695929527282715\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, batch 186, loss = 5.247554302215576\n",
      "Epoch 1, batch 187, loss = 5.122101783752441\n",
      "Epoch 1, batch 188, loss = 4.902321815490723\n",
      "Epoch 1, batch 189, loss = 4.837375640869141\n",
      "Epoch 1, batch 190, loss = 4.730188369750977\n",
      "Epoch 1, batch 191, loss = 4.996434211730957\n",
      "Epoch 1, batch 192, loss = 4.938672065734863\n",
      "Epoch 1, batch 193, loss = 4.94050407409668\n",
      "Epoch 1, batch 194, loss = 4.908953666687012\n",
      "Epoch 1, batch 195, loss = 5.023035526275635\n",
      "Epoch 1, batch 196, loss = 5.020195007324219\n",
      "Epoch 1, batch 197, loss = 4.949326515197754\n",
      "Epoch 1, batch 198, loss = 5.151440143585205\n",
      "Epoch 1, batch 199, loss = 4.9338297843933105\n",
      "Epoch 1, batch 200, loss = 4.577725410461426\n",
      "Epoch 1, batch 201, loss = 4.757533073425293\n",
      "Epoch 1, batch 202, loss = 4.9821295738220215\n",
      "Epoch 1, batch 203, loss = 4.77232027053833\n",
      "Epoch 1, batch 204, loss = 4.8319807052612305\n",
      "Epoch 1, batch 205, loss = 4.704316139221191\n",
      "Epoch 1, batch 206, loss = 4.652942180633545\n",
      "Epoch 1, batch 207, loss = 4.569149971008301\n",
      "Epoch 1, batch 208, loss = 4.583733558654785\n",
      "Epoch 1, batch 209, loss = 4.778688430786133\n",
      "Epoch 1, batch 210, loss = 4.785980224609375\n",
      "Epoch 1, batch 211, loss = 4.669280052185059\n",
      "Epoch 1, batch 212, loss = 4.414385795593262\n",
      "Epoch 1, batch 213, loss = 4.843863010406494\n",
      "Epoch 1, batch 214, loss = 4.562644958496094\n",
      "Epoch 1, batch 215, loss = 4.585118293762207\n",
      "Epoch 1, batch 216, loss = 4.4809250831604\n",
      "Epoch 1, batch 217, loss = 4.697650909423828\n",
      "Epoch 1, batch 218, loss = 4.665340900421143\n",
      "Epoch 1, batch 219, loss = 4.771278381347656\n",
      "Epoch 1, batch 220, loss = 4.739086627960205\n",
      "Epoch 1, batch 221, loss = 4.9456095695495605\n",
      "Epoch 1, batch 222, loss = 4.958352565765381\n",
      "Epoch 1, batch 223, loss = 4.723478317260742\n",
      "Epoch 1, batch 224, loss = 4.80687952041626\n",
      "Epoch 1, batch 225, loss = 4.9000983238220215\n",
      "Epoch 1, batch 226, loss = 4.718832969665527\n",
      "Epoch 1, batch 227, loss = 4.7700018882751465\n",
      "Epoch 1, batch 228, loss = 4.814314365386963\n",
      "Epoch 1, batch 229, loss = 4.933659076690674\n",
      "Epoch 1, batch 230, loss = 4.963185787200928\n",
      "Epoch 1, batch 231, loss = 4.933000087738037\n",
      "Epoch 1, batch 232, loss = 4.663982391357422\n",
      "Epoch 1, batch 233, loss = 4.678875923156738\n",
      "Epoch 1, batch 234, loss = 4.766761302947998\n",
      "Epoch 1, batch 235, loss = 4.676990032196045\n",
      "Epoch 1, batch 236, loss = 4.879556655883789\n",
      "Epoch 1, batch 237, loss = 4.616270065307617\n",
      "Epoch 1, batch 238, loss = 4.683788776397705\n",
      "Epoch 1, batch 239, loss = 4.835893630981445\n",
      "Epoch 1, batch 240, loss = 4.573654651641846\n",
      "Epoch 1, batch 241, loss = 4.615926742553711\n",
      "Epoch 1, batch 242, loss = 4.773769855499268\n",
      "Epoch 1, batch 243, loss = 4.734927177429199\n",
      "Epoch 1, batch 244, loss = 4.7301201820373535\n",
      "Epoch 1, batch 245, loss = 4.656457424163818\n",
      "Epoch 1, batch 246, loss = 4.864229679107666\n",
      "Epoch 1, batch 247, loss = 4.772395133972168\n",
      "Epoch 1, batch 248, loss = 5.024813175201416\n",
      "Epoch 1, batch 249, loss = 4.939674377441406\n",
      "Epoch 1, batch 250, loss = 4.761643886566162\n",
      "Epoch 1, batch 251, loss = 4.874014854431152\n",
      "Epoch 1, batch 252, loss = 4.7834792137146\n",
      "Epoch 1, batch 253, loss = 4.888045310974121\n",
      "Epoch 1, batch 254, loss = 4.893093585968018\n",
      "Epoch 1, batch 255, loss = 5.013465881347656\n",
      "Epoch 1, batch 256, loss = 4.916103839874268\n",
      "Epoch 1, batch 257, loss = 4.87384033203125\n",
      "Epoch 1, batch 258, loss = 4.791139125823975\n",
      "Epoch 1, batch 259, loss = 4.861072540283203\n",
      "Epoch 1, batch 260, loss = 4.919238567352295\n",
      "Epoch 1, batch 261, loss = 5.005555152893066\n",
      "Epoch 1, batch 262, loss = 4.731677532196045\n",
      "Epoch 1, batch 263, loss = 4.869415760040283\n",
      "Epoch 1, batch 264, loss = 4.805731296539307\n",
      "Epoch 1, batch 265, loss = 4.842906475067139\n",
      "Epoch 1, batch 266, loss = 4.701289653778076\n",
      "Epoch 1, batch 267, loss = 4.835162162780762\n",
      "Epoch 1, batch 268, loss = 4.758213043212891\n",
      "Epoch 1, batch 269, loss = 4.863448143005371\n",
      "Epoch 1, batch 270, loss = 5.0882487297058105\n",
      "Epoch 1, batch 271, loss = 4.9176788330078125\n",
      "Epoch 1, batch 272, loss = 4.988607406616211\n",
      "Epoch 1, batch 273, loss = 5.01568603515625\n",
      "Epoch 1, batch 274, loss = 5.0617265701293945\n",
      "Epoch 1, batch 275, loss = 4.949922561645508\n",
      "Epoch 1, batch 276, loss = 4.8139190673828125\n",
      "Epoch 1, batch 277, loss = 5.018017292022705\n",
      "Epoch 1, batch 278, loss = 5.071313858032227\n",
      "Epoch 1, batch 279, loss = 4.839441776275635\n",
      "Epoch 1, batch 280, loss = 5.027494430541992\n",
      "Epoch 1, batch 281, loss = 4.970790863037109\n",
      "Epoch 1, batch 282, loss = 4.923276901245117\n",
      "Epoch 1, batch 283, loss = 5.059506893157959\n",
      "Epoch 1, batch 284, loss = 4.921075820922852\n",
      "Epoch 1, batch 285, loss = 4.836015701293945\n",
      "Epoch 1, batch 286, loss = 4.787403106689453\n",
      "Epoch 1, batch 287, loss = 4.958723068237305\n",
      "Epoch 1, batch 288, loss = 4.8868727684021\n",
      "Epoch 1, batch 289, loss = 4.963470458984375\n",
      "Epoch 1, batch 290, loss = 4.938972473144531\n",
      "Epoch 1, batch 291, loss = 4.890550136566162\n",
      "Epoch 1, batch 292, loss = 5.010304927825928\n",
      "Epoch 1, batch 293, loss = 4.9610114097595215\n",
      "Epoch 1, batch 294, loss = 4.863086223602295\n",
      "Epoch 1, batch 295, loss = 5.039444923400879\n",
      "Epoch 1, batch 296, loss = 5.1383771896362305\n",
      "Epoch 1, batch 297, loss = 5.02134370803833\n",
      "Epoch 1, batch 298, loss = 4.938155651092529\n",
      "Epoch 1, batch 299, loss = 4.932623863220215\n",
      "Epoch 1, batch 300, loss = 4.803224563598633\n",
      "Epoch 1, batch 301, loss = 4.914304733276367\n",
      "Epoch 1, batch 302, loss = 4.857907772064209\n",
      "Epoch 1, batch 303, loss = 4.957479476928711\n",
      "Epoch 1, batch 304, loss = 5.061020851135254\n",
      "Epoch 1, batch 305, loss = 5.002243518829346\n",
      "Epoch 1, batch 306, loss = 4.972653865814209\n",
      "Epoch 1, batch 307, loss = 4.845032691955566\n",
      "Epoch 1, batch 308, loss = 5.123054504394531\n",
      "Epoch 1, batch 309, loss = 5.030572891235352\n",
      "Epoch 1, batch 310, loss = 4.938931941986084\n",
      "Epoch 1, batch 311, loss = 4.8972578048706055\n",
      "Epoch 1, batch 312, loss = 5.036055088043213\n",
      "Epoch 1, batch 313, loss = 4.924765110015869\n",
      "Epoch 1, batch 314, loss = 5.033039093017578\n",
      "Epoch 1, batch 315, loss = 4.996674060821533\n",
      "Epoch 1, batch 316, loss = 4.9603800773620605\n",
      "Epoch 1, batch 317, loss = 4.876190662384033\n",
      "Epoch 1, batch 318, loss = 5.163290023803711\n",
      "Epoch 1, batch 319, loss = 4.9887003898620605\n",
      "Epoch 1, batch 320, loss = 5.179316997528076\n",
      "Epoch 1, batch 321, loss = 5.068532943725586\n",
      "Epoch 1, batch 322, loss = 4.9560394287109375\n",
      "Epoch 1, batch 323, loss = 5.016287326812744\n",
      "Epoch 1, batch 324, loss = 5.167391300201416\n",
      "Epoch 1, batch 325, loss = 4.990031719207764\n",
      "Epoch 1, batch 326, loss = 4.982963562011719\n",
      "Epoch 1, batch 327, loss = 4.968277454376221\n",
      "Epoch 1, batch 328, loss = 4.940494060516357\n",
      "Epoch 1, batch 329, loss = 4.933790683746338\n",
      "Epoch 1, batch 330, loss = 4.841563701629639\n",
      "Epoch 1, batch 331, loss = 5.111608505249023\n",
      "Epoch 1, batch 332, loss = 4.9368085861206055\n",
      "Epoch 1, batch 333, loss = 5.003477573394775\n",
      "Epoch 1, batch 334, loss = 5.003787517547607\n",
      "Epoch 1, batch 335, loss = 5.0682549476623535\n",
      "Epoch 1, batch 336, loss = 4.958664417266846\n",
      "Epoch 1, batch 337, loss = 4.949291706085205\n",
      "Epoch 1, batch 338, loss = 5.086551666259766\n",
      "Epoch 1, batch 339, loss = 4.991223335266113\n",
      "Epoch 1, batch 340, loss = 5.23447847366333\n",
      "Epoch 1, batch 341, loss = 5.226163864135742\n",
      "Epoch 1, batch 342, loss = 5.322637557983398\n",
      "Epoch 1, batch 343, loss = 5.326787948608398\n",
      "Epoch 1, batch 344, loss = 4.967170238494873\n",
      "Epoch 1, batch 345, loss = 5.026261806488037\n",
      "Epoch 1, batch 346, loss = 4.929916858673096\n",
      "Epoch 1, batch 347, loss = 5.159006595611572\n",
      "Epoch 1, batch 348, loss = 5.251875877380371\n",
      "Epoch 1, batch 349, loss = 4.949999809265137\n",
      "Epoch 1, batch 350, loss = 5.1161723136901855\n",
      "Epoch 1, batch 351, loss = 4.850003242492676\n",
      "Epoch 1, batch 352, loss = 5.049431800842285\n",
      "Epoch 1, batch 353, loss = 4.913811683654785\n",
      "Epoch 1, batch 354, loss = 4.981691360473633\n",
      "Epoch 1, batch 355, loss = 5.202094078063965\n",
      "Epoch 1, batch 356, loss = 5.080560207366943\n",
      "Epoch 1, batch 357, loss = 5.012565612792969\n",
      "Epoch 1, batch 358, loss = 5.080014228820801\n",
      "Epoch 1, batch 359, loss = 4.983903884887695\n",
      "Epoch 1, batch 360, loss = 4.892552375793457\n",
      "Epoch 1, batch 361, loss = 5.027987003326416\n",
      "Epoch 1, batch 362, loss = 4.922667026519775\n",
      "Epoch 1, batch 363, loss = 4.7734575271606445\n",
      "Epoch 1, batch 364, loss = 5.019006252288818\n",
      "Epoch 1, batch 365, loss = 4.860317707061768\n",
      "Epoch 1, batch 366, loss = 4.781057357788086\n",
      "Epoch 1, batch 367, loss = 4.859335899353027\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, batch 368, loss = 4.969264030456543\n",
      "Epoch 1, batch 369, loss = 4.905205726623535\n",
      "Epoch 1, batch 370, loss = 4.7759833335876465\n",
      "Epoch 1, batch 371, loss = 4.9342827796936035\n",
      "Epoch 1, batch 372, loss = 4.845490455627441\n",
      "Epoch 1, batch 373, loss = 4.702303886413574\n",
      "Epoch 1, batch 374, loss = 5.051535606384277\n",
      "Epoch 1, batch 375, loss = 4.632069110870361\n",
      "Epoch 1, batch 376, loss = 4.750974178314209\n",
      "Epoch 1, batch 377, loss = 4.958523273468018\n",
      "Epoch 1, batch 378, loss = 4.648577690124512\n",
      "Epoch 1, batch 379, loss = 4.718159198760986\n",
      "Epoch 1, batch 380, loss = 4.863255500793457\n",
      "Epoch 1, batch 381, loss = 4.838286876678467\n",
      "Epoch 1, batch 382, loss = 4.726115703582764\n",
      "Epoch 1, batch 383, loss = 4.663963794708252\n",
      "Epoch 1, batch 384, loss = 4.721309661865234\n",
      "Epoch 1, batch 385, loss = 4.581992149353027\n",
      "Epoch 1, batch 386, loss = 4.624367713928223\n",
      "Epoch 1, batch 387, loss = 4.535958290100098\n",
      "Epoch 1, batch 388, loss = 4.725142002105713\n",
      "Epoch 1, batch 389, loss = 4.76891565322876\n",
      "Epoch 1, batch 390, loss = 4.825939655303955\n",
      "Epoch 1, batch 391, loss = 4.578258037567139\n",
      "Epoch 1, batch 392, loss = 4.800862789154053\n",
      "Epoch 1, batch 393, loss = 5.002710342407227\n",
      "Epoch 1, batch 394, loss = 4.748303413391113\n",
      "Epoch 1, batch 395, loss = 4.764599323272705\n",
      "Epoch 1, batch 396, loss = 4.984734058380127\n",
      "Epoch 1, batch 397, loss = 4.860158443450928\n",
      "Epoch 1, batch 398, loss = 4.729526996612549\n",
      "Epoch 1, batch 399, loss = 4.809837818145752\n",
      "Epoch 1, batch 400, loss = 4.789180755615234\n",
      "Epoch 1, batch 401, loss = 4.953118801116943\n",
      "Epoch 1, batch 402, loss = 5.035599231719971\n",
      "Epoch 1, batch 403, loss = 5.073866367340088\n",
      "Epoch 1, batch 404, loss = 4.6199822425842285\n",
      "Epoch 1, batch 405, loss = 4.749143123626709\n",
      "Epoch 1, batch 406, loss = 5.213440418243408\n",
      "Epoch 1, batch 407, loss = 4.7494797706604\n",
      "Epoch 1, batch 408, loss = 4.893620014190674\n",
      "Epoch 1, batch 409, loss = 4.934237480163574\n",
      "Epoch 1, batch 410, loss = 4.772334575653076\n",
      "Epoch 1, batch 411, loss = 4.795831203460693\n",
      "Epoch 1, batch 412, loss = 4.644009590148926\n",
      "Epoch 1, batch 413, loss = 4.636128902435303\n",
      "Epoch 1, batch 414, loss = 4.723037242889404\n",
      "Epoch 1, batch 415, loss = 4.701152801513672\n",
      "Epoch 1, batch 416, loss = 4.589057922363281\n",
      "Epoch 1, batch 417, loss = 4.69025993347168\n",
      "Epoch 1, batch 418, loss = 4.732633113861084\n",
      "Epoch 1, batch 419, loss = 4.819571018218994\n",
      "Epoch 1, batch 420, loss = 4.762608051300049\n",
      "Epoch 1, batch 421, loss = 5.010321140289307\n",
      "Epoch 1, batch 422, loss = 4.923442363739014\n",
      "Epoch 1, batch 423, loss = 4.784333229064941\n",
      "Epoch 1, batch 424, loss = 4.948668956756592\n",
      "Epoch 1, batch 425, loss = 4.877949237823486\n",
      "Epoch 1, batch 426, loss = 4.9228596687316895\n",
      "Epoch 1, batch 427, loss = 5.002719402313232\n",
      "Epoch 1, batch 428, loss = 5.040782451629639\n",
      "Epoch 1, batch 429, loss = 4.909922122955322\n",
      "Epoch 1, batch 430, loss = 4.7386555671691895\n",
      "Epoch 1, batch 431, loss = 4.993581295013428\n",
      "Epoch 1, batch 432, loss = 4.97904109954834\n",
      "Epoch 1, batch 433, loss = 4.773068428039551\n",
      "Epoch 1, batch 434, loss = 4.873030185699463\n",
      "Epoch 1, batch 435, loss = 4.770983695983887\n",
      "Epoch 1, batch 436, loss = 4.7170538902282715\n",
      "Epoch 1, batch 437, loss = 4.91555118560791\n",
      "Epoch 1, batch 438, loss = 4.7899322509765625\n",
      "Epoch 1, batch 439, loss = 4.78279447555542\n",
      "Epoch 1, batch 440, loss = 4.932712554931641\n",
      "Epoch 1, batch 441, loss = 4.700930118560791\n",
      "Epoch 1, batch 442, loss = 4.928067207336426\n",
      "Epoch 1, batch 443, loss = 5.011856555938721\n",
      "Epoch 1, batch 444, loss = 5.056092262268066\n",
      "Epoch 1, batch 445, loss = 4.907835960388184\n",
      "Epoch 1, batch 446, loss = 5.027199745178223\n",
      "Epoch 1, batch 447, loss = 5.0423173904418945\n",
      "Epoch 1, batch 448, loss = 4.849327564239502\n",
      "Epoch 1, batch 449, loss = 4.932275772094727\n",
      "Epoch 1, batch 450, loss = 4.8970489501953125\n",
      "Epoch 1, batch 451, loss = 4.989301681518555\n",
      "Epoch 1, batch 452, loss = 4.964130401611328\n",
      "Epoch 1, batch 453, loss = 5.090420246124268\n",
      "Epoch 1, batch 454, loss = 5.009176731109619\n",
      "Epoch 1, batch 455, loss = 4.9185380935668945\n",
      "Epoch 1, batch 456, loss = 4.912960529327393\n",
      "Epoch 1, batch 457, loss = 4.98219108581543\n",
      "Epoch 1, batch 458, loss = 4.9375762939453125\n",
      "Epoch 1, batch 459, loss = 5.032167434692383\n",
      "Epoch 1, batch 460, loss = 4.774491310119629\n",
      "Epoch 1, batch 461, loss = 5.013735294342041\n",
      "Epoch 1, batch 462, loss = 5.0402727127075195\n",
      "Epoch 1, batch 463, loss = 5.1269965171813965\n",
      "Epoch 1, batch 464, loss = 4.916561603546143\n",
      "Epoch 1, batch 465, loss = 4.998542785644531\n",
      "Epoch 1, batch 466, loss = 4.8569655418396\n",
      "Epoch 1, batch 467, loss = 4.869326591491699\n",
      "Epoch 1, batch 468, loss = 4.992769718170166\n",
      "Epoch 1, batch 469, loss = 5.054515361785889\n",
      "Epoch 1, batch 470, loss = 5.010703086853027\n",
      "Epoch 1, batch 471, loss = 4.994179725646973\n",
      "Epoch 1, batch 472, loss = 4.88225793838501\n",
      "Epoch 1, batch 473, loss = 4.849466800689697\n",
      "Epoch 1, batch 474, loss = 4.915549278259277\n",
      "Epoch 1, batch 475, loss = 4.776924133300781\n",
      "Epoch 1, batch 476, loss = 4.786376953125\n",
      "Epoch 1, batch 477, loss = 5.057265281677246\n",
      "Epoch 1, batch 478, loss = 4.933520317077637\n",
      "Epoch 1, batch 479, loss = 5.009942054748535\n",
      "Epoch 1, batch 480, loss = 5.042560577392578\n",
      "Epoch 1, batch 481, loss = 4.9521660804748535\n",
      "Epoch 1, batch 482, loss = 5.014448642730713\n",
      "Epoch 1, batch 483, loss = 5.003148078918457\n",
      "Epoch 1, batch 484, loss = 4.993666648864746\n",
      "Epoch 1, batch 485, loss = 5.10946798324585\n",
      "Epoch 1, batch 486, loss = 4.898491859436035\n",
      "Epoch 1, batch 487, loss = 4.793892860412598\n",
      "Epoch 1, batch 488, loss = 5.039896488189697\n",
      "Epoch 1, batch 489, loss = 5.080643177032471\n",
      "Epoch 1, batch 490, loss = 5.040451526641846\n",
      "Epoch 1, batch 491, loss = 4.969325065612793\n",
      "Epoch 1, batch 492, loss = 4.9277567863464355\n",
      "Epoch 1, batch 493, loss = 4.99436616897583\n",
      "Epoch 1, batch 494, loss = 4.972574234008789\n",
      "Epoch 1, batch 495, loss = 4.952863693237305\n",
      "Epoch 1, batch 496, loss = 4.996710300445557\n",
      "Epoch 1, batch 497, loss = 4.896555423736572\n",
      "Epoch 1, batch 498, loss = 4.928676128387451\n",
      "Epoch 1, batch 499, loss = 4.9582953453063965\n",
      "Epoch 1, batch 500, loss = 5.087882995605469\n",
      "Epoch 1, batch 501, loss = 5.1009745597839355\n",
      "Epoch 1, batch 502, loss = 4.877204895019531\n",
      "Epoch 1, batch 503, loss = 5.030709743499756\n",
      "Epoch 1, batch 504, loss = 4.9539642333984375\n",
      "Epoch 1, batch 505, loss = 5.009524822235107\n",
      "Epoch 1, batch 506, loss = 5.046899795532227\n",
      "Epoch 1, batch 507, loss = 4.97648286819458\n",
      "Epoch 1, batch 508, loss = 5.050528049468994\n",
      "Epoch 1, batch 509, loss = 5.010646820068359\n",
      "Epoch 1, batch 510, loss = 4.955393314361572\n",
      "Epoch 1, batch 511, loss = 4.949586391448975\n",
      "Epoch 1, batch 512, loss = 4.8011474609375\n",
      "Epoch 1, batch 513, loss = 5.059476852416992\n",
      "Epoch 1, batch 514, loss = 5.02951192855835\n",
      "Epoch 1, batch 515, loss = 5.319591522216797\n",
      "Epoch 1, batch 516, loss = 5.3600029945373535\n",
      "Epoch 1, batch 517, loss = 5.001612663269043\n",
      "Epoch 1, batch 518, loss = 5.207785606384277\n",
      "Epoch 1, batch 519, loss = 5.158564567565918\n",
      "Epoch 1, batch 520, loss = 5.169219017028809\n",
      "Epoch 1, batch 521, loss = 5.17393684387207\n",
      "Epoch 1, batch 522, loss = 5.35163688659668\n",
      "Epoch 1, batch 523, loss = 5.037208080291748\n",
      "Epoch 1, batch 524, loss = 5.222286224365234\n",
      "Epoch 1, batch 525, loss = 5.259820461273193\n",
      "Epoch 1, batch 526, loss = 5.122951507568359\n",
      "Epoch 1, batch 527, loss = 5.173788070678711\n",
      "Epoch 1, batch 528, loss = 5.093442916870117\n",
      "Epoch 1, batch 529, loss = 4.872030258178711\n",
      "Epoch 1, batch 530, loss = 5.056676387786865\n",
      "Epoch 1, batch 531, loss = 4.968706130981445\n",
      "Epoch 1, batch 532, loss = 5.010788917541504\n",
      "Epoch 1, batch 533, loss = 5.08798360824585\n",
      "Epoch 1, batch 534, loss = 4.898221015930176\n",
      "Epoch 1, batch 535, loss = 4.961702346801758\n",
      "Epoch 1, batch 536, loss = 5.037769794464111\n",
      "Epoch 1, batch 537, loss = 5.007418632507324\n",
      "Epoch 1, batch 538, loss = 4.82408332824707\n",
      "Epoch 1, batch 539, loss = 4.934830665588379\n",
      "Epoch 1, batch 540, loss = 4.8442277908325195\n",
      "Epoch 1, batch 541, loss = 4.878008842468262\n",
      "Epoch 1, batch 542, loss = 4.972154140472412\n",
      "Epoch 1, batch 543, loss = 5.1261138916015625\n",
      "Epoch 1, batch 544, loss = 5.049254417419434\n",
      "Epoch 1, batch 545, loss = 4.607287883758545\n",
      "Epoch 1, batch 546, loss = 4.849501609802246\n",
      "Epoch 1, batch 547, loss = 4.717222690582275\n",
      "Epoch 1, batch 548, loss = 4.877017974853516\n",
      "Epoch 1, batch 549, loss = 5.052758693695068\n",
      "Epoch 1, batch 550, loss = 4.8125176429748535\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, batch 551, loss = 4.751232147216797\n",
      "Epoch 1, batch 552, loss = 4.682823181152344\n",
      "Epoch 1, batch 553, loss = 4.737113952636719\n",
      "Epoch 1, batch 554, loss = 4.715873718261719\n",
      "Epoch 1, batch 555, loss = 4.696431636810303\n",
      "Epoch 1, batch 556, loss = 4.734005451202393\n",
      "Epoch 1, batch 557, loss = 4.721468448638916\n",
      "Epoch 1, batch 558, loss = 4.756258964538574\n",
      "Epoch 1, batch 559, loss = 4.709580898284912\n",
      "Epoch 1, batch 560, loss = 4.94669771194458\n",
      "Epoch 1, batch 561, loss = 4.808894634246826\n",
      "Epoch 1, batch 562, loss = 4.7358880043029785\n",
      "Epoch 1, batch 563, loss = 4.934584140777588\n",
      "Epoch 1, batch 564, loss = 4.783270359039307\n",
      "Epoch 1, batch 565, loss = 4.698782444000244\n",
      "Epoch 1, batch 566, loss = 4.9135589599609375\n",
      "Epoch 1, batch 567, loss = 4.849909782409668\n",
      "Epoch 1, batch 568, loss = 4.920575141906738\n",
      "Epoch 1, batch 569, loss = 4.906631946563721\n",
      "Epoch 1, batch 570, loss = 4.702907562255859\n",
      "Epoch 1, batch 571, loss = 4.829619884490967\n",
      "Epoch 1, batch 572, loss = 4.7317657470703125\n",
      "Epoch 1, batch 573, loss = 4.857738971710205\n",
      "Epoch 1, batch 574, loss = 4.674337387084961\n",
      "Epoch 1, batch 575, loss = 4.742142200469971\n",
      "Epoch 1, batch 576, loss = 4.772380352020264\n",
      "Epoch 1, batch 577, loss = 4.744967937469482\n",
      "Epoch 1, batch 578, loss = 4.624016284942627\n",
      "Epoch 1, batch 579, loss = 4.627478122711182\n",
      "Epoch 1, batch 580, loss = 4.718485355377197\n",
      "Epoch 1, batch 581, loss = 4.728396415710449\n",
      "Epoch 1, batch 582, loss = 4.997836112976074\n",
      "Epoch 1, batch 583, loss = 4.909127712249756\n",
      "Epoch 1, batch 584, loss = 5.225166320800781\n",
      "Epoch 1, batch 585, loss = 4.885807037353516\n",
      "Epoch 1, batch 586, loss = 4.833835124969482\n",
      "Epoch 1, batch 587, loss = 4.948791027069092\n",
      "Epoch 1, batch 588, loss = 4.955000877380371\n",
      "Epoch 1, batch 589, loss = 4.820954322814941\n",
      "Epoch 1, batch 590, loss = 4.8051557540893555\n",
      "Epoch 1, batch 591, loss = 4.858468532562256\n",
      "Epoch 1, batch 592, loss = 4.885064125061035\n",
      "Epoch 1, batch 593, loss = 4.838761329650879\n",
      "Epoch 1, batch 594, loss = 4.909146785736084\n",
      "Epoch 1, batch 595, loss = 5.083296775817871\n",
      "Epoch 1, batch 596, loss = 4.958504676818848\n",
      "Epoch 1, batch 597, loss = 4.90298318862915\n",
      "Epoch 1, batch 598, loss = 4.897170543670654\n",
      "Epoch 1, batch 599, loss = 5.0081095695495605\n",
      "Epoch 1, batch 600, loss = 4.84791898727417\n",
      "Epoch 1, batch 601, loss = 4.7319817543029785\n",
      "Epoch 1, batch 602, loss = 4.709009647369385\n",
      "Epoch 1, batch 603, loss = 4.891184329986572\n",
      "Epoch 1, batch 604, loss = 4.955618858337402\n",
      "Epoch 1, batch 605, loss = 4.99791955947876\n",
      "Epoch 1, batch 606, loss = 5.007331848144531\n",
      "Epoch 1, batch 607, loss = 4.947239875793457\n",
      "Epoch 1, batch 608, loss = 5.008963584899902\n",
      "Epoch 1, batch 609, loss = 4.89615535736084\n",
      "Epoch 1, batch 610, loss = 4.871797561645508\n",
      "Epoch 1, batch 611, loss = 4.838921546936035\n",
      "Epoch 1, batch 612, loss = 4.891875267028809\n",
      "Epoch 1, batch 613, loss = 4.919710159301758\n",
      "Epoch 1, batch 614, loss = 4.8725996017456055\n",
      "Epoch 1, batch 615, loss = 5.0735883712768555\n",
      "Epoch 1, batch 616, loss = 4.973519802093506\n",
      "Epoch 1, batch 617, loss = 5.050368309020996\n",
      "Epoch 1, batch 618, loss = 5.108146667480469\n",
      "Epoch 1, batch 619, loss = 4.900695323944092\n",
      "Epoch 1, batch 620, loss = 4.848140239715576\n",
      "Epoch 1, batch 621, loss = 4.839797019958496\n",
      "Epoch 1, batch 622, loss = 4.97904634475708\n",
      "Epoch 1, batch 623, loss = 4.8949785232543945\n",
      "Epoch 1, batch 624, loss = 5.035905361175537\n",
      "Epoch 1, batch 625, loss = 5.05792236328125\n",
      "Epoch 1, batch 626, loss = 4.95796537399292\n",
      "Epoch 1, batch 627, loss = 4.993024826049805\n",
      "Epoch 1, batch 628, loss = 4.924343585968018\n",
      "Epoch 1, batch 629, loss = 5.02201509475708\n",
      "Epoch 1, batch 630, loss = 4.898423194885254\n",
      "Epoch 1, batch 631, loss = 4.946288108825684\n",
      "Epoch 1, batch 632, loss = 5.04008674621582\n",
      "Epoch 1, batch 633, loss = 4.998747825622559\n",
      "Epoch 1, batch 634, loss = 5.1609206199646\n",
      "Epoch 1, batch 635, loss = 4.946657657623291\n",
      "Epoch 1, batch 636, loss = 4.835318565368652\n",
      "Epoch 1, batch 637, loss = 4.980355262756348\n",
      "Epoch 1, batch 638, loss = 5.031111240386963\n",
      "Epoch 1, batch 639, loss = 4.942885398864746\n",
      "Epoch 1, batch 640, loss = 5.0426506996154785\n",
      "Epoch 1, batch 641, loss = 5.005582809448242\n",
      "Epoch 1, batch 642, loss = 5.031668186187744\n",
      "Epoch 1, batch 643, loss = 5.06389045715332\n",
      "Epoch 1, batch 644, loss = 4.934621334075928\n",
      "Epoch 1, batch 645, loss = 5.053653717041016\n",
      "Epoch 1, batch 646, loss = 4.901608943939209\n",
      "Epoch 1, batch 647, loss = 4.907607078552246\n",
      "Epoch 1, batch 648, loss = 4.9295549392700195\n",
      "Epoch 1, batch 649, loss = 4.843137741088867\n",
      "Epoch 1, batch 650, loss = 4.951447010040283\n",
      "Epoch 1, batch 651, loss = 4.9972944259643555\n",
      "Epoch 1, batch 652, loss = 4.9286065101623535\n",
      "Epoch 1, batch 653, loss = 5.074239730834961\n",
      "Epoch 1, batch 654, loss = 5.010921001434326\n",
      "Epoch 1, batch 655, loss = 4.933834552764893\n",
      "Epoch 1, batch 656, loss = 4.941099166870117\n",
      "Epoch 1, batch 657, loss = 4.898062229156494\n",
      "Epoch 1, batch 658, loss = 5.022615432739258\n",
      "Epoch 1, batch 659, loss = 5.070810794830322\n",
      "Epoch 1, batch 660, loss = 4.990886211395264\n",
      "Epoch 1, batch 661, loss = 5.176600456237793\n",
      "Epoch 1, batch 662, loss = 5.1352858543396\n",
      "Epoch 1, batch 663, loss = 5.035397052764893\n",
      "Epoch 1, batch 664, loss = 4.9809722900390625\n",
      "Epoch 1, batch 665, loss = 4.979494094848633\n",
      "Epoch 1, batch 666, loss = 4.926146984100342\n",
      "Epoch 1, batch 667, loss = 4.846435546875\n",
      "Epoch 1, batch 668, loss = 5.036685943603516\n",
      "Epoch 1, batch 669, loss = 5.056582450866699\n",
      "Epoch 1, batch 670, loss = 4.919636249542236\n",
      "Epoch 1, batch 671, loss = 4.922487258911133\n",
      "Epoch 1, batch 672, loss = 4.959984302520752\n",
      "Epoch 1, batch 673, loss = 5.006984710693359\n",
      "Epoch 1, batch 674, loss = 4.974166393280029\n",
      "Epoch 1, batch 675, loss = 5.133477687835693\n",
      "Epoch 1, batch 676, loss = 5.390944480895996\n",
      "Epoch 1, batch 677, loss = 5.379800796508789\n",
      "Epoch 1, batch 678, loss = 5.169950485229492\n",
      "Epoch 1, batch 679, loss = 5.162788391113281\n",
      "Epoch 1, batch 680, loss = 5.283144950866699\n",
      "Epoch 1, batch 681, loss = 5.101593494415283\n",
      "Epoch 1, batch 682, loss = 5.113412857055664\n",
      "Epoch 1, batch 683, loss = 5.139063358306885\n",
      "Epoch 1, batch 684, loss = 4.9732513427734375\n",
      "Epoch 1, batch 685, loss = 4.905144214630127\n",
      "Epoch 1, batch 686, loss = 5.181743621826172\n",
      "Epoch 1, batch 687, loss = 5.133781909942627\n",
      "Epoch 1, batch 688, loss = 5.119295120239258\n",
      "Epoch 1, batch 689, loss = 5.0469136238098145\n",
      "Epoch 1, batch 690, loss = 4.902735710144043\n",
      "Epoch 1, batch 691, loss = 5.093188285827637\n",
      "Epoch 1, batch 692, loss = 4.88712215423584\n",
      "Epoch 1, batch 693, loss = 5.055240631103516\n",
      "Epoch 1, batch 694, loss = 4.8944411277771\n",
      "Epoch 1, batch 695, loss = 4.739170074462891\n",
      "Epoch 1, batch 696, loss = 4.7693772315979\n",
      "Epoch 1, batch 697, loss = 5.002018928527832\n",
      "Epoch 1, batch 698, loss = 4.947073459625244\n",
      "Epoch 1, batch 699, loss = 4.833184719085693\n",
      "Epoch 1, batch 700, loss = 4.919062614440918\n",
      "Epoch 1, batch 701, loss = 4.878830909729004\n",
      "Epoch 1, batch 702, loss = 4.8892974853515625\n",
      "Epoch 1, batch 703, loss = 4.838175296783447\n",
      "Epoch 1, batch 704, loss = 4.821524620056152\n",
      "Epoch 1, batch 705, loss = 4.709380626678467\n",
      "Epoch 1, batch 706, loss = 4.6597700119018555\n",
      "Epoch 1, batch 707, loss = 4.64736795425415\n",
      "Epoch 1, batch 708, loss = 4.847972869873047\n",
      "Epoch 1, batch 709, loss = 4.867523670196533\n",
      "Epoch 1, batch 710, loss = 4.814385414123535\n",
      "Epoch 1, batch 711, loss = 4.7678961753845215\n",
      "Epoch 1, batch 712, loss = 4.872664928436279\n",
      "Epoch 1, batch 713, loss = 4.875633239746094\n",
      "Epoch 1, batch 714, loss = 4.7274250984191895\n",
      "Epoch 1, batch 715, loss = 4.913570880889893\n",
      "Epoch 1, batch 716, loss = 4.822787761688232\n",
      "Epoch 1, batch 717, loss = 4.76375150680542\n",
      "Epoch 1, batch 718, loss = 4.835362911224365\n",
      "Epoch 1, batch 719, loss = 4.84528923034668\n",
      "Epoch 1, batch 720, loss = 4.849442958831787\n",
      "Epoch 1, batch 721, loss = 4.699962139129639\n",
      "Epoch 1, batch 722, loss = 4.781358242034912\n",
      "Epoch 1, batch 723, loss = 4.947143077850342\n",
      "Epoch 1, batch 724, loss = 4.8353190422058105\n",
      "Epoch 1, batch 725, loss = 4.761791229248047\n",
      "Epoch 1, batch 726, loss = 4.724075794219971\n",
      "Epoch 1, batch 727, loss = 4.7943596839904785\n",
      "Epoch 1, batch 728, loss = 4.857851982116699\n",
      "Epoch 1, batch 729, loss = 4.837191104888916\n",
      "Epoch 1, batch 730, loss = 4.970943927764893\n",
      "Epoch 1, batch 731, loss = 4.780674934387207\n",
      "Epoch 1, batch 732, loss = 4.87627649307251\n",
      "Epoch 1, batch 733, loss = 4.936764240264893\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, batch 734, loss = 4.818121910095215\n",
      "Epoch 1, batch 735, loss = 4.959081172943115\n",
      "Epoch 1, batch 736, loss = 5.028728008270264\n",
      "Epoch 1, batch 737, loss = 4.948211193084717\n",
      "Epoch 1, batch 738, loss = 4.847878932952881\n",
      "Epoch 1, batch 739, loss = 4.919224262237549\n",
      "Epoch 1, batch 740, loss = 5.122117042541504\n",
      "Epoch 1, batch 741, loss = 4.939424991607666\n",
      "Epoch 1, batch 742, loss = 4.922999858856201\n",
      "Epoch 1, batch 743, loss = 4.897873878479004\n",
      "Epoch 1, batch 744, loss = 4.87775182723999\n",
      "Epoch 1, batch 745, loss = 4.975220680236816\n",
      "Epoch 1, batch 746, loss = 4.960134029388428\n",
      "Epoch 1, batch 747, loss = 4.972772598266602\n",
      "Epoch 1, batch 748, loss = 4.997978210449219\n",
      "Epoch 1, batch 749, loss = 4.984060287475586\n",
      "Epoch 1, batch 750, loss = 4.983859062194824\n",
      "Epoch 1, batch 751, loss = 4.923836708068848\n",
      "Epoch 1, batch 752, loss = 4.761726379394531\n",
      "Epoch 1, batch 753, loss = 4.846303939819336\n",
      "Epoch 1, batch 754, loss = 4.985813140869141\n",
      "Epoch 1, batch 755, loss = 4.907032012939453\n",
      "Epoch 1, batch 756, loss = 4.91923713684082\n",
      "Epoch 1, batch 757, loss = 4.969019412994385\n",
      "Epoch 1, batch 758, loss = 4.728346824645996\n",
      "Epoch 1, batch 759, loss = 4.811626434326172\n",
      "Epoch 1, batch 760, loss = 4.982941627502441\n",
      "Epoch 1, batch 761, loss = 4.889344215393066\n",
      "Epoch 1, batch 762, loss = 4.932577133178711\n",
      "Epoch 1, batch 763, loss = 4.909143447875977\n",
      "Epoch 1, batch 764, loss = 4.976353168487549\n",
      "Epoch 1, batch 765, loss = 5.157858848571777\n",
      "Epoch 1, batch 766, loss = 4.986907958984375\n",
      "Epoch 1, batch 767, loss = 4.981722831726074\n",
      "Epoch 1, batch 768, loss = 4.886619567871094\n",
      "Epoch 1, batch 769, loss = 5.0099358558654785\n",
      "Epoch 1, batch 770, loss = 4.9873552322387695\n",
      "Epoch 1, batch 771, loss = 4.987559795379639\n",
      "Epoch 1, batch 772, loss = 5.082529067993164\n",
      "Epoch 1, batch 773, loss = 4.955742359161377\n",
      "Epoch 1, batch 774, loss = 4.790067672729492\n",
      "Epoch 1, batch 775, loss = 4.83903169631958\n",
      "Epoch 1, batch 776, loss = 4.849891185760498\n",
      "Epoch 1, batch 777, loss = 4.949071884155273\n",
      "Epoch 1, batch 778, loss = 5.074864387512207\n",
      "Epoch 1, batch 779, loss = 5.012636661529541\n",
      "Epoch 1, batch 780, loss = 4.963769435882568\n",
      "Epoch 1, batch 781, loss = 4.913232326507568\n",
      "Epoch 1, batch 782, loss = 4.997450351715088\n",
      "Epoch 1, batch 783, loss = 4.991555690765381\n",
      "Epoch 1, batch 784, loss = 5.075182914733887\n",
      "Epoch 1, batch 785, loss = 4.992333889007568\n",
      "Epoch 1, batch 786, loss = 4.9149651527404785\n",
      "Epoch 1, batch 787, loss = 4.913784503936768\n",
      "Epoch 1, batch 788, loss = 4.876959323883057\n",
      "Epoch 1, batch 789, loss = 4.887449264526367\n",
      "Epoch 1, batch 790, loss = 4.946277618408203\n",
      "Epoch 1, batch 791, loss = 5.030437469482422\n",
      "Epoch 1, batch 792, loss = 5.056376934051514\n",
      "Epoch 1, batch 793, loss = 5.052367210388184\n",
      "Epoch 1, batch 794, loss = 5.047345161437988\n",
      "Epoch 1, batch 795, loss = 4.9795756340026855\n",
      "Epoch 1, batch 796, loss = 4.973178386688232\n",
      "Epoch 1, batch 797, loss = 5.08788537979126\n",
      "Epoch 1, batch 798, loss = 4.924643516540527\n",
      "Epoch 1, batch 799, loss = 4.987275123596191\n",
      "Epoch 1, batch 800, loss = 5.008202075958252\n",
      "Epoch 1, batch 801, loss = 4.929134845733643\n",
      "Epoch 1, batch 802, loss = 4.96960973739624\n",
      "Epoch 1, batch 803, loss = 4.954887866973877\n",
      "Epoch 1, batch 804, loss = 4.917070388793945\n",
      "Epoch 1, batch 805, loss = 5.031984806060791\n",
      "Epoch 1, batch 806, loss = 5.005699157714844\n",
      "Epoch 1, batch 807, loss = 4.93637228012085\n",
      "Epoch 1, batch 808, loss = 5.005180835723877\n",
      "Epoch 1, batch 809, loss = 5.053470611572266\n",
      "Epoch 1, batch 810, loss = 4.724453449249268\n",
      "Epoch 1, batch 811, loss = 5.332447052001953\n",
      "Epoch 1, batch 812, loss = 5.118135929107666\n",
      "Epoch 1, batch 813, loss = 4.763252258300781\n",
      "Epoch 1, batch 814, loss = 5.03407096862793\n",
      "Epoch 1, batch 815, loss = 5.2577948570251465\n",
      "Epoch 1, batch 816, loss = 5.097033500671387\n",
      "Epoch 1, batch 817, loss = 5.081984996795654\n",
      "Epoch 1, batch 818, loss = 5.030123710632324\n",
      "Epoch 1, batch 819, loss = 5.331217288970947\n",
      "Epoch 1, batch 820, loss = 4.775604248046875\n",
      "Epoch 1, batch 821, loss = 4.960180282592773\n",
      "Epoch 1, batch 822, loss = 5.021767616271973\n",
      "Epoch 1, batch 823, loss = 5.015998840332031\n",
      "Epoch 1, batch 824, loss = 4.934551239013672\n",
      "Epoch 1, batch 825, loss = 4.914164066314697\n",
      "Epoch 1, batch 826, loss = 4.988490104675293\n",
      "Epoch 1, batch 827, loss = 4.764495849609375\n",
      "Epoch 1, batch 828, loss = 4.977053642272949\n",
      "Epoch 1, batch 829, loss = 4.809922218322754\n",
      "Epoch 1, batch 830, loss = 4.748770713806152\n",
      "Epoch 1, batch 831, loss = 4.899784088134766\n",
      "Epoch 1, batch 832, loss = 4.867506504058838\n",
      "Epoch 1, batch 833, loss = 4.919191837310791\n",
      "Epoch 1, batch 834, loss = 4.715322971343994\n",
      "Epoch 1, batch 835, loss = 4.739571571350098\n",
      "Epoch 1, batch 836, loss = 4.768984794616699\n",
      "Epoch 1, batch 837, loss = 5.10961389541626\n",
      "Epoch 1, batch 838, loss = 4.877840995788574\n",
      "Epoch 1, batch 839, loss = 4.855849266052246\n",
      "Epoch 1, batch 840, loss = 4.932971477508545\n",
      "Epoch 1, batch 841, loss = 4.810194969177246\n",
      "Epoch 1, batch 842, loss = 4.772166728973389\n",
      "Epoch 1, batch 843, loss = 5.047837734222412\n",
      "Epoch 1, batch 844, loss = 4.891862392425537\n",
      "Epoch 1, batch 845, loss = 4.908191680908203\n",
      "Epoch 1, batch 846, loss = 4.836728572845459\n",
      "Epoch 1, batch 847, loss = 4.799275875091553\n",
      "Epoch 1, batch 848, loss = 4.780729293823242\n",
      "Epoch 1, batch 849, loss = 4.730527400970459\n",
      "Epoch 1, batch 850, loss = 4.756570816040039\n",
      "Epoch 1, batch 851, loss = 4.627029895782471\n",
      "Epoch 1, batch 852, loss = 4.68589973449707\n",
      "Epoch 1, batch 853, loss = 4.8646392822265625\n",
      "Epoch 1, batch 854, loss = 4.918683052062988\n",
      "Epoch 1, batch 855, loss = 4.910793781280518\n",
      "Epoch 1, batch 856, loss = 4.92432165145874\n",
      "Epoch 1, batch 857, loss = 4.87662410736084\n",
      "Epoch 1, batch 858, loss = 4.968026638031006\n",
      "Epoch 1, batch 859, loss = 5.02274751663208\n",
      "Epoch 1, batch 860, loss = 4.77791166305542\n",
      "Epoch 1, batch 861, loss = 4.993900775909424\n",
      "Epoch 1, batch 862, loss = 4.82656717300415\n",
      "Epoch 1, batch 863, loss = 4.855492115020752\n",
      "Epoch 1, batch 864, loss = 4.882519245147705\n",
      "Epoch 1, batch 865, loss = 4.794200420379639\n",
      "Epoch 1, batch 866, loss = 4.849445343017578\n",
      "Epoch 1, batch 867, loss = 4.650835990905762\n",
      "Epoch 1, batch 868, loss = 4.900066375732422\n",
      "Epoch 1, batch 869, loss = 4.957500457763672\n",
      "Epoch 1, batch 870, loss = 4.916757583618164\n",
      "Epoch 1, batch 871, loss = 4.823237419128418\n",
      "Epoch 1, batch 872, loss = 4.918129920959473\n",
      "Epoch 1, batch 873, loss = 4.992727279663086\n",
      "Epoch 1, batch 874, loss = 4.962322235107422\n",
      "Epoch 1, batch 875, loss = 5.152362823486328\n",
      "Epoch 1, batch 876, loss = 5.045834541320801\n",
      "Epoch 1, batch 877, loss = 5.013002872467041\n",
      "Epoch 1, batch 878, loss = 4.952633857727051\n",
      "Epoch 1, batch 879, loss = 4.875791549682617\n",
      "Epoch 1, batch 880, loss = 4.799817085266113\n",
      "Epoch 1, batch 881, loss = 5.006923675537109\n",
      "Epoch 1, batch 882, loss = 5.085019588470459\n",
      "Epoch 1, batch 883, loss = 4.8773064613342285\n",
      "Epoch 1, batch 884, loss = 4.938467979431152\n",
      "Epoch 1, batch 885, loss = 4.932875633239746\n",
      "Epoch 1, batch 886, loss = 4.867265701293945\n",
      "Epoch 1, batch 887, loss = 5.033024311065674\n",
      "Epoch 1, batch 888, loss = 4.929686546325684\n",
      "Epoch 1, batch 889, loss = 4.883755207061768\n",
      "Epoch 1, batch 890, loss = 4.9010467529296875\n",
      "Epoch 1, batch 891, loss = 4.801708221435547\n",
      "Epoch 1, batch 892, loss = 4.779970645904541\n",
      "Epoch 1, batch 893, loss = 5.106094837188721\n",
      "Epoch 1, batch 894, loss = 4.958217144012451\n",
      "Epoch 1, batch 895, loss = 4.9515700340271\n",
      "Epoch 1, batch 896, loss = 4.930530548095703\n",
      "Epoch 1, batch 897, loss = 4.9358696937561035\n",
      "Epoch 1, batch 898, loss = 5.099949836730957\n",
      "Epoch 1, batch 899, loss = 5.102652549743652\n",
      "Epoch 1, batch 900, loss = 4.904024124145508\n",
      "Epoch 1, batch 901, loss = 4.987256050109863\n",
      "Epoch 1, batch 902, loss = 4.792567729949951\n",
      "Epoch 1, batch 903, loss = 4.980504989624023\n",
      "Epoch 1, batch 904, loss = 4.934102535247803\n",
      "Epoch 1, batch 905, loss = 4.98150634765625\n",
      "Epoch 1, batch 906, loss = 4.9780097007751465\n",
      "Epoch 1, batch 907, loss = 4.857306003570557\n",
      "Epoch 1, batch 908, loss = 5.041125297546387\n",
      "Epoch 1, batch 909, loss = 5.011651515960693\n",
      "Epoch 1, batch 910, loss = 5.055685997009277\n",
      "Epoch 1, batch 911, loss = 5.063615322113037\n",
      "Epoch 1, batch 912, loss = 4.969533920288086\n",
      "Epoch 1, batch 913, loss = 4.940694808959961\n",
      "Epoch 1, batch 914, loss = 4.925345420837402\n",
      "Epoch 1, batch 915, loss = 4.938138961791992\n",
      "Epoch 1, batch 916, loss = 4.9528279304504395\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, batch 917, loss = 4.857975959777832\n",
      "Epoch 1, batch 918, loss = 5.066794395446777\n",
      "Epoch 1, batch 919, loss = 5.0245256423950195\n",
      "Epoch 1, batch 920, loss = 5.039724826812744\n",
      "Epoch 1, batch 921, loss = 4.808847904205322\n",
      "Epoch 1, batch 922, loss = 4.9074811935424805\n",
      "Epoch 1, batch 923, loss = 5.269526481628418\n",
      "Epoch 1, batch 924, loss = 5.062376499176025\n",
      "Epoch 1, batch 925, loss = 5.147191047668457\n",
      "Epoch 1, batch 926, loss = 5.1408891677856445\n",
      "Epoch 1, batch 927, loss = 5.208934307098389\n",
      "Epoch 1, batch 928, loss = 5.189998626708984\n",
      "Epoch 1, batch 929, loss = 4.942867279052734\n",
      "Epoch 1, batch 930, loss = 4.874709606170654\n",
      "Epoch 1, batch 931, loss = 4.931658744812012\n",
      "Epoch 1, batch 932, loss = 4.983899116516113\n",
      "Epoch 1, batch 933, loss = 4.896878719329834\n",
      "Epoch 1, batch 934, loss = 4.998264789581299\n",
      "Epoch 1, batch 935, loss = 4.98276424407959\n",
      "Epoch 1, batch 936, loss = 4.993286609649658\n",
      "Epoch 1, batch 937, loss = 4.7206830978393555\n",
      "Epoch 1, batch 938, loss = 4.9601569175720215\n",
      "Epoch 1, batch 939, loss = 4.791273593902588\n",
      "Epoch 1, batch 940, loss = 4.796971797943115\n",
      "Epoch 1, batch 941, loss = 4.712409019470215\n",
      "Epoch 1, batch 942, loss = 4.6854681968688965\n",
      "Epoch 1, batch 943, loss = 4.783626556396484\n",
      "Epoch 1, batch 944, loss = 4.773416519165039\n",
      "Epoch 1, batch 945, loss = 4.909889221191406\n",
      "Epoch 1, batch 946, loss = 4.785858154296875\n",
      "Epoch 1, batch 947, loss = 4.828494071960449\n",
      "Epoch 1, batch 948, loss = 4.796110153198242\n",
      "Epoch 1, batch 949, loss = 4.839416980743408\n",
      "Epoch 1, batch 950, loss = 4.962609767913818\n",
      "Epoch 1, batch 951, loss = 4.838235378265381\n",
      "Epoch 1, batch 952, loss = 4.628167152404785\n",
      "Epoch 1, batch 953, loss = 4.683326244354248\n",
      "Epoch 1, batch 954, loss = 4.779623508453369\n",
      "Epoch 1, batch 955, loss = 4.898474216461182\n",
      "Epoch 1, batch 956, loss = 4.821534156799316\n",
      "Epoch 1, batch 957, loss = 4.9002485275268555\n",
      "Epoch 1, batch 958, loss = 4.840519428253174\n",
      "Epoch 1, batch 959, loss = 4.78842306137085\n",
      "Epoch 1, batch 960, loss = 4.869574069976807\n",
      "Epoch 1, batch 961, loss = 4.950419902801514\n",
      "Epoch 1, batch 962, loss = 4.887423992156982\n",
      "Epoch 1, batch 963, loss = 5.019322395324707\n",
      "Epoch 1, batch 964, loss = 4.8232421875\n",
      "Epoch 1, batch 965, loss = 4.823270320892334\n",
      "Epoch 1, batch 966, loss = 4.669212818145752\n",
      "Epoch 1, batch 967, loss = 4.982004165649414\n",
      "Epoch 1, batch 968, loss = 4.909211158752441\n",
      "Epoch 1, batch 969, loss = 4.8699421882629395\n",
      "Epoch 1, batch 970, loss = 4.7946624755859375\n",
      "Epoch 1, batch 971, loss = 4.9256720542907715\n",
      "Epoch 1, batch 972, loss = 4.868167877197266\n",
      "Epoch 1, batch 973, loss = 4.9511847496032715\n",
      "Epoch 1, batch 974, loss = 4.888221740722656\n",
      "Epoch 1, batch 975, loss = 4.819075107574463\n",
      "Epoch 1, batch 976, loss = 4.795259475708008\n",
      "Epoch 1, batch 977, loss = 4.878077507019043\n",
      "Epoch 1, batch 978, loss = 4.924230098724365\n",
      "Epoch 1, batch 979, loss = 4.870131969451904\n",
      "Epoch 1, batch 980, loss = 5.002725601196289\n",
      "Epoch 1, batch 981, loss = 4.8247904777526855\n",
      "Epoch 1, batch 982, loss = 4.972502708435059\n",
      "Epoch 1, batch 983, loss = 5.006906509399414\n",
      "Epoch 1, batch 984, loss = 4.960649013519287\n",
      "Epoch 1, batch 985, loss = 5.027385234832764\n",
      "Epoch 1, batch 986, loss = 5.0135626792907715\n",
      "Epoch 1, batch 987, loss = 4.842892646789551\n",
      "Epoch 1, batch 988, loss = 5.0865302085876465\n",
      "Epoch 1, batch 989, loss = 4.9566240310668945\n",
      "Epoch 1, batch 990, loss = 5.062747001647949\n",
      "Epoch 1, batch 991, loss = 4.939757347106934\n",
      "Epoch 1, batch 992, loss = 4.944128513336182\n",
      "Epoch 1, batch 993, loss = 4.956818103790283\n",
      "Epoch 1, batch 994, loss = 5.078448295593262\n",
      "Epoch 1, batch 995, loss = 4.965457916259766\n",
      "Epoch 1, batch 996, loss = 4.924392223358154\n",
      "Epoch 1, batch 997, loss = 5.0046491622924805\n",
      "Epoch 1, batch 998, loss = 5.022726535797119\n",
      "Epoch 1, batch 999, loss = 4.9758687019348145\n",
      "Epoch 1, batch 1000, loss = 4.9938578605651855\n",
      "Epoch 1, batch 1001, loss = 5.029808521270752\n",
      "Epoch 1, batch 1002, loss = 5.063438415527344\n",
      "Epoch 1, batch 1003, loss = 4.954084873199463\n",
      "Epoch 1, batch 1004, loss = 5.05587911605835\n",
      "Epoch 1, batch 1005, loss = 5.010443687438965\n",
      "Epoch 1, batch 1006, loss = 5.112536907196045\n",
      "Epoch 1, batch 1007, loss = 4.908873558044434\n",
      "Epoch 1, batch 1008, loss = 4.990744590759277\n",
      "Epoch 1, batch 1009, loss = 5.037280559539795\n",
      "Epoch 1, batch 1010, loss = 5.017022132873535\n",
      "Epoch 1, batch 1011, loss = 4.96626091003418\n",
      "Epoch 1, batch 1012, loss = 5.089051246643066\n",
      "Epoch 1, batch 1013, loss = 4.90474796295166\n",
      "Epoch 1, batch 1014, loss = 4.893486499786377\n",
      "Epoch 1, batch 1015, loss = 4.790431976318359\n",
      "Epoch 1, batch 1016, loss = 5.129172325134277\n",
      "Epoch 1, batch 1017, loss = 4.9669365882873535\n",
      "Epoch 1, batch 1018, loss = 4.931677341461182\n",
      "Epoch 1, batch 1019, loss = 4.834263801574707\n",
      "Epoch 1, batch 1020, loss = 4.817607402801514\n",
      "Epoch 1, batch 1021, loss = 4.821186065673828\n",
      "Epoch 1, batch 1022, loss = 4.8206987380981445\n",
      "Epoch 1, batch 1023, loss = 4.989002704620361\n",
      "Epoch 1, batch 1024, loss = 4.841700553894043\n",
      "Epoch 1, batch 1025, loss = 4.772845268249512\n",
      "Epoch 1, batch 1026, loss = 4.928974151611328\n",
      "Epoch 1, batch 1027, loss = 4.9136528968811035\n",
      "Epoch 1, batch 1028, loss = 4.895942211151123\n",
      "Epoch 1, batch 1029, loss = 4.726921558380127\n",
      "Epoch 1, batch 1030, loss = 4.929638862609863\n",
      "Epoch 1, batch 1031, loss = 4.849524021148682\n",
      "Epoch 1, batch 1032, loss = 4.880774974822998\n",
      "Epoch 1, batch 1033, loss = 4.8445000648498535\n",
      "Epoch 1, batch 1034, loss = 4.8388352394104\n",
      "Epoch 1, batch 1035, loss = 4.896631240844727\n",
      "Epoch 1, batch 1036, loss = 5.012515544891357\n",
      "Epoch 1, batch 1037, loss = 4.807621955871582\n",
      "Epoch 1, batch 1038, loss = 4.8166069984436035\n",
      "Epoch 1, batch 1039, loss = 4.8676276206970215\n",
      "Epoch 1, batch 1040, loss = 4.86795711517334\n",
      "Epoch 1, batch 1041, loss = 4.784966468811035\n",
      "Epoch 1, batch 1042, loss = 4.895644187927246\n",
      "Epoch 1, batch 1043, loss = 4.7856764793396\n",
      "Epoch 1, batch 1044, loss = 4.890700817108154\n",
      "Epoch 1, batch 1045, loss = 5.034040451049805\n",
      "Epoch 1, batch 1046, loss = 4.861309051513672\n",
      "Epoch 1, batch 1047, loss = 4.97214937210083\n",
      "Epoch 1, batch 1048, loss = 4.9058709144592285\n",
      "Epoch 1, batch 1049, loss = 4.7680253982543945\n",
      "Epoch 1, batch 1050, loss = 4.833292484283447\n",
      "Epoch 1, batch 1051, loss = 4.896209716796875\n",
      "Epoch 1, batch 1052, loss = 5.014923095703125\n",
      "Epoch 1, batch 1053, loss = 4.972419738769531\n",
      "Epoch 1, batch 1054, loss = 4.918067455291748\n",
      "Epoch 1, batch 1055, loss = 5.068235874176025\n",
      "Epoch 1, batch 1056, loss = 4.943883419036865\n",
      "Epoch 1, batch 1057, loss = 4.87076997756958\n",
      "Epoch 1, batch 1058, loss = 4.812308311462402\n",
      "Epoch 1, batch 1059, loss = 4.887617588043213\n",
      "Epoch 1, batch 1060, loss = 5.077195167541504\n",
      "Epoch 1, batch 1061, loss = 4.933446884155273\n",
      "Epoch 1, batch 1062, loss = 5.094521522521973\n",
      "Epoch 1, batch 1063, loss = 4.823258399963379\n",
      "Epoch 1, batch 1064, loss = 4.941446781158447\n",
      "Epoch 1, batch 1065, loss = 4.9857401847839355\n",
      "Epoch 1, batch 1066, loss = 4.958600997924805\n",
      "Epoch 1, batch 1067, loss = 5.132390975952148\n",
      "Epoch 1, batch 1068, loss = 4.900775909423828\n",
      "Epoch 1, batch 1069, loss = 4.933501720428467\n",
      "Epoch 1, batch 1070, loss = 4.993062973022461\n",
      "Epoch 1, batch 1071, loss = 4.967874050140381\n",
      "Epoch 1, batch 1072, loss = 4.966418266296387\n",
      "Epoch 1, batch 1073, loss = 5.01362419128418\n",
      "Epoch 1, batch 1074, loss = 4.9464921951293945\n",
      "Epoch 1, batch 1075, loss = 5.109135627746582\n",
      "Epoch 1, batch 1076, loss = 5.046828269958496\n",
      "Epoch 1, batch 1077, loss = 5.137900352478027\n",
      "Epoch 1, batch 1078, loss = 4.982043743133545\n",
      "Epoch 1, batch 1079, loss = 4.888787269592285\n",
      "Epoch 1, batch 1080, loss = 4.9962615966796875\n",
      "Epoch 1, batch 1081, loss = 5.140837669372559\n",
      "Epoch 1, batch 1082, loss = 5.191393852233887\n",
      "Epoch 1, batch 1083, loss = 4.848932266235352\n",
      "Epoch 1, batch 1084, loss = 4.842566013336182\n",
      "Epoch 1, batch 1085, loss = 4.936217308044434\n",
      "Epoch 1, batch 1086, loss = 4.7437005043029785\n",
      "Epoch 1, batch 1087, loss = 4.7914137840271\n",
      "Epoch 1, batch 1088, loss = 4.615905284881592\n",
      "Epoch 1, batch 1089, loss = 4.780880451202393\n",
      "Epoch 1, batch 1090, loss = 4.82356595993042\n",
      "Epoch 1, batch 1091, loss = 4.707499980926514\n",
      "Epoch 1, batch 1092, loss = 4.881418704986572\n",
      "Epoch 1, batch 1093, loss = 4.668710708618164\n",
      "Epoch 1, batch 1094, loss = 4.505981922149658\n",
      "Epoch 1, batch 1095, loss = 4.818393707275391\n",
      "Epoch 1, batch 1096, loss = 4.880000114440918\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, batch 1097, loss = 4.895880699157715\n",
      "Epoch 1, batch 1098, loss = 4.798929691314697\n",
      "Epoch 1, batch 1099, loss = 4.8261871337890625\n",
      "Epoch 1, batch 1100, loss = 4.688638210296631\n",
      "Epoch 1, batch 1101, loss = 4.926206111907959\n",
      "Epoch 1, batch 1102, loss = 4.684839725494385\n",
      "Epoch 1, batch 1103, loss = 4.962404251098633\n",
      "Epoch 1, batch 1104, loss = 4.813090801239014\n",
      "Epoch 1, batch 1105, loss = 4.832010269165039\n",
      "Epoch 1, batch 1106, loss = 4.869467735290527\n",
      "Epoch 1, batch 1107, loss = 5.031116485595703\n",
      "Epoch 1, batch 1108, loss = 4.939477443695068\n",
      "Epoch 1, batch 1109, loss = 4.829483985900879\n",
      "Epoch 1, batch 1110, loss = 4.756002426147461\n",
      "Epoch 1, batch 1111, loss = 4.897763252258301\n",
      "Epoch 1, batch 1112, loss = 4.923279762268066\n",
      "Epoch 1, batch 1113, loss = 4.984637260437012\n",
      "Epoch 1, batch 1114, loss = 4.860135555267334\n",
      "Epoch 1, batch 1115, loss = 4.972149848937988\n",
      "Epoch 1, batch 1116, loss = 5.016913414001465\n",
      "Epoch 1, batch 1117, loss = 5.095404148101807\n",
      "Epoch 1, batch 1118, loss = 4.942789077758789\n",
      "Epoch 1, batch 1119, loss = 5.003047943115234\n",
      "Epoch 1, batch 1120, loss = 4.971975803375244\n",
      "Epoch 1, batch 1121, loss = 5.031267166137695\n",
      "Epoch 1, batch 1122, loss = 4.9079790115356445\n",
      "Epoch 1, batch 1123, loss = 4.978843688964844\n",
      "Epoch 1, batch 1124, loss = 4.986820697784424\n",
      "Epoch 1, batch 1125, loss = 5.061306476593018\n",
      "Epoch 1, batch 1126, loss = 5.005603313446045\n",
      "Epoch 1, batch 1127, loss = 5.061415672302246\n",
      "Epoch 1, batch 1128, loss = 5.158687591552734\n",
      "Epoch 1, batch 1129, loss = 4.946783065795898\n",
      "Epoch 1, batch 1130, loss = 4.992852210998535\n",
      "Epoch 1, batch 1131, loss = 4.684225082397461\n",
      "Epoch 1, batch 1132, loss = 4.832674980163574\n",
      "Epoch 1, batch 1133, loss = 4.750277042388916\n",
      "Epoch 1, batch 1134, loss = 4.69555139541626\n",
      "Epoch 1, batch 1135, loss = 4.966403961181641\n",
      "Epoch 1, batch 1136, loss = 4.671064853668213\n",
      "Epoch 1, batch 1137, loss = 4.89233922958374\n",
      "Epoch 1, batch 1138, loss = 4.776609420776367\n",
      "Epoch 1, batch 1139, loss = 4.888496398925781\n",
      "Epoch 1, batch 1140, loss = 4.818924903869629\n",
      "Epoch 1, batch 1141, loss = 4.848708152770996\n",
      "Epoch 1, batch 1142, loss = 4.911273956298828\n",
      "Epoch 1, batch 1143, loss = 4.912654876708984\n",
      "Epoch 1, batch 1144, loss = 4.824926853179932\n",
      "Epoch 1, batch 1145, loss = 4.94863224029541\n",
      "Epoch 1, batch 1146, loss = 4.874544143676758\n",
      "Epoch 1, batch 1147, loss = 4.862394332885742\n",
      "Epoch 1, batch 1148, loss = 4.8371148109436035\n",
      "Epoch 1, batch 1149, loss = 4.9767022132873535\n",
      "Epoch 1, batch 1150, loss = 4.668214797973633\n",
      "Epoch 1, batch 1151, loss = 4.96448278427124\n",
      "Epoch 1, batch 1152, loss = 5.021981239318848\n",
      "Epoch 1, batch 1153, loss = 4.943496227264404\n",
      "Epoch 1, batch 1154, loss = 4.9965314865112305\n",
      "Epoch 1, batch 1155, loss = 5.056190013885498\n",
      "Epoch 1, batch 1156, loss = 4.978602886199951\n",
      "Epoch 1, batch 1157, loss = 4.955533504486084\n",
      "Epoch 1, batch 1158, loss = 4.93989896774292\n",
      "Epoch 1, batch 1159, loss = 5.05224609375\n",
      "Epoch 1, batch 1160, loss = 5.041038990020752\n",
      "Epoch 1, batch 1161, loss = 4.862533092498779\n",
      "Epoch 1, batch 1162, loss = 4.9876179695129395\n",
      "Epoch 1, batch 1163, loss = 4.979334354400635\n",
      "Epoch 1, batch 1164, loss = 4.695672988891602\n",
      "Epoch 1, batch 1165, loss = 4.691769599914551\n",
      "Epoch 1, batch 1166, loss = 4.561466693878174\n",
      "Epoch 1, batch 1167, loss = 4.699458599090576\n",
      "Epoch 1, batch 1168, loss = 4.827395439147949\n",
      "Epoch 1, batch 1169, loss = 4.89788818359375\n",
      "Epoch 1, batch 1170, loss = 4.978137493133545\n",
      "Epoch 1, batch 1171, loss = 4.868677616119385\n",
      "Epoch 1, batch 1172, loss = 4.897271156311035\n",
      "Epoch 1, batch 1173, loss = 4.853360176086426\n",
      "Epoch 1, batch 1174, loss = 4.873416900634766\n",
      "Epoch 1, batch 1175, loss = 5.018435001373291\n",
      "Epoch 1, batch 1176, loss = 4.977505683898926\n",
      "Epoch 1, batch 1177, loss = 4.88916540145874\n",
      "Epoch 1, batch 1178, loss = 5.068021297454834\n",
      "Epoch 1, batch 1179, loss = 4.972280025482178\n",
      "Epoch 1, batch 1180, loss = 4.918879985809326\n",
      "Epoch 1, batch 1181, loss = 4.9246602058410645\n",
      "Epoch 1, batch 1182, loss = 4.9019083976745605\n",
      "Epoch 1, batch 1183, loss = 4.953303337097168\n",
      "Epoch 1, batch 1184, loss = 5.091640949249268\n",
      "Epoch 1, batch 1185, loss = 4.991664886474609\n",
      "Epoch 1, batch 1186, loss = 4.875672340393066\n",
      "Epoch 1, batch 1187, loss = 4.798035621643066\n",
      "Epoch 1, batch 1188, loss = 4.813714027404785\n",
      "Epoch 1, batch 1189, loss = 4.792591571807861\n",
      "Epoch 1, batch 1190, loss = 4.729015827178955\n",
      "Epoch 1, batch 1191, loss = 4.759722709655762\n",
      "Epoch 1, batch 1192, loss = 4.744231224060059\n",
      "Epoch 1, batch 1193, loss = 4.784558296203613\n",
      "Epoch 1, batch 1194, loss = 4.814974784851074\n",
      "Epoch 1, batch 1195, loss = 4.881180286407471\n",
      "Epoch 1, batch 1196, loss = 4.925127983093262\n",
      "Epoch 1, batch 1197, loss = 4.9729766845703125\n",
      "Epoch 1, batch 1198, loss = 5.0403971672058105\n",
      "Epoch 1, batch 1199, loss = 4.9451494216918945\n",
      "Epoch 1, batch 1200, loss = 4.66105842590332\n",
      "Epoch 1, batch 1201, loss = 4.833076000213623\n",
      "Epoch 1, batch 1202, loss = 4.841219902038574\n",
      "Epoch 1, batch 1203, loss = 4.727422714233398\n",
      "Epoch 1, batch 1204, loss = 4.784106731414795\n",
      "Epoch 1, batch 1205, loss = 4.870964050292969\n",
      "Epoch 1, batch 1206, loss = 4.965845584869385\n",
      "Epoch 1, batch 1207, loss = 5.007858753204346\n",
      "Epoch 1, batch 1208, loss = 4.924386024475098\n",
      "Epoch 1, batch 1209, loss = 4.853457927703857\n",
      "Epoch 1, batch 1210, loss = 4.87530517578125\n",
      "Epoch 1, batch 1211, loss = 4.899647235870361\n",
      "Epoch 1, batch 1212, loss = 4.850830078125\n",
      "Epoch 1, batch 1213, loss = 4.786661624908447\n",
      "Epoch 1, batch 1214, loss = 4.961766242980957\n",
      "Epoch 1, batch 1215, loss = 4.879481792449951\n",
      "Epoch 1, batch 1216, loss = 4.76997709274292\n",
      "Epoch 2, batch 1, loss = 4.919001579284668\n",
      "Epoch 2, batch 2, loss = 5.1063008308410645\n",
      "Epoch 2, batch 3, loss = 5.253974437713623\n",
      "Epoch 2, batch 4, loss = 5.4767656326293945\n",
      "Epoch 2, batch 5, loss = 5.173770427703857\n",
      "Epoch 2, batch 6, loss = 4.859335899353027\n",
      "Epoch 2, batch 7, loss = 5.274446964263916\n",
      "Epoch 2, batch 8, loss = 4.974451065063477\n",
      "Epoch 2, batch 9, loss = 4.959996223449707\n",
      "Epoch 2, batch 10, loss = 4.847084999084473\n",
      "Epoch 2, batch 11, loss = 5.123332977294922\n",
      "Epoch 2, batch 12, loss = 5.359459400177002\n",
      "Epoch 2, batch 13, loss = 5.100940704345703\n",
      "Epoch 2, batch 14, loss = 5.002004146575928\n",
      "Epoch 2, batch 15, loss = 4.762684345245361\n",
      "Epoch 2, batch 16, loss = 5.058493614196777\n",
      "Epoch 2, batch 17, loss = 4.880054473876953\n",
      "Epoch 2, batch 18, loss = 4.624574661254883\n",
      "Epoch 2, batch 19, loss = 4.8124494552612305\n",
      "Epoch 2, batch 20, loss = 4.795692443847656\n",
      "Epoch 2, batch 21, loss = 4.852794170379639\n",
      "Epoch 2, batch 22, loss = 4.945745944976807\n",
      "Epoch 2, batch 23, loss = 5.043888092041016\n",
      "Epoch 2, batch 24, loss = 4.85145378112793\n",
      "Epoch 2, batch 25, loss = 4.62860107421875\n",
      "Epoch 2, batch 26, loss = 4.872348785400391\n",
      "Epoch 2, batch 27, loss = 4.701954364776611\n",
      "Epoch 2, batch 28, loss = 4.7365922927856445\n",
      "Epoch 2, batch 29, loss = 4.605576992034912\n",
      "Epoch 2, batch 30, loss = 4.8208537101745605\n",
      "Epoch 2, batch 31, loss = 4.81859016418457\n",
      "Epoch 2, batch 32, loss = 4.750512599945068\n",
      "Epoch 2, batch 33, loss = 4.740357875823975\n",
      "Epoch 2, batch 34, loss = 5.052731990814209\n",
      "Epoch 2, batch 35, loss = 4.732566833496094\n",
      "Epoch 2, batch 36, loss = 4.629435062408447\n",
      "Epoch 2, batch 37, loss = 4.905108451843262\n",
      "Epoch 2, batch 38, loss = 4.772860050201416\n",
      "Epoch 2, batch 39, loss = 4.476660251617432\n",
      "Epoch 2, batch 40, loss = 4.775076866149902\n",
      "Epoch 2, batch 41, loss = 4.8728461265563965\n",
      "Epoch 2, batch 42, loss = 4.673487186431885\n",
      "Epoch 2, batch 43, loss = 4.667374610900879\n",
      "Epoch 2, batch 44, loss = 4.426197528839111\n",
      "Epoch 2, batch 45, loss = 4.899623394012451\n",
      "Epoch 2, batch 46, loss = 4.869112491607666\n",
      "Epoch 2, batch 47, loss = 4.659457206726074\n",
      "Epoch 2, batch 48, loss = 4.614902496337891\n",
      "Epoch 2, batch 49, loss = 4.6914753913879395\n",
      "Epoch 2, batch 50, loss = 4.693548679351807\n",
      "Epoch 2, batch 51, loss = 4.600095748901367\n",
      "Epoch 2, batch 52, loss = 4.41957950592041\n",
      "Epoch 2, batch 53, loss = 4.439465522766113\n",
      "Epoch 2, batch 54, loss = 4.514004707336426\n",
      "Epoch 2, batch 55, loss = 4.423266410827637\n",
      "Epoch 2, batch 56, loss = 4.373098850250244\n",
      "Epoch 2, batch 57, loss = 4.467430114746094\n",
      "Epoch 2, batch 58, loss = 4.582640647888184\n",
      "Epoch 2, batch 59, loss = 4.645681858062744\n",
      "Epoch 2, batch 60, loss = 4.929131031036377\n",
      "Epoch 2, batch 61, loss = 4.883963108062744\n",
      "Epoch 2, batch 62, loss = 4.786706447601318\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, batch 63, loss = 4.774672031402588\n",
      "Epoch 2, batch 64, loss = 4.834579944610596\n",
      "Epoch 2, batch 65, loss = 4.592548370361328\n",
      "Epoch 2, batch 66, loss = 4.697696685791016\n",
      "Epoch 2, batch 67, loss = 4.725263595581055\n",
      "Epoch 2, batch 68, loss = 4.764459133148193\n",
      "Epoch 2, batch 69, loss = 4.660142421722412\n",
      "Epoch 2, batch 70, loss = 4.606378555297852\n",
      "Epoch 2, batch 71, loss = 4.748021125793457\n",
      "Epoch 2, batch 72, loss = 4.554324626922607\n",
      "Epoch 2, batch 73, loss = 4.642367839813232\n",
      "Epoch 2, batch 74, loss = 4.649215221405029\n",
      "Epoch 2, batch 75, loss = 4.539402008056641\n",
      "Epoch 2, batch 76, loss = 4.647367477416992\n",
      "Epoch 2, batch 77, loss = 4.529375076293945\n",
      "Epoch 2, batch 78, loss = 4.753905773162842\n",
      "Epoch 2, batch 79, loss = 4.519237518310547\n",
      "Epoch 2, batch 80, loss = 4.511504650115967\n",
      "Epoch 2, batch 81, loss = 4.646632671356201\n",
      "Epoch 2, batch 82, loss = 4.799868106842041\n",
      "Epoch 2, batch 83, loss = 4.855743885040283\n",
      "Epoch 2, batch 84, loss = 4.891713619232178\n",
      "Epoch 2, batch 85, loss = 4.834053039550781\n",
      "Epoch 2, batch 86, loss = 5.024721622467041\n",
      "Epoch 2, batch 87, loss = 4.702824592590332\n",
      "Epoch 2, batch 88, loss = 4.869303226470947\n",
      "Epoch 2, batch 89, loss = 4.898914337158203\n",
      "Epoch 2, batch 90, loss = 4.814956188201904\n",
      "Epoch 2, batch 91, loss = 4.85613489151001\n",
      "Epoch 2, batch 92, loss = 4.870826721191406\n",
      "Epoch 2, batch 93, loss = 4.780825614929199\n",
      "Epoch 2, batch 94, loss = 4.9427690505981445\n",
      "Epoch 2, batch 95, loss = 4.758537292480469\n",
      "Epoch 2, batch 96, loss = 4.989333152770996\n",
      "Epoch 2, batch 97, loss = 4.579792022705078\n",
      "Epoch 2, batch 98, loss = 4.708436012268066\n",
      "Epoch 2, batch 99, loss = 4.710379123687744\n",
      "Epoch 2, batch 100, loss = 4.7645745277404785\n",
      "Epoch 2, batch 101, loss = 4.811674118041992\n",
      "Epoch 2, batch 102, loss = 4.81135368347168\n",
      "Epoch 2, batch 103, loss = 4.973871231079102\n",
      "Epoch 2, batch 104, loss = 4.890859603881836\n",
      "Epoch 2, batch 105, loss = 4.862527847290039\n",
      "Epoch 2, batch 106, loss = 4.8269758224487305\n",
      "Epoch 2, batch 107, loss = 4.911907196044922\n",
      "Epoch 2, batch 108, loss = 5.0136494636535645\n",
      "Epoch 2, batch 109, loss = 4.879648208618164\n",
      "Epoch 2, batch 110, loss = 4.871787071228027\n",
      "Epoch 2, batch 111, loss = 4.852425575256348\n",
      "Epoch 2, batch 112, loss = 4.939571857452393\n",
      "Epoch 2, batch 113, loss = 4.763781547546387\n",
      "Epoch 2, batch 114, loss = 4.843041896820068\n",
      "Epoch 2, batch 115, loss = 4.817843437194824\n",
      "Epoch 2, batch 116, loss = 4.766994476318359\n",
      "Epoch 2, batch 117, loss = 4.995935916900635\n",
      "Epoch 2, batch 118, loss = 5.064208030700684\n",
      "Epoch 2, batch 119, loss = 5.102153301239014\n",
      "Epoch 2, batch 120, loss = 4.85330057144165\n",
      "Epoch 2, batch 121, loss = 4.875534534454346\n",
      "Epoch 2, batch 122, loss = 4.887217044830322\n",
      "Epoch 2, batch 123, loss = 4.826881408691406\n",
      "Epoch 2, batch 124, loss = 4.976166248321533\n",
      "Epoch 2, batch 125, loss = 4.894349098205566\n",
      "Epoch 2, batch 126, loss = 4.8709235191345215\n",
      "Epoch 2, batch 127, loss = 4.924607753753662\n",
      "Epoch 2, batch 128, loss = 4.884839057922363\n",
      "Epoch 2, batch 129, loss = 4.969644069671631\n",
      "Epoch 2, batch 130, loss = 5.0809197425842285\n",
      "Epoch 2, batch 131, loss = 4.904870510101318\n",
      "Epoch 2, batch 132, loss = 4.977548599243164\n",
      "Epoch 2, batch 133, loss = 4.938555717468262\n",
      "Epoch 2, batch 134, loss = 4.970578670501709\n",
      "Epoch 2, batch 135, loss = 4.93546724319458\n",
      "Epoch 2, batch 136, loss = 5.016592979431152\n",
      "Epoch 2, batch 137, loss = 4.965519428253174\n",
      "Epoch 2, batch 138, loss = 4.88348913192749\n",
      "Epoch 2, batch 139, loss = 4.9712982177734375\n",
      "Epoch 2, batch 140, loss = 4.846343040466309\n",
      "Epoch 2, batch 141, loss = 5.01533842086792\n",
      "Epoch 2, batch 142, loss = 5.042886734008789\n",
      "Epoch 2, batch 143, loss = 4.9111738204956055\n",
      "Epoch 2, batch 144, loss = 5.02288293838501\n",
      "Epoch 2, batch 145, loss = 5.013847827911377\n",
      "Epoch 2, batch 146, loss = 5.094971179962158\n",
      "Epoch 2, batch 147, loss = 4.950297832489014\n",
      "Epoch 2, batch 148, loss = 5.032002925872803\n",
      "Epoch 2, batch 149, loss = 4.950739860534668\n",
      "Epoch 2, batch 150, loss = 5.026352405548096\n",
      "Epoch 2, batch 151, loss = 4.932112216949463\n",
      "Epoch 2, batch 152, loss = 4.894816875457764\n",
      "Epoch 2, batch 153, loss = 5.0342817306518555\n",
      "Epoch 2, batch 154, loss = 4.935497760772705\n",
      "Epoch 2, batch 155, loss = 4.892593860626221\n",
      "Epoch 2, batch 156, loss = 5.0210418701171875\n",
      "Epoch 2, batch 157, loss = 5.038382530212402\n",
      "Epoch 2, batch 158, loss = 5.12217378616333\n",
      "Epoch 2, batch 159, loss = 4.920377731323242\n",
      "Epoch 2, batch 160, loss = 5.152796268463135\n",
      "Epoch 2, batch 161, loss = 5.199154853820801\n",
      "Epoch 2, batch 162, loss = 5.380003929138184\n",
      "Epoch 2, batch 163, loss = 4.891607284545898\n",
      "Epoch 2, batch 164, loss = 4.957159519195557\n",
      "Epoch 2, batch 165, loss = 5.080756664276123\n",
      "Epoch 2, batch 166, loss = 5.157880783081055\n",
      "Epoch 2, batch 167, loss = 4.96077823638916\n",
      "Epoch 2, batch 168, loss = 4.81874418258667\n",
      "Epoch 2, batch 169, loss = 4.931696891784668\n",
      "Epoch 2, batch 170, loss = 5.1091694831848145\n",
      "Epoch 2, batch 171, loss = 5.071263313293457\n",
      "Epoch 2, batch 172, loss = 5.043149948120117\n",
      "Epoch 2, batch 173, loss = 5.041626930236816\n",
      "Epoch 2, batch 174, loss = 4.96207857131958\n",
      "Epoch 2, batch 175, loss = 5.0838470458984375\n",
      "Epoch 2, batch 176, loss = 4.929547309875488\n",
      "Epoch 2, batch 177, loss = 4.7602386474609375\n",
      "Epoch 2, batch 178, loss = 4.927168369293213\n",
      "Epoch 2, batch 179, loss = 4.7605438232421875\n",
      "Epoch 2, batch 180, loss = 4.647994041442871\n",
      "Epoch 2, batch 181, loss = 4.751675605773926\n",
      "Epoch 2, batch 182, loss = 4.7245073318481445\n",
      "Epoch 2, batch 183, loss = 4.996033668518066\n",
      "Epoch 2, batch 184, loss = 4.763777256011963\n",
      "Epoch 2, batch 185, loss = 4.6340227127075195\n",
      "Epoch 2, batch 186, loss = 5.170517921447754\n",
      "Epoch 2, batch 187, loss = 5.057406425476074\n",
      "Epoch 2, batch 188, loss = 4.845478057861328\n",
      "Epoch 2, batch 189, loss = 4.775096416473389\n",
      "Epoch 2, batch 190, loss = 4.687892436981201\n",
      "Epoch 2, batch 191, loss = 4.950796127319336\n",
      "Epoch 2, batch 192, loss = 4.88455057144165\n",
      "Epoch 2, batch 193, loss = 4.883217811584473\n",
      "Epoch 2, batch 194, loss = 4.862248420715332\n",
      "Epoch 2, batch 195, loss = 4.95894718170166\n",
      "Epoch 2, batch 196, loss = 4.958126068115234\n",
      "Epoch 2, batch 197, loss = 4.8834638595581055\n",
      "Epoch 2, batch 198, loss = 5.094722270965576\n",
      "Epoch 2, batch 199, loss = 4.872476577758789\n",
      "Epoch 2, batch 200, loss = 4.512676239013672\n",
      "Epoch 2, batch 201, loss = 4.690533638000488\n",
      "Epoch 2, batch 202, loss = 4.91588830947876\n",
      "Epoch 2, batch 203, loss = 4.6989312171936035\n",
      "Epoch 2, batch 204, loss = 4.767893314361572\n",
      "Epoch 2, batch 205, loss = 4.640410423278809\n",
      "Epoch 2, batch 206, loss = 4.589245796203613\n",
      "Epoch 2, batch 207, loss = 4.499063968658447\n",
      "Epoch 2, batch 208, loss = 4.507321834564209\n",
      "Epoch 2, batch 209, loss = 4.720208168029785\n",
      "Epoch 2, batch 210, loss = 4.729560852050781\n",
      "Epoch 2, batch 211, loss = 4.610642433166504\n",
      "Epoch 2, batch 212, loss = 4.356191158294678\n",
      "Epoch 2, batch 213, loss = 4.791680812835693\n",
      "Epoch 2, batch 214, loss = 4.495789527893066\n",
      "Epoch 2, batch 215, loss = 4.52609920501709\n",
      "Epoch 2, batch 216, loss = 4.426053047180176\n",
      "Epoch 2, batch 217, loss = 4.6308512687683105\n",
      "Epoch 2, batch 218, loss = 4.601289749145508\n",
      "Epoch 2, batch 219, loss = 4.705551624298096\n",
      "Epoch 2, batch 220, loss = 4.689208984375\n",
      "Epoch 2, batch 221, loss = 4.891426086425781\n",
      "Epoch 2, batch 222, loss = 4.898532390594482\n",
      "Epoch 2, batch 223, loss = 4.671746730804443\n",
      "Epoch 2, batch 224, loss = 4.754683017730713\n",
      "Epoch 2, batch 225, loss = 4.845373630523682\n",
      "Epoch 2, batch 226, loss = 4.657529830932617\n",
      "Epoch 2, batch 227, loss = 4.716169357299805\n",
      "Epoch 2, batch 228, loss = 4.749027729034424\n",
      "Epoch 2, batch 229, loss = 4.8822340965271\n",
      "Epoch 2, batch 230, loss = 4.907936096191406\n",
      "Epoch 2, batch 231, loss = 4.866674423217773\n",
      "Epoch 2, batch 232, loss = 4.604349613189697\n",
      "Epoch 2, batch 233, loss = 4.62927770614624\n",
      "Epoch 2, batch 234, loss = 4.713468074798584\n",
      "Epoch 2, batch 235, loss = 4.61931848526001\n",
      "Epoch 2, batch 236, loss = 4.81797456741333\n",
      "Epoch 2, batch 237, loss = 4.561824798583984\n",
      "Epoch 2, batch 238, loss = 4.628049850463867\n",
      "Epoch 2, batch 239, loss = 4.779722690582275\n",
      "Epoch 2, batch 240, loss = 4.505347728729248\n",
      "Epoch 2, batch 241, loss = 4.567135334014893\n",
      "Epoch 2, batch 242, loss = 4.720954418182373\n",
      "Epoch 2, batch 243, loss = 4.6801533699035645\n",
      "Epoch 2, batch 244, loss = 4.67207145690918\n",
      "Epoch 2, batch 245, loss = 4.612752437591553\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, batch 246, loss = 4.80899715423584\n",
      "Epoch 2, batch 247, loss = 4.725472450256348\n",
      "Epoch 2, batch 248, loss = 4.965302467346191\n",
      "Epoch 2, batch 249, loss = 4.876402854919434\n",
      "Epoch 2, batch 250, loss = 4.710331439971924\n",
      "Epoch 2, batch 251, loss = 4.8229827880859375\n",
      "Epoch 2, batch 252, loss = 4.725132942199707\n",
      "Epoch 2, batch 253, loss = 4.8316779136657715\n",
      "Epoch 2, batch 254, loss = 4.839599609375\n",
      "Epoch 2, batch 255, loss = 4.963573932647705\n",
      "Epoch 2, batch 256, loss = 4.874353885650635\n",
      "Epoch 2, batch 257, loss = 4.8206024169921875\n",
      "Epoch 2, batch 258, loss = 4.7366108894348145\n",
      "Epoch 2, batch 259, loss = 4.80191707611084\n",
      "Epoch 2, batch 260, loss = 4.866938591003418\n",
      "Epoch 2, batch 261, loss = 4.95264196395874\n",
      "Epoch 2, batch 262, loss = 4.677886009216309\n",
      "Epoch 2, batch 263, loss = 4.8153228759765625\n",
      "Epoch 2, batch 264, loss = 4.757725238800049\n",
      "Epoch 2, batch 265, loss = 4.788525104522705\n",
      "Epoch 2, batch 266, loss = 4.649202823638916\n",
      "Epoch 2, batch 267, loss = 4.779543876647949\n",
      "Epoch 2, batch 268, loss = 4.71294641494751\n",
      "Epoch 2, batch 269, loss = 4.798089027404785\n",
      "Epoch 2, batch 270, loss = 5.033359050750732\n",
      "Epoch 2, batch 271, loss = 4.867184638977051\n",
      "Epoch 2, batch 272, loss = 4.931859493255615\n",
      "Epoch 2, batch 273, loss = 4.952945709228516\n",
      "Epoch 2, batch 274, loss = 5.004230499267578\n",
      "Epoch 2, batch 275, loss = 4.89266300201416\n",
      "Epoch 2, batch 276, loss = 4.761616230010986\n",
      "Epoch 2, batch 277, loss = 4.965332508087158\n",
      "Epoch 2, batch 278, loss = 5.0174880027771\n",
      "Epoch 2, batch 279, loss = 4.776834011077881\n",
      "Epoch 2, batch 280, loss = 4.9841108322143555\n",
      "Epoch 2, batch 281, loss = 4.916745185852051\n",
      "Epoch 2, batch 282, loss = 4.8751325607299805\n",
      "Epoch 2, batch 283, loss = 5.001032829284668\n",
      "Epoch 2, batch 284, loss = 4.858944892883301\n",
      "Epoch 2, batch 285, loss = 4.780271530151367\n",
      "Epoch 2, batch 286, loss = 4.733614444732666\n",
      "Epoch 2, batch 287, loss = 4.898628234863281\n",
      "Epoch 2, batch 288, loss = 4.838363170623779\n",
      "Epoch 2, batch 289, loss = 4.912726879119873\n",
      "Epoch 2, batch 290, loss = 4.8803629875183105\n",
      "Epoch 2, batch 291, loss = 4.832681655883789\n",
      "Epoch 2, batch 292, loss = 4.955969333648682\n",
      "Epoch 2, batch 293, loss = 4.911645889282227\n",
      "Epoch 2, batch 294, loss = 4.811420440673828\n",
      "Epoch 2, batch 295, loss = 4.986018657684326\n",
      "Epoch 2, batch 296, loss = 5.072654724121094\n",
      "Epoch 2, batch 297, loss = 4.961831092834473\n",
      "Epoch 2, batch 298, loss = 4.880922794342041\n",
      "Epoch 2, batch 299, loss = 4.88109827041626\n",
      "Epoch 2, batch 300, loss = 4.74428129196167\n",
      "Epoch 2, batch 301, loss = 4.84871244430542\n",
      "Epoch 2, batch 302, loss = 4.802905082702637\n",
      "Epoch 2, batch 303, loss = 4.894305229187012\n",
      "Epoch 2, batch 304, loss = 5.004110336303711\n",
      "Epoch 2, batch 305, loss = 4.94351863861084\n",
      "Epoch 2, batch 306, loss = 4.918061256408691\n",
      "Epoch 2, batch 307, loss = 4.794378280639648\n",
      "Epoch 2, batch 308, loss = 5.065985202789307\n",
      "Epoch 2, batch 309, loss = 4.981717586517334\n",
      "Epoch 2, batch 310, loss = 4.889609336853027\n",
      "Epoch 2, batch 311, loss = 4.830195426940918\n",
      "Epoch 2, batch 312, loss = 4.982153415679932\n",
      "Epoch 2, batch 313, loss = 4.873648166656494\n",
      "Epoch 2, batch 314, loss = 4.980798721313477\n",
      "Epoch 2, batch 315, loss = 4.936990737915039\n",
      "Epoch 2, batch 316, loss = 4.9041643142700195\n",
      "Epoch 2, batch 317, loss = 4.818141937255859\n",
      "Epoch 2, batch 318, loss = 5.1084065437316895\n",
      "Epoch 2, batch 319, loss = 4.945125102996826\n",
      "Epoch 2, batch 320, loss = 5.116228103637695\n",
      "Epoch 2, batch 321, loss = 5.016137599945068\n",
      "Epoch 2, batch 322, loss = 4.902444839477539\n",
      "Epoch 2, batch 323, loss = 4.969089984893799\n",
      "Epoch 2, batch 324, loss = 5.115256309509277\n",
      "Epoch 2, batch 325, loss = 4.940985202789307\n",
      "Epoch 2, batch 326, loss = 4.9287261962890625\n",
      "Epoch 2, batch 327, loss = 4.914870262145996\n",
      "Epoch 2, batch 328, loss = 4.8907084465026855\n",
      "Epoch 2, batch 329, loss = 4.879486083984375\n",
      "Epoch 2, batch 330, loss = 4.791974067687988\n",
      "Epoch 2, batch 331, loss = 5.057213306427002\n",
      "Epoch 2, batch 332, loss = 4.881707191467285\n",
      "Epoch 2, batch 333, loss = 4.945852279663086\n",
      "Epoch 2, batch 334, loss = 4.954659461975098\n",
      "Epoch 2, batch 335, loss = 5.006776332855225\n",
      "Epoch 2, batch 336, loss = 4.9021477699279785\n",
      "Epoch 2, batch 337, loss = 4.872501850128174\n",
      "Epoch 2, batch 338, loss = 4.969020366668701\n",
      "Epoch 2, batch 339, loss = 4.946837425231934\n",
      "Epoch 2, batch 340, loss = 5.202760696411133\n",
      "Epoch 2, batch 341, loss = 5.1757001876831055\n",
      "Epoch 2, batch 342, loss = 5.270404815673828\n",
      "Epoch 2, batch 343, loss = 5.2797627449035645\n",
      "Epoch 2, batch 344, loss = 4.913548469543457\n",
      "Epoch 2, batch 345, loss = 4.976349830627441\n",
      "Epoch 2, batch 346, loss = 4.862401962280273\n",
      "Epoch 2, batch 347, loss = 5.099661350250244\n",
      "Epoch 2, batch 348, loss = 5.200977325439453\n",
      "Epoch 2, batch 349, loss = 4.893528938293457\n",
      "Epoch 2, batch 350, loss = 5.06441593170166\n",
      "Epoch 2, batch 351, loss = 4.809847354888916\n",
      "Epoch 2, batch 352, loss = 4.996771812438965\n",
      "Epoch 2, batch 353, loss = 4.85545015335083\n",
      "Epoch 2, batch 354, loss = 4.9185333251953125\n",
      "Epoch 2, batch 355, loss = 5.126530647277832\n",
      "Epoch 2, batch 356, loss = 5.025814056396484\n",
      "Epoch 2, batch 357, loss = 4.961932182312012\n",
      "Epoch 2, batch 358, loss = 5.025321006774902\n",
      "Epoch 2, batch 359, loss = 4.9366350173950195\n",
      "Epoch 2, batch 360, loss = 4.838701248168945\n",
      "Epoch 2, batch 361, loss = 4.973175525665283\n",
      "Epoch 2, batch 362, loss = 4.870383262634277\n",
      "Epoch 2, batch 363, loss = 4.720350742340088\n",
      "Epoch 2, batch 364, loss = 4.9596967697143555\n",
      "Epoch 2, batch 365, loss = 4.799356937408447\n",
      "Epoch 2, batch 366, loss = 4.7263288497924805\n",
      "Epoch 2, batch 367, loss = 4.798146724700928\n",
      "Epoch 2, batch 368, loss = 4.927701473236084\n",
      "Epoch 2, batch 369, loss = 4.832397937774658\n",
      "Epoch 2, batch 370, loss = 4.713743686676025\n",
      "Epoch 2, batch 371, loss = 4.876941204071045\n",
      "Epoch 2, batch 372, loss = 4.778501987457275\n",
      "Epoch 2, batch 373, loss = 4.640796184539795\n",
      "Epoch 2, batch 374, loss = 4.996565818786621\n",
      "Epoch 2, batch 375, loss = 4.562403678894043\n",
      "Epoch 2, batch 376, loss = 4.686124324798584\n",
      "Epoch 2, batch 377, loss = 4.9052324295043945\n",
      "Epoch 2, batch 378, loss = 4.595417022705078\n",
      "Epoch 2, batch 379, loss = 4.6689605712890625\n",
      "Epoch 2, batch 380, loss = 4.795482635498047\n",
      "Epoch 2, batch 381, loss = 4.773867130279541\n",
      "Epoch 2, batch 382, loss = 4.672415733337402\n",
      "Epoch 2, batch 383, loss = 4.608696937561035\n",
      "Epoch 2, batch 384, loss = 4.6615400314331055\n",
      "Epoch 2, batch 385, loss = 4.5150628089904785\n",
      "Epoch 2, batch 386, loss = 4.565932273864746\n",
      "Epoch 2, batch 387, loss = 4.467092037200928\n",
      "Epoch 2, batch 388, loss = 4.669322967529297\n",
      "Epoch 2, batch 389, loss = 4.705513954162598\n",
      "Epoch 2, batch 390, loss = 4.769074440002441\n",
      "Epoch 2, batch 391, loss = 4.510397434234619\n",
      "Epoch 2, batch 392, loss = 4.733473300933838\n",
      "Epoch 2, batch 393, loss = 4.9425201416015625\n",
      "Epoch 2, batch 394, loss = 4.68573522567749\n",
      "Epoch 2, batch 395, loss = 4.713512420654297\n",
      "Epoch 2, batch 396, loss = 4.924557209014893\n",
      "Epoch 2, batch 397, loss = 4.811338901519775\n",
      "Epoch 2, batch 398, loss = 4.653950214385986\n",
      "Epoch 2, batch 399, loss = 4.748598575592041\n",
      "Epoch 2, batch 400, loss = 4.719192028045654\n",
      "Epoch 2, batch 401, loss = 4.896493434906006\n",
      "Epoch 2, batch 402, loss = 4.9769511222839355\n",
      "Epoch 2, batch 403, loss = 5.01675271987915\n",
      "Epoch 2, batch 404, loss = 4.573013782501221\n",
      "Epoch 2, batch 405, loss = 4.705658435821533\n",
      "Epoch 2, batch 406, loss = 5.164708614349365\n",
      "Epoch 2, batch 407, loss = 4.697483539581299\n",
      "Epoch 2, batch 408, loss = 4.8335795402526855\n",
      "Epoch 2, batch 409, loss = 4.881890773773193\n",
      "Epoch 2, batch 410, loss = 4.7242431640625\n",
      "Epoch 2, batch 411, loss = 4.736114025115967\n",
      "Epoch 2, batch 412, loss = 4.568536758422852\n",
      "Epoch 2, batch 413, loss = 4.593934059143066\n",
      "Epoch 2, batch 414, loss = 4.663590908050537\n",
      "Epoch 2, batch 415, loss = 4.641294956207275\n",
      "Epoch 2, batch 416, loss = 4.533512115478516\n",
      "Epoch 2, batch 417, loss = 4.644378662109375\n",
      "Epoch 2, batch 418, loss = 4.676454544067383\n",
      "Epoch 2, batch 419, loss = 4.765617370605469\n",
      "Epoch 2, batch 420, loss = 4.710660457611084\n",
      "Epoch 2, batch 421, loss = 4.957824230194092\n",
      "Epoch 2, batch 422, loss = 4.868167400360107\n",
      "Epoch 2, batch 423, loss = 4.7287068367004395\n",
      "Epoch 2, batch 424, loss = 4.894810199737549\n",
      "Epoch 2, batch 425, loss = 4.840849876403809\n",
      "Epoch 2, batch 426, loss = 4.88248348236084\n",
      "Epoch 2, batch 427, loss = 4.947268009185791\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, batch 428, loss = 4.980357646942139\n",
      "Epoch 2, batch 429, loss = 4.857085704803467\n",
      "Epoch 2, batch 430, loss = 4.6919026374816895\n",
      "Epoch 2, batch 431, loss = 4.940173625946045\n",
      "Epoch 2, batch 432, loss = 4.936397075653076\n",
      "Epoch 2, batch 433, loss = 4.731425762176514\n",
      "Epoch 2, batch 434, loss = 4.82080602645874\n",
      "Epoch 2, batch 435, loss = 4.719088077545166\n",
      "Epoch 2, batch 436, loss = 4.6636810302734375\n",
      "Epoch 2, batch 437, loss = 4.859326362609863\n",
      "Epoch 2, batch 438, loss = 4.745948791503906\n",
      "Epoch 2, batch 439, loss = 4.723484992980957\n",
      "Epoch 2, batch 440, loss = 4.8755974769592285\n",
      "Epoch 2, batch 441, loss = 4.647143840789795\n",
      "Epoch 2, batch 442, loss = 4.8740034103393555\n",
      "Epoch 2, batch 443, loss = 4.951768398284912\n",
      "Epoch 2, batch 444, loss = 5.001911640167236\n",
      "Epoch 2, batch 445, loss = 4.85504150390625\n",
      "Epoch 2, batch 446, loss = 4.97410249710083\n",
      "Epoch 2, batch 447, loss = 4.983949661254883\n",
      "Epoch 2, batch 448, loss = 4.790013790130615\n",
      "Epoch 2, batch 449, loss = 4.879027366638184\n",
      "Epoch 2, batch 450, loss = 4.832736968994141\n",
      "Epoch 2, batch 451, loss = 4.94692325592041\n",
      "Epoch 2, batch 452, loss = 4.898136138916016\n",
      "Epoch 2, batch 453, loss = 5.04442024230957\n",
      "Epoch 2, batch 454, loss = 4.953136444091797\n",
      "Epoch 2, batch 455, loss = 4.869916915893555\n",
      "Epoch 2, batch 456, loss = 4.860743999481201\n",
      "Epoch 2, batch 457, loss = 4.93132209777832\n",
      "Epoch 2, batch 458, loss = 4.8914384841918945\n",
      "Epoch 2, batch 459, loss = 4.987074375152588\n",
      "Epoch 2, batch 460, loss = 4.718107223510742\n",
      "Epoch 2, batch 461, loss = 4.962127208709717\n",
      "Epoch 2, batch 462, loss = 4.980881690979004\n",
      "Epoch 2, batch 463, loss = 5.069780349731445\n",
      "Epoch 2, batch 464, loss = 4.856467247009277\n",
      "Epoch 2, batch 465, loss = 4.946887493133545\n",
      "Epoch 2, batch 466, loss = 4.804269790649414\n",
      "Epoch 2, batch 467, loss = 4.811868667602539\n",
      "Epoch 2, batch 468, loss = 4.942112445831299\n",
      "Epoch 2, batch 469, loss = 5.0053911209106445\n",
      "Epoch 2, batch 470, loss = 4.960176467895508\n",
      "Epoch 2, batch 471, loss = 4.938314437866211\n",
      "Epoch 2, batch 472, loss = 4.838094711303711\n",
      "Epoch 2, batch 473, loss = 4.800831317901611\n",
      "Epoch 2, batch 474, loss = 4.868799209594727\n",
      "Epoch 2, batch 475, loss = 4.729037761688232\n",
      "Epoch 2, batch 476, loss = 4.734752655029297\n",
      "Epoch 2, batch 477, loss = 5.007643699645996\n",
      "Epoch 2, batch 478, loss = 4.88253116607666\n",
      "Epoch 2, batch 479, loss = 4.962761878967285\n",
      "Epoch 2, batch 480, loss = 4.9862470626831055\n",
      "Epoch 2, batch 481, loss = 4.904821872711182\n",
      "Epoch 2, batch 482, loss = 4.977585792541504\n",
      "Epoch 2, batch 483, loss = 4.9460954666137695\n",
      "Epoch 2, batch 484, loss = 4.9408135414123535\n",
      "Epoch 2, batch 485, loss = 5.054612636566162\n",
      "Epoch 2, batch 486, loss = 4.850402355194092\n",
      "Epoch 2, batch 487, loss = 4.74130916595459\n",
      "Epoch 2, batch 488, loss = 4.98600435256958\n",
      "Epoch 2, batch 489, loss = 5.036752700805664\n",
      "Epoch 2, batch 490, loss = 4.979424953460693\n",
      "Epoch 2, batch 491, loss = 4.91647481918335\n",
      "Epoch 2, batch 492, loss = 4.8796162605285645\n",
      "Epoch 2, batch 493, loss = 4.946084499359131\n",
      "Epoch 2, batch 494, loss = 4.9208526611328125\n",
      "Epoch 2, batch 495, loss = 4.894850254058838\n",
      "Epoch 2, batch 496, loss = 4.949028491973877\n",
      "Epoch 2, batch 497, loss = 4.849486827850342\n",
      "Epoch 2, batch 498, loss = 4.86509370803833\n",
      "Epoch 2, batch 499, loss = 4.905755519866943\n",
      "Epoch 2, batch 500, loss = 5.020725250244141\n",
      "Epoch 2, batch 501, loss = 5.041419506072998\n",
      "Epoch 2, batch 502, loss = 4.826579570770264\n",
      "Epoch 2, batch 503, loss = 4.981119155883789\n",
      "Epoch 2, batch 504, loss = 4.909359931945801\n",
      "Epoch 2, batch 505, loss = 4.956149101257324\n",
      "Epoch 2, batch 506, loss = 4.991365432739258\n",
      "Epoch 2, batch 507, loss = 4.925758361816406\n",
      "Epoch 2, batch 508, loss = 4.997832298278809\n",
      "Epoch 2, batch 509, loss = 4.954867839813232\n",
      "Epoch 2, batch 510, loss = 4.898682117462158\n",
      "Epoch 2, batch 511, loss = 4.883040428161621\n",
      "Epoch 2, batch 512, loss = 4.743977069854736\n",
      "Epoch 2, batch 513, loss = 4.966985702514648\n",
      "Epoch 2, batch 514, loss = 4.979987144470215\n",
      "Epoch 2, batch 515, loss = 5.273690223693848\n",
      "Epoch 2, batch 516, loss = 5.305564880371094\n",
      "Epoch 2, batch 517, loss = 4.942074298858643\n",
      "Epoch 2, batch 518, loss = 5.143464088439941\n",
      "Epoch 2, batch 519, loss = 5.094320774078369\n",
      "Epoch 2, batch 520, loss = 5.120009422302246\n",
      "Epoch 2, batch 521, loss = 5.109394550323486\n",
      "Epoch 2, batch 522, loss = 5.304020881652832\n",
      "Epoch 2, batch 523, loss = 4.975295066833496\n",
      "Epoch 2, batch 524, loss = 5.171520233154297\n",
      "Epoch 2, batch 525, loss = 5.217660903930664\n",
      "Epoch 2, batch 526, loss = 5.062416076660156\n",
      "Epoch 2, batch 527, loss = 5.1224045753479\n",
      "Epoch 2, batch 528, loss = 5.017088413238525\n",
      "Epoch 2, batch 529, loss = 4.82230281829834\n",
      "Epoch 2, batch 530, loss = 4.984013080596924\n",
      "Epoch 2, batch 531, loss = 4.926067352294922\n",
      "Epoch 2, batch 532, loss = 4.957679748535156\n",
      "Epoch 2, batch 533, loss = 5.030000686645508\n",
      "Epoch 2, batch 534, loss = 4.835198402404785\n",
      "Epoch 2, batch 535, loss = 4.903347969055176\n",
      "Epoch 2, batch 536, loss = 4.972259521484375\n",
      "Epoch 2, batch 537, loss = 4.94149112701416\n",
      "Epoch 2, batch 538, loss = 4.759369850158691\n",
      "Epoch 2, batch 539, loss = 4.877615928649902\n",
      "Epoch 2, batch 540, loss = 4.7836480140686035\n",
      "Epoch 2, batch 541, loss = 4.80867862701416\n",
      "Epoch 2, batch 542, loss = 4.90694522857666\n",
      "Epoch 2, batch 543, loss = 5.0578742027282715\n",
      "Epoch 2, batch 544, loss = 4.9794158935546875\n",
      "Epoch 2, batch 545, loss = 4.541905403137207\n",
      "Epoch 2, batch 546, loss = 4.769742965698242\n",
      "Epoch 2, batch 547, loss = 4.652432441711426\n",
      "Epoch 2, batch 548, loss = 4.801082134246826\n",
      "Epoch 2, batch 549, loss = 4.987274169921875\n",
      "Epoch 2, batch 550, loss = 4.746028423309326\n",
      "Epoch 2, batch 551, loss = 4.688037872314453\n",
      "Epoch 2, batch 552, loss = 4.6281867027282715\n",
      "Epoch 2, batch 553, loss = 4.664616584777832\n",
      "Epoch 2, batch 554, loss = 4.656349182128906\n",
      "Epoch 2, batch 555, loss = 4.63660192489624\n",
      "Epoch 2, batch 556, loss = 4.671314239501953\n",
      "Epoch 2, batch 557, loss = 4.657227516174316\n",
      "Epoch 2, batch 558, loss = 4.691047191619873\n",
      "Epoch 2, batch 559, loss = 4.643253803253174\n",
      "Epoch 2, batch 560, loss = 4.882556915283203\n",
      "Epoch 2, batch 561, loss = 4.749230861663818\n",
      "Epoch 2, batch 562, loss = 4.67776346206665\n",
      "Epoch 2, batch 563, loss = 4.866745471954346\n",
      "Epoch 2, batch 564, loss = 4.715745449066162\n",
      "Epoch 2, batch 565, loss = 4.6272664070129395\n",
      "Epoch 2, batch 566, loss = 4.851163387298584\n",
      "Epoch 2, batch 567, loss = 4.790441989898682\n",
      "Epoch 2, batch 568, loss = 4.859578609466553\n",
      "Epoch 2, batch 569, loss = 4.845897197723389\n",
      "Epoch 2, batch 570, loss = 4.638493061065674\n",
      "Epoch 2, batch 571, loss = 4.76540994644165\n",
      "Epoch 2, batch 572, loss = 4.666598320007324\n",
      "Epoch 2, batch 573, loss = 4.799799919128418\n",
      "Epoch 2, batch 574, loss = 4.617368698120117\n",
      "Epoch 2, batch 575, loss = 4.681852340698242\n",
      "Epoch 2, batch 576, loss = 4.714402675628662\n",
      "Epoch 2, batch 577, loss = 4.678774356842041\n",
      "Epoch 2, batch 578, loss = 4.568349838256836\n",
      "Epoch 2, batch 579, loss = 4.569624900817871\n",
      "Epoch 2, batch 580, loss = 4.660477638244629\n",
      "Epoch 2, batch 581, loss = 4.67729377746582\n",
      "Epoch 2, batch 582, loss = 4.9355878829956055\n",
      "Epoch 2, batch 583, loss = 4.845459938049316\n",
      "Epoch 2, batch 584, loss = 5.15310001373291\n",
      "Epoch 2, batch 585, loss = 4.841487407684326\n",
      "Epoch 2, batch 586, loss = 4.790401935577393\n",
      "Epoch 2, batch 587, loss = 4.896239757537842\n",
      "Epoch 2, batch 588, loss = 4.911128520965576\n",
      "Epoch 2, batch 589, loss = 4.768566131591797\n",
      "Epoch 2, batch 590, loss = 4.743013858795166\n",
      "Epoch 2, batch 591, loss = 4.79760217666626\n",
      "Epoch 2, batch 592, loss = 4.836880207061768\n",
      "Epoch 2, batch 593, loss = 4.779000282287598\n",
      "Epoch 2, batch 594, loss = 4.857161998748779\n",
      "Epoch 2, batch 595, loss = 5.022387981414795\n",
      "Epoch 2, batch 596, loss = 4.897106647491455\n",
      "Epoch 2, batch 597, loss = 4.855872631072998\n",
      "Epoch 2, batch 598, loss = 4.830832481384277\n",
      "Epoch 2, batch 599, loss = 4.9518890380859375\n",
      "Epoch 2, batch 600, loss = 4.799159526824951\n",
      "Epoch 2, batch 601, loss = 4.6675286293029785\n",
      "Epoch 2, batch 602, loss = 4.658760070800781\n",
      "Epoch 2, batch 603, loss = 4.835501194000244\n",
      "Epoch 2, batch 604, loss = 4.896188259124756\n",
      "Epoch 2, batch 605, loss = 4.940313339233398\n",
      "Epoch 2, batch 606, loss = 4.960236549377441\n",
      "Epoch 2, batch 607, loss = 4.898150444030762\n",
      "Epoch 2, batch 608, loss = 4.960004806518555\n",
      "Epoch 2, batch 609, loss = 4.833927631378174\n",
      "Epoch 2, batch 610, loss = 4.799588203430176\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, batch 611, loss = 4.789205551147461\n",
      "Epoch 2, batch 612, loss = 4.836023330688477\n",
      "Epoch 2, batch 613, loss = 4.861111640930176\n",
      "Epoch 2, batch 614, loss = 4.82060432434082\n",
      "Epoch 2, batch 615, loss = 5.023276329040527\n",
      "Epoch 2, batch 616, loss = 4.93698263168335\n",
      "Epoch 2, batch 617, loss = 4.986858367919922\n",
      "Epoch 2, batch 618, loss = 5.062138080596924\n",
      "Epoch 2, batch 619, loss = 4.841671943664551\n",
      "Epoch 2, batch 620, loss = 4.792093276977539\n",
      "Epoch 2, batch 621, loss = 4.800561904907227\n",
      "Epoch 2, batch 622, loss = 4.940847396850586\n",
      "Epoch 2, batch 623, loss = 4.841614246368408\n",
      "Epoch 2, batch 624, loss = 4.981656074523926\n",
      "Epoch 2, batch 625, loss = 5.002005577087402\n",
      "Epoch 2, batch 626, loss = 4.917489051818848\n",
      "Epoch 2, batch 627, loss = 4.945966720581055\n",
      "Epoch 2, batch 628, loss = 4.870005130767822\n",
      "Epoch 2, batch 629, loss = 4.971165180206299\n",
      "Epoch 2, batch 630, loss = 4.852994918823242\n",
      "Epoch 2, batch 631, loss = 4.888704299926758\n",
      "Epoch 2, batch 632, loss = 5.001876354217529\n",
      "Epoch 2, batch 633, loss = 4.95118522644043\n",
      "Epoch 2, batch 634, loss = 5.115165710449219\n",
      "Epoch 2, batch 635, loss = 4.889760971069336\n",
      "Epoch 2, batch 636, loss = 4.782857894897461\n",
      "Epoch 2, batch 637, loss = 4.925907611846924\n",
      "Epoch 2, batch 638, loss = 4.977559566497803\n",
      "Epoch 2, batch 639, loss = 4.888777732849121\n",
      "Epoch 2, batch 640, loss = 4.990748405456543\n",
      "Epoch 2, batch 641, loss = 4.953546524047852\n",
      "Epoch 2, batch 642, loss = 4.980759143829346\n",
      "Epoch 2, batch 643, loss = 5.01463508605957\n",
      "Epoch 2, batch 644, loss = 4.886043071746826\n",
      "Epoch 2, batch 645, loss = 4.996006965637207\n",
      "Epoch 2, batch 646, loss = 4.8532538414001465\n",
      "Epoch 2, batch 647, loss = 4.853882789611816\n",
      "Epoch 2, batch 648, loss = 4.8787736892700195\n",
      "Epoch 2, batch 649, loss = 4.796835899353027\n",
      "Epoch 2, batch 650, loss = 4.89609956741333\n",
      "Epoch 2, batch 651, loss = 4.943671703338623\n",
      "Epoch 2, batch 652, loss = 4.882619857788086\n",
      "Epoch 2, batch 653, loss = 5.027453899383545\n",
      "Epoch 2, batch 654, loss = 4.942704677581787\n",
      "Epoch 2, batch 655, loss = 4.881119251251221\n",
      "Epoch 2, batch 656, loss = 4.879919528961182\n",
      "Epoch 2, batch 657, loss = 4.846347808837891\n",
      "Epoch 2, batch 658, loss = 4.982326030731201\n",
      "Epoch 2, batch 659, loss = 5.008581161499023\n",
      "Epoch 2, batch 660, loss = 4.937367916107178\n",
      "Epoch 2, batch 661, loss = 5.130636692047119\n",
      "Epoch 2, batch 662, loss = 5.088790416717529\n",
      "Epoch 2, batch 663, loss = 4.976269721984863\n",
      "Epoch 2, batch 664, loss = 4.931633472442627\n",
      "Epoch 2, batch 665, loss = 4.918109893798828\n",
      "Epoch 2, batch 666, loss = 4.872764587402344\n",
      "Epoch 2, batch 667, loss = 4.789168834686279\n",
      "Epoch 2, batch 668, loss = 4.980245590209961\n",
      "Epoch 2, batch 669, loss = 4.999958038330078\n",
      "Epoch 2, batch 670, loss = 4.867375373840332\n",
      "Epoch 2, batch 671, loss = 4.866553783416748\n",
      "Epoch 2, batch 672, loss = 4.902712821960449\n",
      "Epoch 2, batch 673, loss = 4.947274684906006\n",
      "Epoch 2, batch 674, loss = 4.878291606903076\n",
      "Epoch 2, batch 675, loss = 5.0800299644470215\n",
      "Epoch 2, batch 676, loss = 5.346776485443115\n",
      "Epoch 2, batch 677, loss = 5.313484191894531\n",
      "Epoch 2, batch 678, loss = 5.1349778175354\n",
      "Epoch 2, batch 679, loss = 5.102890491485596\n",
      "Epoch 2, batch 680, loss = 5.241065502166748\n",
      "Epoch 2, batch 681, loss = 5.041534423828125\n",
      "Epoch 2, batch 682, loss = 5.055480003356934\n",
      "Epoch 2, batch 683, loss = 5.087244033813477\n",
      "Epoch 2, batch 684, loss = 4.930289268493652\n",
      "Epoch 2, batch 685, loss = 4.856500148773193\n",
      "Epoch 2, batch 686, loss = 5.13539457321167\n",
      "Epoch 2, batch 687, loss = 5.093887805938721\n",
      "Epoch 2, batch 688, loss = 5.072315216064453\n",
      "Epoch 2, batch 689, loss = 5.001625061035156\n",
      "Epoch 2, batch 690, loss = 4.848428249359131\n",
      "Epoch 2, batch 691, loss = 5.032944679260254\n",
      "Epoch 2, batch 692, loss = 4.830561637878418\n",
      "Epoch 2, batch 693, loss = 4.9897141456604\n",
      "Epoch 2, batch 694, loss = 4.83461332321167\n",
      "Epoch 2, batch 695, loss = 4.681535243988037\n",
      "Epoch 2, batch 696, loss = 4.697192192077637\n",
      "Epoch 2, batch 697, loss = 4.93731689453125\n",
      "Epoch 2, batch 698, loss = 4.888367652893066\n",
      "Epoch 2, batch 699, loss = 4.774751663208008\n",
      "Epoch 2, batch 700, loss = 4.854282855987549\n",
      "Epoch 2, batch 701, loss = 4.827887535095215\n",
      "Epoch 2, batch 702, loss = 4.836065769195557\n",
      "Epoch 2, batch 703, loss = 4.761423587799072\n",
      "Epoch 2, batch 704, loss = 4.757994651794434\n",
      "Epoch 2, batch 705, loss = 4.654008388519287\n",
      "Epoch 2, batch 706, loss = 4.595077037811279\n",
      "Epoch 2, batch 707, loss = 4.587928771972656\n",
      "Epoch 2, batch 708, loss = 4.79302453994751\n",
      "Epoch 2, batch 709, loss = 4.801870822906494\n",
      "Epoch 2, batch 710, loss = 4.75665807723999\n",
      "Epoch 2, batch 711, loss = 4.706079959869385\n",
      "Epoch 2, batch 712, loss = 4.807417392730713\n",
      "Epoch 2, batch 713, loss = 4.8102030754089355\n",
      "Epoch 2, batch 714, loss = 4.6701579093933105\n",
      "Epoch 2, batch 715, loss = 4.871583938598633\n",
      "Epoch 2, batch 716, loss = 4.779536724090576\n",
      "Epoch 2, batch 717, loss = 4.698728084564209\n",
      "Epoch 2, batch 718, loss = 4.778377056121826\n",
      "Epoch 2, batch 719, loss = 4.784480571746826\n",
      "Epoch 2, batch 720, loss = 4.789824962615967\n",
      "Epoch 2, batch 721, loss = 4.641620635986328\n",
      "Epoch 2, batch 722, loss = 4.727044582366943\n",
      "Epoch 2, batch 723, loss = 4.885813236236572\n",
      "Epoch 2, batch 724, loss = 4.7745680809021\n",
      "Epoch 2, batch 725, loss = 4.698568820953369\n",
      "Epoch 2, batch 726, loss = 4.662707805633545\n",
      "Epoch 2, batch 727, loss = 4.743228912353516\n",
      "Epoch 2, batch 728, loss = 4.816188812255859\n",
      "Epoch 2, batch 729, loss = 4.780220031738281\n",
      "Epoch 2, batch 730, loss = 4.914844989776611\n",
      "Epoch 2, batch 731, loss = 4.739033222198486\n",
      "Epoch 2, batch 732, loss = 4.833861827850342\n",
      "Epoch 2, batch 733, loss = 4.877688884735107\n",
      "Epoch 2, batch 734, loss = 4.772217273712158\n",
      "Epoch 2, batch 735, loss = 4.908024311065674\n",
      "Epoch 2, batch 736, loss = 4.976010322570801\n",
      "Epoch 2, batch 737, loss = 4.894432067871094\n",
      "Epoch 2, batch 738, loss = 4.792469501495361\n",
      "Epoch 2, batch 739, loss = 4.871894836425781\n",
      "Epoch 2, batch 740, loss = 5.062983512878418\n",
      "Epoch 2, batch 741, loss = 4.885143756866455\n",
      "Epoch 2, batch 742, loss = 4.8701090812683105\n",
      "Epoch 2, batch 743, loss = 4.844386100769043\n",
      "Epoch 2, batch 744, loss = 4.825146675109863\n",
      "Epoch 2, batch 745, loss = 4.931119441986084\n",
      "Epoch 2, batch 746, loss = 4.920140743255615\n",
      "Epoch 2, batch 747, loss = 4.924383640289307\n",
      "Epoch 2, batch 748, loss = 4.932673454284668\n",
      "Epoch 2, batch 749, loss = 4.939261436462402\n",
      "Epoch 2, batch 750, loss = 4.937469482421875\n",
      "Epoch 2, batch 751, loss = 4.87005615234375\n",
      "Epoch 2, batch 752, loss = 4.708032608032227\n",
      "Epoch 2, batch 753, loss = 4.78875732421875\n",
      "Epoch 2, batch 754, loss = 4.935788154602051\n",
      "Epoch 2, batch 755, loss = 4.852350234985352\n",
      "Epoch 2, batch 756, loss = 4.855884552001953\n",
      "Epoch 2, batch 757, loss = 4.914039611816406\n",
      "Epoch 2, batch 758, loss = 4.676172733306885\n",
      "Epoch 2, batch 759, loss = 4.7593278884887695\n",
      "Epoch 2, batch 760, loss = 4.938355445861816\n",
      "Epoch 2, batch 761, loss = 4.8416547775268555\n",
      "Epoch 2, batch 762, loss = 4.87290620803833\n",
      "Epoch 2, batch 763, loss = 4.867926120758057\n",
      "Epoch 2, batch 764, loss = 4.926774978637695\n",
      "Epoch 2, batch 765, loss = 5.111171722412109\n",
      "Epoch 2, batch 766, loss = 4.93464994430542\n",
      "Epoch 2, batch 767, loss = 4.921616077423096\n",
      "Epoch 2, batch 768, loss = 4.8369622230529785\n",
      "Epoch 2, batch 769, loss = 4.961132526397705\n",
      "Epoch 2, batch 770, loss = 4.93910026550293\n",
      "Epoch 2, batch 771, loss = 4.937899589538574\n",
      "Epoch 2, batch 772, loss = 5.037956237792969\n",
      "Epoch 2, batch 773, loss = 4.913810729980469\n",
      "Epoch 2, batch 774, loss = 4.732976913452148\n",
      "Epoch 2, batch 775, loss = 4.786709785461426\n",
      "Epoch 2, batch 776, loss = 4.803304672241211\n",
      "Epoch 2, batch 777, loss = 4.898013591766357\n",
      "Epoch 2, batch 778, loss = 5.0184807777404785\n",
      "Epoch 2, batch 779, loss = 4.9571533203125\n",
      "Epoch 2, batch 780, loss = 4.917209625244141\n",
      "Epoch 2, batch 781, loss = 4.866995334625244\n",
      "Epoch 2, batch 782, loss = 4.942572593688965\n",
      "Epoch 2, batch 783, loss = 4.950965881347656\n",
      "Epoch 2, batch 784, loss = 5.027143955230713\n",
      "Epoch 2, batch 785, loss = 4.947037696838379\n",
      "Epoch 2, batch 786, loss = 4.867741584777832\n",
      "Epoch 2, batch 787, loss = 4.8610358238220215\n",
      "Epoch 2, batch 788, loss = 4.808760166168213\n",
      "Epoch 2, batch 789, loss = 4.839424133300781\n",
      "Epoch 2, batch 790, loss = 4.895979881286621\n",
      "Epoch 2, batch 791, loss = 4.985074520111084\n",
      "Epoch 2, batch 792, loss = 5.014598369598389\n",
      "Epoch 2, batch 793, loss = 4.99832010269165\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, batch 794, loss = 4.995962142944336\n",
      "Epoch 2, batch 795, loss = 4.923344135284424\n",
      "Epoch 2, batch 796, loss = 4.925938606262207\n",
      "Epoch 2, batch 797, loss = 5.041539669036865\n",
      "Epoch 2, batch 798, loss = 4.8770270347595215\n",
      "Epoch 2, batch 799, loss = 4.940107345581055\n",
      "Epoch 2, batch 800, loss = 4.964145183563232\n",
      "Epoch 2, batch 801, loss = 4.882061004638672\n",
      "Epoch 2, batch 802, loss = 4.926233291625977\n",
      "Epoch 2, batch 803, loss = 4.904962539672852\n",
      "Epoch 2, batch 804, loss = 4.866240978240967\n",
      "Epoch 2, batch 805, loss = 4.974330425262451\n",
      "Epoch 2, batch 806, loss = 4.952371120452881\n",
      "Epoch 2, batch 807, loss = 4.886685371398926\n",
      "Epoch 2, batch 808, loss = 4.940040588378906\n",
      "Epoch 2, batch 809, loss = 4.988296985626221\n",
      "Epoch 2, batch 810, loss = 4.650825500488281\n",
      "Epoch 2, batch 811, loss = 5.2806782722473145\n",
      "Epoch 2, batch 812, loss = 5.076151371002197\n",
      "Epoch 2, batch 813, loss = 4.72935676574707\n",
      "Epoch 2, batch 814, loss = 4.973908424377441\n",
      "Epoch 2, batch 815, loss = 5.218023300170898\n",
      "Epoch 2, batch 816, loss = 5.051259517669678\n",
      "Epoch 2, batch 817, loss = 5.034703254699707\n",
      "Epoch 2, batch 818, loss = 4.971008777618408\n",
      "Epoch 2, batch 819, loss = 5.290966510772705\n",
      "Epoch 2, batch 820, loss = 4.730819225311279\n",
      "Epoch 2, batch 821, loss = 4.9178853034973145\n",
      "Epoch 2, batch 822, loss = 4.977980136871338\n",
      "Epoch 2, batch 823, loss = 4.95623254776001\n",
      "Epoch 2, batch 824, loss = 4.8893842697143555\n",
      "Epoch 2, batch 825, loss = 4.85952091217041\n",
      "Epoch 2, batch 826, loss = 4.934805870056152\n",
      "Epoch 2, batch 827, loss = 4.7106475830078125\n",
      "Epoch 2, batch 828, loss = 4.922792434692383\n",
      "Epoch 2, batch 829, loss = 4.7453131675720215\n",
      "Epoch 2, batch 830, loss = 4.673422336578369\n",
      "Epoch 2, batch 831, loss = 4.8579888343811035\n",
      "Epoch 2, batch 832, loss = 4.810266971588135\n",
      "Epoch 2, batch 833, loss = 4.860270023345947\n",
      "Epoch 2, batch 834, loss = 4.659371376037598\n",
      "Epoch 2, batch 835, loss = 4.679039001464844\n",
      "Epoch 2, batch 836, loss = 4.715496063232422\n",
      "Epoch 2, batch 837, loss = 5.040307998657227\n",
      "Epoch 2, batch 838, loss = 4.824756145477295\n",
      "Epoch 2, batch 839, loss = 4.798133373260498\n",
      "Epoch 2, batch 840, loss = 4.8768696784973145\n",
      "Epoch 2, batch 841, loss = 4.753175258636475\n",
      "Epoch 2, batch 842, loss = 4.731185436248779\n",
      "Epoch 2, batch 843, loss = 4.982272624969482\n",
      "Epoch 2, batch 844, loss = 4.842925548553467\n",
      "Epoch 2, batch 845, loss = 4.844775676727295\n",
      "Epoch 2, batch 846, loss = 4.783793926239014\n",
      "Epoch 2, batch 847, loss = 4.743142604827881\n",
      "Epoch 2, batch 848, loss = 4.725483417510986\n",
      "Epoch 2, batch 849, loss = 4.678173542022705\n",
      "Epoch 2, batch 850, loss = 4.697042942047119\n",
      "Epoch 2, batch 851, loss = 4.563063144683838\n",
      "Epoch 2, batch 852, loss = 4.640206813812256\n",
      "Epoch 2, batch 853, loss = 4.815221309661865\n",
      "Epoch 2, batch 854, loss = 4.868132591247559\n",
      "Epoch 2, batch 855, loss = 4.852461814880371\n",
      "Epoch 2, batch 856, loss = 4.88918399810791\n",
      "Epoch 2, batch 857, loss = 4.821162700653076\n",
      "Epoch 2, batch 858, loss = 4.917136192321777\n",
      "Epoch 2, batch 859, loss = 4.9716949462890625\n",
      "Epoch 2, batch 860, loss = 4.7303032875061035\n",
      "Epoch 2, batch 861, loss = 4.943612098693848\n",
      "Epoch 2, batch 862, loss = 4.7727532386779785\n",
      "Epoch 2, batch 863, loss = 4.807383060455322\n",
      "Epoch 2, batch 864, loss = 4.832447528839111\n",
      "Epoch 2, batch 865, loss = 4.742927551269531\n",
      "Epoch 2, batch 866, loss = 4.800370216369629\n",
      "Epoch 2, batch 867, loss = 4.595322608947754\n",
      "Epoch 2, batch 868, loss = 4.846559047698975\n",
      "Epoch 2, batch 869, loss = 4.917455196380615\n",
      "Epoch 2, batch 870, loss = 4.868106365203857\n",
      "Epoch 2, batch 871, loss = 4.784338474273682\n",
      "Epoch 2, batch 872, loss = 4.87237024307251\n",
      "Epoch 2, batch 873, loss = 4.950992584228516\n",
      "Epoch 2, batch 874, loss = 4.91172456741333\n",
      "Epoch 2, batch 875, loss = 5.111370086669922\n",
      "Epoch 2, batch 876, loss = 4.985313415527344\n",
      "Epoch 2, batch 877, loss = 4.961363315582275\n",
      "Epoch 2, batch 878, loss = 4.905655860900879\n",
      "Epoch 2, batch 879, loss = 4.831080436706543\n",
      "Epoch 2, batch 880, loss = 4.743933200836182\n",
      "Epoch 2, batch 881, loss = 4.96316385269165\n",
      "Epoch 2, batch 882, loss = 5.031290531158447\n",
      "Epoch 2, batch 883, loss = 4.835306167602539\n",
      "Epoch 2, batch 884, loss = 4.891470432281494\n",
      "Epoch 2, batch 885, loss = 4.883301258087158\n",
      "Epoch 2, batch 886, loss = 4.817060470581055\n",
      "Epoch 2, batch 887, loss = 4.982990741729736\n",
      "Epoch 2, batch 888, loss = 4.87923002243042\n",
      "Epoch 2, batch 889, loss = 4.841088771820068\n",
      "Epoch 2, batch 890, loss = 4.847602367401123\n",
      "Epoch 2, batch 891, loss = 4.756708145141602\n",
      "Epoch 2, batch 892, loss = 4.733975410461426\n",
      "Epoch 2, batch 893, loss = 5.053225994110107\n",
      "Epoch 2, batch 894, loss = 4.913918972015381\n",
      "Epoch 2, batch 895, loss = 4.912554740905762\n",
      "Epoch 2, batch 896, loss = 4.876861095428467\n",
      "Epoch 2, batch 897, loss = 4.888984203338623\n",
      "Epoch 2, batch 898, loss = 5.056488990783691\n",
      "Epoch 2, batch 899, loss = 5.05449104309082\n",
      "Epoch 2, batch 900, loss = 4.852782249450684\n",
      "Epoch 2, batch 901, loss = 4.942113876342773\n",
      "Epoch 2, batch 902, loss = 4.7419328689575195\n",
      "Epoch 2, batch 903, loss = 4.930455684661865\n",
      "Epoch 2, batch 904, loss = 4.880980968475342\n",
      "Epoch 2, batch 905, loss = 4.941954612731934\n",
      "Epoch 2, batch 906, loss = 4.929483890533447\n",
      "Epoch 2, batch 907, loss = 4.810055732727051\n",
      "Epoch 2, batch 908, loss = 4.990478992462158\n",
      "Epoch 2, batch 909, loss = 4.961788654327393\n",
      "Epoch 2, batch 910, loss = 4.999431133270264\n",
      "Epoch 2, batch 911, loss = 5.01240348815918\n",
      "Epoch 2, batch 912, loss = 4.9121294021606445\n",
      "Epoch 2, batch 913, loss = 4.896493434906006\n",
      "Epoch 2, batch 914, loss = 4.873102188110352\n",
      "Epoch 2, batch 915, loss = 4.884881973266602\n",
      "Epoch 2, batch 916, loss = 4.903306007385254\n",
      "Epoch 2, batch 917, loss = 4.803597450256348\n",
      "Epoch 2, batch 918, loss = 5.014410495758057\n",
      "Epoch 2, batch 919, loss = 4.970674991607666\n",
      "Epoch 2, batch 920, loss = 4.980820178985596\n",
      "Epoch 2, batch 921, loss = 4.7456254959106445\n",
      "Epoch 2, batch 922, loss = 4.839710712432861\n",
      "Epoch 2, batch 923, loss = 5.214547634124756\n",
      "Epoch 2, batch 924, loss = 5.031354904174805\n",
      "Epoch 2, batch 925, loss = 5.0914225578308105\n",
      "Epoch 2, batch 926, loss = 5.08916711807251\n",
      "Epoch 2, batch 927, loss = 5.1691460609436035\n",
      "Epoch 2, batch 928, loss = 5.146456241607666\n",
      "Epoch 2, batch 929, loss = 4.888325214385986\n",
      "Epoch 2, batch 930, loss = 4.844661712646484\n",
      "Epoch 2, batch 931, loss = 4.881331443786621\n",
      "Epoch 2, batch 932, loss = 4.928313255310059\n",
      "Epoch 2, batch 933, loss = 4.8472900390625\n",
      "Epoch 2, batch 934, loss = 4.938549518585205\n",
      "Epoch 2, batch 935, loss = 4.926718711853027\n",
      "Epoch 2, batch 936, loss = 4.940772533416748\n",
      "Epoch 2, batch 937, loss = 4.665035724639893\n",
      "Epoch 2, batch 938, loss = 4.900918960571289\n",
      "Epoch 2, batch 939, loss = 4.735899448394775\n",
      "Epoch 2, batch 940, loss = 4.74081563949585\n",
      "Epoch 2, batch 941, loss = 4.64512300491333\n",
      "Epoch 2, batch 942, loss = 4.64316463470459\n",
      "Epoch 2, batch 943, loss = 4.730251789093018\n",
      "Epoch 2, batch 944, loss = 4.728935718536377\n",
      "Epoch 2, batch 945, loss = 4.863442897796631\n",
      "Epoch 2, batch 946, loss = 4.737356185913086\n",
      "Epoch 2, batch 947, loss = 4.77656364440918\n",
      "Epoch 2, batch 948, loss = 4.732107639312744\n",
      "Epoch 2, batch 949, loss = 4.768479824066162\n",
      "Epoch 2, batch 950, loss = 4.909581661224365\n",
      "Epoch 2, batch 951, loss = 4.78108024597168\n",
      "Epoch 2, batch 952, loss = 4.577613353729248\n",
      "Epoch 2, batch 953, loss = 4.630223274230957\n",
      "Epoch 2, batch 954, loss = 4.7281813621521\n",
      "Epoch 2, batch 955, loss = 4.845569610595703\n",
      "Epoch 2, batch 956, loss = 4.764251232147217\n",
      "Epoch 2, batch 957, loss = 4.848032474517822\n",
      "Epoch 2, batch 958, loss = 4.783489227294922\n",
      "Epoch 2, batch 959, loss = 4.722077369689941\n",
      "Epoch 2, batch 960, loss = 4.808213233947754\n",
      "Epoch 2, batch 961, loss = 4.894796848297119\n",
      "Epoch 2, batch 962, loss = 4.836949825286865\n",
      "Epoch 2, batch 963, loss = 4.972713470458984\n",
      "Epoch 2, batch 964, loss = 4.775242328643799\n",
      "Epoch 2, batch 965, loss = 4.772104740142822\n",
      "Epoch 2, batch 966, loss = 4.62306547164917\n",
      "Epoch 2, batch 967, loss = 4.936779975891113\n",
      "Epoch 2, batch 968, loss = 4.860655784606934\n",
      "Epoch 2, batch 969, loss = 4.816432476043701\n",
      "Epoch 2, batch 970, loss = 4.735210418701172\n",
      "Epoch 2, batch 971, loss = 4.872556209564209\n",
      "Epoch 2, batch 972, loss = 4.822286605834961\n",
      "Epoch 2, batch 973, loss = 4.911297798156738\n",
      "Epoch 2, batch 974, loss = 4.83638334274292\n",
      "Epoch 2, batch 975, loss = 4.7677202224731445\n",
      "Epoch 2, batch 976, loss = 4.751583099365234\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, batch 977, loss = 4.836925983428955\n",
      "Epoch 2, batch 978, loss = 4.869237899780273\n",
      "Epoch 2, batch 979, loss = 4.825521469116211\n",
      "Epoch 2, batch 980, loss = 4.965408802032471\n",
      "Epoch 2, batch 981, loss = 4.773140907287598\n",
      "Epoch 2, batch 982, loss = 4.919412612915039\n",
      "Epoch 2, batch 983, loss = 4.959787845611572\n",
      "Epoch 2, batch 984, loss = 4.908205509185791\n",
      "Epoch 2, batch 985, loss = 4.986608505249023\n",
      "Epoch 2, batch 986, loss = 4.962819576263428\n",
      "Epoch 2, batch 987, loss = 4.805509090423584\n",
      "Epoch 2, batch 988, loss = 5.041299819946289\n",
      "Epoch 2, batch 989, loss = 4.91310977935791\n",
      "Epoch 2, batch 990, loss = 5.01199197769165\n",
      "Epoch 2, batch 991, loss = 4.89693546295166\n",
      "Epoch 2, batch 992, loss = 4.887451648712158\n",
      "Epoch 2, batch 993, loss = 4.918759822845459\n",
      "Epoch 2, batch 994, loss = 5.027114391326904\n",
      "Epoch 2, batch 995, loss = 4.919490337371826\n",
      "Epoch 2, batch 996, loss = 4.877673625946045\n",
      "Epoch 2, batch 997, loss = 4.957156181335449\n",
      "Epoch 2, batch 998, loss = 4.975280284881592\n",
      "Epoch 2, batch 999, loss = 4.931600093841553\n",
      "Epoch 2, batch 1000, loss = 4.942098140716553\n",
      "Epoch 2, batch 1001, loss = 4.971951961517334\n",
      "Epoch 2, batch 1002, loss = 5.017909049987793\n",
      "Epoch 2, batch 1003, loss = 4.903252124786377\n",
      "Epoch 2, batch 1004, loss = 5.002828598022461\n",
      "Epoch 2, batch 1005, loss = 4.953941822052002\n",
      "Epoch 2, batch 1006, loss = 5.058159828186035\n",
      "Epoch 2, batch 1007, loss = 4.8457770347595215\n",
      "Epoch 2, batch 1008, loss = 4.93692684173584\n",
      "Epoch 2, batch 1009, loss = 4.983793258666992\n",
      "Epoch 2, batch 1010, loss = 4.9585137367248535\n",
      "Epoch 2, batch 1011, loss = 4.8884429931640625\n",
      "Epoch 2, batch 1012, loss = 5.0360612869262695\n",
      "Epoch 2, batch 1013, loss = 4.879233360290527\n",
      "Epoch 2, batch 1014, loss = 4.866652488708496\n",
      "Epoch 2, batch 1015, loss = 4.746437072753906\n",
      "Epoch 2, batch 1016, loss = 5.075695037841797\n",
      "Epoch 2, batch 1017, loss = 4.906181335449219\n",
      "Epoch 2, batch 1018, loss = 4.892568588256836\n",
      "Epoch 2, batch 1019, loss = 4.781031131744385\n",
      "Epoch 2, batch 1020, loss = 4.7598795890808105\n",
      "Epoch 2, batch 1021, loss = 4.775429725646973\n",
      "Epoch 2, batch 1022, loss = 4.7628350257873535\n",
      "Epoch 2, batch 1023, loss = 4.935948371887207\n",
      "Epoch 2, batch 1024, loss = 4.791568756103516\n",
      "Epoch 2, batch 1025, loss = 4.72227144241333\n",
      "Epoch 2, batch 1026, loss = 4.883631229400635\n",
      "Epoch 2, batch 1027, loss = 4.8661932945251465\n",
      "Epoch 2, batch 1028, loss = 4.840524673461914\n",
      "Epoch 2, batch 1029, loss = 4.674006462097168\n",
      "Epoch 2, batch 1030, loss = 4.869814872741699\n",
      "Epoch 2, batch 1031, loss = 4.801307201385498\n",
      "Epoch 2, batch 1032, loss = 4.836660385131836\n",
      "Epoch 2, batch 1033, loss = 4.78713321685791\n",
      "Epoch 2, batch 1034, loss = 4.794570446014404\n",
      "Epoch 2, batch 1035, loss = 4.841378211975098\n",
      "Epoch 2, batch 1036, loss = 4.9696364402771\n",
      "Epoch 2, batch 1037, loss = 4.761881351470947\n",
      "Epoch 2, batch 1038, loss = 4.7647199630737305\n",
      "Epoch 2, batch 1039, loss = 4.8271074295043945\n",
      "Epoch 2, batch 1040, loss = 4.8260369300842285\n",
      "Epoch 2, batch 1041, loss = 4.734875202178955\n",
      "Epoch 2, batch 1042, loss = 4.844169616699219\n",
      "Epoch 2, batch 1043, loss = 4.746470928192139\n",
      "Epoch 2, batch 1044, loss = 4.857599258422852\n",
      "Epoch 2, batch 1045, loss = 4.98917293548584\n",
      "Epoch 2, batch 1046, loss = 4.819202423095703\n",
      "Epoch 2, batch 1047, loss = 4.923879623413086\n",
      "Epoch 2, batch 1048, loss = 4.85500431060791\n",
      "Epoch 2, batch 1049, loss = 4.717978477478027\n",
      "Epoch 2, batch 1050, loss = 4.785373687744141\n",
      "Epoch 2, batch 1051, loss = 4.846211910247803\n",
      "Epoch 2, batch 1052, loss = 4.963128089904785\n",
      "Epoch 2, batch 1053, loss = 4.923771858215332\n",
      "Epoch 2, batch 1054, loss = 4.876463890075684\n",
      "Epoch 2, batch 1055, loss = 5.013130187988281\n",
      "Epoch 2, batch 1056, loss = 4.905526638031006\n",
      "Epoch 2, batch 1057, loss = 4.827636241912842\n",
      "Epoch 2, batch 1058, loss = 4.7655534744262695\n",
      "Epoch 2, batch 1059, loss = 4.84890079498291\n",
      "Epoch 2, batch 1060, loss = 5.03610897064209\n",
      "Epoch 2, batch 1061, loss = 4.884587287902832\n",
      "Epoch 2, batch 1062, loss = 5.052570343017578\n",
      "Epoch 2, batch 1063, loss = 4.769321441650391\n",
      "Epoch 2, batch 1064, loss = 4.889989852905273\n",
      "Epoch 2, batch 1065, loss = 4.945419788360596\n",
      "Epoch 2, batch 1066, loss = 4.906436920166016\n",
      "Epoch 2, batch 1067, loss = 5.088257312774658\n",
      "Epoch 2, batch 1068, loss = 4.859455108642578\n",
      "Epoch 2, batch 1069, loss = 4.876465797424316\n",
      "Epoch 2, batch 1070, loss = 4.946026802062988\n",
      "Epoch 2, batch 1071, loss = 4.9114670753479\n",
      "Epoch 2, batch 1072, loss = 4.9132819175720215\n",
      "Epoch 2, batch 1073, loss = 4.9677886962890625\n",
      "Epoch 2, batch 1074, loss = 4.890003204345703\n",
      "Epoch 2, batch 1075, loss = 5.048977851867676\n",
      "Epoch 2, batch 1076, loss = 5.002680778503418\n",
      "Epoch 2, batch 1077, loss = 5.086447238922119\n",
      "Epoch 2, batch 1078, loss = 4.920891284942627\n",
      "Epoch 2, batch 1079, loss = 4.815976142883301\n",
      "Epoch 2, batch 1080, loss = 4.948053359985352\n",
      "Epoch 2, batch 1081, loss = 5.067121505737305\n",
      "Epoch 2, batch 1082, loss = 5.1510515213012695\n",
      "Epoch 2, batch 1083, loss = 4.810009956359863\n",
      "Epoch 2, batch 1084, loss = 4.795736312866211\n",
      "Epoch 2, batch 1085, loss = 4.886404037475586\n",
      "Epoch 2, batch 1086, loss = 4.705495357513428\n",
      "Epoch 2, batch 1087, loss = 4.744309902191162\n",
      "Epoch 2, batch 1088, loss = 4.5724663734436035\n",
      "Epoch 2, batch 1089, loss = 4.7275261878967285\n",
      "Epoch 2, batch 1090, loss = 4.768177509307861\n",
      "Epoch 2, batch 1091, loss = 4.647523403167725\n",
      "Epoch 2, batch 1092, loss = 4.8406453132629395\n",
      "Epoch 2, batch 1093, loss = 4.616425037384033\n",
      "Epoch 2, batch 1094, loss = 4.445096492767334\n",
      "Epoch 2, batch 1095, loss = 4.766360282897949\n",
      "Epoch 2, batch 1096, loss = 4.828150272369385\n",
      "Epoch 2, batch 1097, loss = 4.84734582901001\n",
      "Epoch 2, batch 1098, loss = 4.749566078186035\n",
      "Epoch 2, batch 1099, loss = 4.77933931350708\n",
      "Epoch 2, batch 1100, loss = 4.639710426330566\n",
      "Epoch 2, batch 1101, loss = 4.875818252563477\n",
      "Epoch 2, batch 1102, loss = 4.643254280090332\n",
      "Epoch 2, batch 1103, loss = 4.921236991882324\n",
      "Epoch 2, batch 1104, loss = 4.764465808868408\n",
      "Epoch 2, batch 1105, loss = 4.787514686584473\n",
      "Epoch 2, batch 1106, loss = 4.820722579956055\n",
      "Epoch 2, batch 1107, loss = 4.98183012008667\n",
      "Epoch 2, batch 1108, loss = 4.891908645629883\n",
      "Epoch 2, batch 1109, loss = 4.783055305480957\n",
      "Epoch 2, batch 1110, loss = 4.709534645080566\n",
      "Epoch 2, batch 1111, loss = 4.846843242645264\n",
      "Epoch 2, batch 1112, loss = 4.872379302978516\n",
      "Epoch 2, batch 1113, loss = 4.9242777824401855\n",
      "Epoch 2, batch 1114, loss = 4.816948413848877\n",
      "Epoch 2, batch 1115, loss = 4.932365894317627\n",
      "Epoch 2, batch 1116, loss = 4.962646007537842\n",
      "Epoch 2, batch 1117, loss = 5.046777248382568\n",
      "Epoch 2, batch 1118, loss = 4.900278568267822\n",
      "Epoch 2, batch 1119, loss = 4.954265594482422\n",
      "Epoch 2, batch 1120, loss = 4.9133620262146\n",
      "Epoch 2, batch 1121, loss = 4.973663806915283\n",
      "Epoch 2, batch 1122, loss = 4.853177070617676\n",
      "Epoch 2, batch 1123, loss = 4.927976131439209\n",
      "Epoch 2, batch 1124, loss = 4.927871227264404\n",
      "Epoch 2, batch 1125, loss = 5.002583980560303\n",
      "Epoch 2, batch 1126, loss = 4.94936466217041\n",
      "Epoch 2, batch 1127, loss = 5.002313137054443\n",
      "Epoch 2, batch 1128, loss = 5.085545063018799\n",
      "Epoch 2, batch 1129, loss = 4.897977828979492\n",
      "Epoch 2, batch 1130, loss = 4.955996036529541\n",
      "Epoch 2, batch 1131, loss = 4.632736682891846\n",
      "Epoch 2, batch 1132, loss = 4.771641731262207\n",
      "Epoch 2, batch 1133, loss = 4.687586307525635\n",
      "Epoch 2, batch 1134, loss = 4.637277603149414\n",
      "Epoch 2, batch 1135, loss = 4.910173416137695\n",
      "Epoch 2, batch 1136, loss = 4.621592998504639\n",
      "Epoch 2, batch 1137, loss = 4.836252689361572\n",
      "Epoch 2, batch 1138, loss = 4.725621223449707\n",
      "Epoch 2, batch 1139, loss = 4.849220275878906\n",
      "Epoch 2, batch 1140, loss = 4.7829813957214355\n",
      "Epoch 2, batch 1141, loss = 4.805217266082764\n",
      "Epoch 2, batch 1142, loss = 4.862372398376465\n",
      "Epoch 2, batch 1143, loss = 4.868427753448486\n",
      "Epoch 2, batch 1144, loss = 4.775656223297119\n",
      "Epoch 2, batch 1145, loss = 4.898314476013184\n",
      "Epoch 2, batch 1146, loss = 4.822413921356201\n",
      "Epoch 2, batch 1147, loss = 4.8179707527160645\n",
      "Epoch 2, batch 1148, loss = 4.773951053619385\n",
      "Epoch 2, batch 1149, loss = 4.913121700286865\n",
      "Epoch 2, batch 1150, loss = 4.629095554351807\n",
      "Epoch 2, batch 1151, loss = 4.921966552734375\n",
      "Epoch 2, batch 1152, loss = 4.987196922302246\n",
      "Epoch 2, batch 1153, loss = 4.899312973022461\n",
      "Epoch 2, batch 1154, loss = 4.949663162231445\n",
      "Epoch 2, batch 1155, loss = 5.004133701324463\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, batch 1156, loss = 4.938380241394043\n",
      "Epoch 2, batch 1157, loss = 4.902214527130127\n",
      "Epoch 2, batch 1158, loss = 4.882693767547607\n",
      "Epoch 2, batch 1159, loss = 5.002616882324219\n",
      "Epoch 2, batch 1160, loss = 4.991044998168945\n",
      "Epoch 2, batch 1161, loss = 4.802206993103027\n",
      "Epoch 2, batch 1162, loss = 4.928064823150635\n",
      "Epoch 2, batch 1163, loss = 4.916528701782227\n",
      "Epoch 2, batch 1164, loss = 4.648209095001221\n",
      "Epoch 2, batch 1165, loss = 4.641732692718506\n",
      "Epoch 2, batch 1166, loss = 4.516732215881348\n",
      "Epoch 2, batch 1167, loss = 4.648763179779053\n",
      "Epoch 2, batch 1168, loss = 4.777879238128662\n",
      "Epoch 2, batch 1169, loss = 4.8534255027771\n",
      "Epoch 2, batch 1170, loss = 4.933951377868652\n",
      "Epoch 2, batch 1171, loss = 4.814910411834717\n",
      "Epoch 2, batch 1172, loss = 4.85117769241333\n",
      "Epoch 2, batch 1173, loss = 4.808319091796875\n",
      "Epoch 2, batch 1174, loss = 4.833578586578369\n",
      "Epoch 2, batch 1175, loss = 4.977574348449707\n",
      "Epoch 2, batch 1176, loss = 4.936861515045166\n",
      "Epoch 2, batch 1177, loss = 4.830727577209473\n",
      "Epoch 2, batch 1178, loss = 5.026658535003662\n",
      "Epoch 2, batch 1179, loss = 4.921705722808838\n",
      "Epoch 2, batch 1180, loss = 4.866100788116455\n",
      "Epoch 2, batch 1181, loss = 4.874967575073242\n",
      "Epoch 2, batch 1182, loss = 4.853210926055908\n",
      "Epoch 2, batch 1183, loss = 4.89638614654541\n",
      "Epoch 2, batch 1184, loss = 5.038244724273682\n",
      "Epoch 2, batch 1185, loss = 4.93569803237915\n",
      "Epoch 2, batch 1186, loss = 4.819096088409424\n",
      "Epoch 2, batch 1187, loss = 4.7561187744140625\n",
      "Epoch 2, batch 1188, loss = 4.770849704742432\n",
      "Epoch 2, batch 1189, loss = 4.74105978012085\n",
      "Epoch 2, batch 1190, loss = 4.68001127243042\n",
      "Epoch 2, batch 1191, loss = 4.719634532928467\n",
      "Epoch 2, batch 1192, loss = 4.695978164672852\n",
      "Epoch 2, batch 1193, loss = 4.736246109008789\n",
      "Epoch 2, batch 1194, loss = 4.778748035430908\n",
      "Epoch 2, batch 1195, loss = 4.834582805633545\n",
      "Epoch 2, batch 1196, loss = 4.874781131744385\n",
      "Epoch 2, batch 1197, loss = 4.922160625457764\n",
      "Epoch 2, batch 1198, loss = 4.989351272583008\n",
      "Epoch 2, batch 1199, loss = 4.890284061431885\n",
      "Epoch 2, batch 1200, loss = 4.612245559692383\n",
      "Epoch 2, batch 1201, loss = 4.786294937133789\n",
      "Epoch 2, batch 1202, loss = 4.794680595397949\n",
      "Epoch 2, batch 1203, loss = 4.676810264587402\n",
      "Epoch 2, batch 1204, loss = 4.741036891937256\n",
      "Epoch 2, batch 1205, loss = 4.821518421173096\n",
      "Epoch 2, batch 1206, loss = 4.917911052703857\n",
      "Epoch 2, batch 1207, loss = 4.959830284118652\n",
      "Epoch 2, batch 1208, loss = 4.870129108428955\n",
      "Epoch 2, batch 1209, loss = 4.809385776519775\n",
      "Epoch 2, batch 1210, loss = 4.830503940582275\n",
      "Epoch 2, batch 1211, loss = 4.846719264984131\n",
      "Epoch 2, batch 1212, loss = 4.793583869934082\n",
      "Epoch 2, batch 1213, loss = 4.739231109619141\n",
      "Epoch 2, batch 1214, loss = 4.905035972595215\n",
      "Epoch 2, batch 1215, loss = 4.834722995758057\n",
      "Epoch 2, batch 1216, loss = 4.714635848999023\n"
     ]
    }
   ],
   "source": [
    "chatbot.train(session,\n",
    "              2, 256,\n",
    "              input_sentences_idx,\n",
    "              ground_truth_sentences_idx,\n",
    "              input_lengths,\n",
    "              ground_truth_lengths,\n",
    "              3e-5,\n",
    "              0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i m not'"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatbot.get_reply(session, \"what is your name\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
